---
title: "Tendências Avançadas em Sistemas Operacionais"
---

## A arquitetura do `kernel` se transforma para computação heterogênea

A mudança fundamental na arquitetura do `kernel` centra-se nas implementações de **Arquitetura de Sistema Heterogêneo (HSA)** que eliminam a distinção  tradicional entre `CPU` e `GPU` no nível de gerenciamento de memória. O **Linux** introduziu **Gerenciamento de Memória Heterogênea (HMM)** na versão 3.19 do `kernel` especificamente para suportar placas gráficas AMD Radeon, permitindo que `GPU` IOMMU e `CPU` MMU mantenham mapeamentos de memória coerentes sem cópia explícita entre dispositivos.

Essa mudança arquitetural requer que os kernels implementem espaços de endereçamento virtual unificados onde `CPU`s e aceleradores compartilham tabelas de páginas idênticas. **Especificações HSA** exigem que espaços de endereçamento virtual de dispositivos correspondam exatamente ao host, suportando endereçamento de 32 e 64 bits com 48 bits utilizáveis. A complexidade emerge no gerenciamento de diferentes domínios de coerência—`GPU`s e `CPU`s requerem protocolos distintos de coerência de _cache_ com mecanismos de sincronização explícitos através de ioctls específicos do dispositivo.

O **Kernel Fusion Driver (KFD)** da AMD, incorporado no **Linux** 3.19, exemplifica essa evolução arquitetural implementando filas heterogêneas (HQ) que distribuem trabalhos computacionais através de múltiplas `CPU`s e `GPU`s de uma interface unificada. O KFD funciona junto com drivers gráficos existentes, fornecendo interface de runtime HSA para espaço de usuário enquanto mantém compatibilidade com padrões tradicionais de uso de `GPU`.

**Extensões conscientes de NUMA** representam outra adaptação arquitetural crítica. Sistemas heterogêneos frequentemente requerem políticas de escalonamento e alocação de memória que consideram diferentes hierarquias de memória e padrões de acesso através de aceleradores. Kernels modernos implementam consciência sofisticada de topologia de memória que considera largura de banda de memória de `GPU`, latência de interconexão `CPU`-`GPU` e características de consumo de energia ao tomar decisões de alocação de recursos.

## Frameworks de drivers evoluem além do gerenciamento  tradicional de dispositivos

A evolução de drivers de dispositivos simples para frameworks computacionais abrangentes representa uma das mudanças mais dramáticas no design de SO. O **subsistema Direct Rendering Manager (DRM)** do **Linux** se transform  de suporte gráfico básico para uma infraestrutura computacional sofisticada suportando gerenciadores de memória **Graphics Execution Manager (GEM)** e **Translation Table Manager (TTM)**.

**DMA Buffer Sharing (DMA-BUF)** fornece a base técnica para compartilhamento de memória entre dispositivos, enquanto o **framework PRIME** permite offloading de `GPU` entre gráficos discretos e integrados. O DRM moderno suporta computação `GPU`PU através de mecanismos de submissão de comandos, escalonadores de `GPU` para gerenciar filas de execução e gerenciamento de endereços virtuais de `GPU` para isolamento de processos—recursos que estavam completamente ausentes dos primeiros drivers gráficos.

A evolução do **Windows Display Driver Model (WDDM)** demonstra complexidade arquitetural similar. WDDM 2.0 introduziu **Endereçamento Virtual de `GPU`** onde cada processo recebe espaço de endereços virtuais único de `GPU`, enquanto WDDM 3.0-3.2 adicionaram o **Microsoft Compute Driver Model (MCDM)** especificamente para processadores de IA incluindo NPUs. O WDDM mais recente suporta migração ao vivo para `GPU`s virtualizadas e atualizações de driver a quente—capacidades que requerem mudanças fundamentais no subsistema de gerenciamento de dispositivos do `kernel` Windows.

O **framework Metal** da Apple representa uma abordagem arquitetural diferente, fornecendo acesso de baixo overhead à `GPU` com gerenciamento explícito de buffer de comandos e pipelines unificados gráficos-computacionais. **Metal Performance Shaders Graph (MPSGraph)** permite operações tensores multidimensionais com fusão automática de kernels, enquanto Metal 3 introduziu suporte ray tracing acelerado por hardware para arquiteturas `GPU` especializadas do Apple Silicon.

O surgimento da **integração de `TPU`** requer abstrações de driver inteiramente novas. `TPU`s do Google usam conjuntos de instruções CISC com instruções especializadas para operações de array sistólico, gerenciamento de memória de pesos e padrões de acesso de memória de alta largura de banda que não se mapeiam limpiamente para modelos tradicionais de driver de `GPU`. A arquitetura sistólica elimina acessos intermediários à memória mantendo fluxo de dados através de elementos de processamento, requerendo integração de SO que entenda modelos de programação de fluxo de dados ao invés de kernels computacionais tradicionais.

## Gerenciamento de memória se adapta a arquiteturas unificadas e sistemas de memória especializados

**CUDA Unified Virtual Memory (UVM)** representa a arquitetura de memória unificada mais amplamente implantada, implementando políticas de migração de primeiro toque onde páginas migram sob demanda entre memória de `CPU` e `GPU`. Entretanto, UVM introduz overhead significativo de performance devido à complexidade de tratamento de page faults—pesquisas recentes mostram implementações **`GPU`VM** usando dispositivos com capacidade RDMA podem alcançar melhorias de performance de 4x sobre UVm tradicional eliminando envolvimento de `CPU`/SO no gerenciamento de memória.

**OpenCL Shared Virtual Memory** fornece três níveis de abstração: compartilhamento de granularidade grossa em limites de buffer, SVM de buffer de granularidade fina com consistência atômica, e SVM de sistema de granularidade fina suportando operações individuais de load/store dentro da memória host. Esses diferentes níveis de abstração requerem mecanismos distintos de suporte do `kernel` para coerência de memória e sincronização.

**Integração de High-Bandwidth Memory (HBM)** demanda novas estratégias de gerenciamento de memória. HBM3 fornece largura de banda de 819 GB/s através de barramento de memória de 1.024 bits comparado a 32-64 bits para memória convencional. Processadores Intel com suporte HBM implementam três modos de operação: somente HBM (espaço de endereços plano único), modo _cache_ (HBM como _cache_ do lado da memória) e modo Plano (HBM e DDR como nós NUMA separados). Cada modo requer diferentes políticas de gerenciamento de memória do `kernel` e afeta características de performance da aplicação dramaticamente.

A arquitetura de memória de `TPU` introduz complexidade adicional com **integração de array sistólico**. `TPU` v1 apresenta 28 MiB de memória on-chip com acumuladores de 4 MiB 32-bit e 8 GiB DDR3 fornecendo largura de banda de 34 GB/s. O design sistólico elimina hierarquias de _cache_ tradicionais, requerendo gerenciamento de memória de SO que entenda padrões de fluxo de dados tensor ao invés de semântica de memória virtual convencional.

O **Gerenciamento de Memória Heterogênea (HMM)** do **Linux** fornece a base do `kernel` para essas arquiteturas de memória diversas através de compartilhamento de espaço de endereços (duplicando tabelas de páginas de `CPU` em tabelas de páginas de dispositivo) e integração de memória de dispositivo usando o tipo de memória ZONE_DEVICE. Isso permite migração entre memória host e dispositivo usando mecanismos existentes do `kernel` enquanto previne mapeamento inadequado de `CPU` de regiões de memória específicas do dispositivo.

## Escalonamento e alocação de recursos passam por redesign fundamental

Escalonadores tradicionais de `CPU` se provam inadequados para cargas de trabalho heterogêneas requerendo **escalonamento consciente de memória** que considera largura de banda, latência e consumo de energia através de diferentes tipos de processadores. Escalonadores modernos implementam posicionamento consciente de topologia NUMA garantindo que `CPU`s e `GPU`s sejam alocadas dentro do mesmo domínio NUMA para minimizar overhead de interconexão.

**Implementações de escalonador de `GPU`** evoluíram de filas simples de dispositivo para sistemas sofisticados de submissão de trabalhos baseados em entidades com escalonamento FIFO, suporte de prioridade e tratamento de dependências entre trabalhos. O **subsistema escalonador de `GPU`** do **Linux** (`drivers/gpu/drm/scheduler/`) fornece filas de trabalho de software com execução em hardware, sincronização entre `GPU`s e gerenciamento de fence/timeline para coordenação complexa de cargas de trabalho.

**Algoritmos de escalonamento eficientes em energia** como UEJS (Utilization aware Energy-efficient Job Scheduling) e H-PSO (Hybrid Particle Swarm Optimization) demonstram economias de energia de 20-35% em clusters acelerados por `GPU` incorporando Dynamic Voltage and Frequency Scaling (DVFS) com decisões de escalonamento de tarefas. Esses algoritmos mantêm garantias de deadline em tempo real enquanto otimizam consumo de energia através de unidades de processamento heterogêneas.

Pesquisas mostram **algoritmos de escalonamento multi-acelerador** que consideram diferentes características de processadores (`CPU`, `GPU`, `TPU`) podem alcançar até 37% de melhoria no tempo de conclusão em ambientes com recursos limitados através de otimização de partição de dados e overhead minimizado de transferência entre unidades de processamento.

## Mecanismos de segurança e isolamento abordam novos vetores de ataque

A natureza compartilhada de recursos de `GPU` cria desafios de segurança sem precedentes. **AMD MxGPU** fornece isolamento baseado em hardware usando SR-IOV, separando fisicamente framebuffers de máquinas virtuais e mecanismos de shader, enquanto **NVIDIA vGPU** depende de isolamento baseado em software com time-slicing. Pesquisas demonstram as implicações de segurança: isolamento de software cria riscos potenciais de vazamento de dados comparado à separação em nível de hardware.

**Ataques de canal lateral** contra `GPU`s representam ameaças emergentes. Pesquisas documentam canais secretos alcançando largura de banda de 3,95 MB/s através de sistemas multi-`GPU` via congestionamento NVLink, enquanto **ataques `GPU`.zip** aproveitam compressão de dados para roubo de pixels. Análise de energia e emanações eletromagnéticas de operações de `GPU` fornecem vetores de ataque adicionais requerendo novos mecanismos de defesa.

**NVIDIA H100 Confidential Computing** introduz ambientes de execução confiáveis baseados em hardware com raiz de confiança de hardware on-die, cadeias de boot seguro e capacidades de atestação. Isso representa uma mudança arquitetural fundamental em direção a **limites de segurança impostos por hardware** ao invés de mecanismos de isolamento baseados em software.

Segurança de contêineres requer abordagens especializadas. **NVIDIA Container Toolkit** fornece acesso à `GPU` através de montagem de dispositivos, mas vulnerabilidades como CVE-2024-0132 permitem cenários de escape de contêiner. A correção implementada na versão 1.17.4 demonstra a evolução contínua de mecanismos de segurança para cargas de trabalho de `GPU` containerizadas.

## Interfaces de chamadas de sistema e `APIs`se expandem além de limites tradicionais

O **Linux** implementa **ioctls específicos do DRM** extensivos incluindo `DRM_IOCTL_GEM_CREATE` para criação de objetos buffer, `DRM_IOCTL_PRIME_HANDLE_TO_FD` para compartilhamento entre dispositivos e `DRM_IOCTL_SYNCOBJ_CREATE` para primitivos de sincronização. Essas chamadas de sistema fornecem aplicações de espaço de usuário com acesso direto ao escalonamento de `GPU`, gerenciamento de memória e capacidades de sincronização.

**Funções da família D3DKMT** do Windows habilitam acesso de dispositivo em modo `kernel` com alocação de endereços virtuais de `GPU`, criação de contexto e gerenciamento de residência de memória. **Integração DirectML** compila operadores ML para listas de comandos DirectX 12 com suporte meta-comando para cargas de trabalho de IA, representando uma partida completa de `APIs`tradicionalmente orientadas a gráficos.

A pesquisa identifica **`GPU`fs** (UT Austin/EPFL) como trabalho pioneiro fornecendo `APIs`similares ao POSIX para programas de `GPU` acessando sistemas de arquivos host. `GPU`fs alcança melhoria de performance de 7x sobre `CPU`s de 8 núcleos para operações de busca de arquivos estendendo _cache_ de buffer de `CPU` para memória de `GPU` e suportando milhares de operações de arquivo concorrentes.

**APIs de memória entre dispositivos** habilitam modelos de memória unificados com endereçamento virtual compartilhado estilo HSA, protocolos de coerência de memória heterogênea e consciência de topologia de memória multi-`GPU`. Essas `APIs`representam expansões fundamentais de interfaces tradicionais de SO que eram historicamente centradas na `CPU`.

## Abordagens de virtualização e containerização se transformam dramaticamente

**Tecnologias de virtualização de `GPU`** abrangem múltiplas abordagens de implementação. **NVIDIA GRID/vGPU** usa time-slicing com isolamento de software alcançando 88-96% de performance nativa mas requerendo taxas de licenciamento. **AMD MxGPU** implementa SR-IOV de hardware com isolamento físico alcançando 96-99% de performance nativa sem custos de licenciamento. **Intel GVT-g** fornece pass-through mediado com 80-90% de performance mas suporte de hardware limitado a gerações mais antigas.

**Modificações de hypervisor** refletem essas diferentes abordagens. A implementação **XenGT do Xen** aproveita design de microkernel para melhor isolamento com 96-99% de performance nativa, enquanto **passthrough de `GPU` KVM** via VFIO alcança 98-100% de performance beneficiando-se de recursos de segurança do Linux. **VMware ESXi** fornece gerenciamento de nível empresarial mas com performance inconsistente através de diferentes arquiteturas.

**Orquestração de contêineres** requer gerenciamento especializado de recursos. **Kubernetes `GPU` Operator** automatiza instalação de software de `GPU` com suporte Multi-Instance `GPU` (MIG) para isolamento de cargas de trabalho e time-slicing de `GPU` para compartilhamento de recursos. **NVIDIA Container Toolkit**trata montagem de dispositivos e bibliotecas de driver, enquanto **ClusterPolicy Custom Resource Definitions** fornecem gerenciamento centralizado de configuração.

A pesquisa revela **implementação SR-IOV** como a abordagem de virtualização mais promissora, criando Funções Virtuais (VFs) para acesso direto de VM enquanto reduz overhead de hypervisor e melhora isolamento de segurança através de particionamento de recursos em nível de hardware.

## Ferramentas de monitoramento de performance e depuração alcançam sofisticação sem precedentes

A evolução do **Intel VTune Profiler** de análise somente de `CPU` para suporte abrangente de `GPU` demonstra a complexidade da análise de performance heterogênea. O VTune moderno fornece análise de Hotspots de Compute/Media de `GPU`, análise de Offload de `GPU` e análise Roofline de `GPU` usando contadores de performance de hardware, amostragem de PC e coleta de eventos através de Intel Graphics, `GPU`s NVIDIA e arquiteturas Intel Xe.

**NVIDIA Nsight Systems e Nsight Compute** representam o estado da arte atual em profiling de `GPU`, fornecendo profiling de timeline com precisão de nanossegundos, rastreamento de API CUDA, análise de `throughput` de memória e métricas de eficiência de execução de warp. A evolução do Visual Profiler (2008) para a suíte Nsight atual demonstra o aumento dramático na sofisticação de profiling requerida para aplicações heterogêneas modernas.

**Contribuições de pesquisa acadêmica** como **extensões HPCToolkit** fornecem medição e análise escaláveis de aplicações aceleradas por `GPU`, associando métricas de performance com loop nests e contextos de chamada através de `GPU`s NVIDIA, AMD e Intel. Esta pesquisa, publicada em múltiplas conferências HPCA, ISCA e MICRO, demonstra as fundações acadêmicas subjacentes ao desenvolvimento de ferramentas comerciais de profiling.

**AMD ROCProfiler SDK (rocprofv3)** construído em rocprofiler-sdk substitui profilers legados com rastreamento abrangente de API HIP, monitoramento de API HSA, coleta de contadores de hardware e análise de cópia de memória para aplicações baseadas em ROCm, destacando a natureza específica do fornecedor da análise de performance heterogênea.

## Gerenciamento de energia e considerações térmicas alcançam complexidade extrema

A evolução do **subsistema cpufreq do Linux** demonstra a complexidade do gerenciamento de energia heterogênea. A arquitetura de três camadas (núcleo, governadores de escalonamento, drivers de escalonamento) agora coordena através de estados de energia de `CPU` e aceleradores. **Drivers Intel P-State** fornecem P-states gerenciados por hardware com dicas de Energy Performance Preference, enquanto tecnologias **AMD PowerNow!/Cool'n'Quiet**tratam diferentes abordagens arquiteturais para escalonamento de energia.

**NVIDIA Dynamic Boost** representa gerenciamento de energia em nível de sistema, com _nvidia-powered_ redistribuindo energia entre `CPU` e `GPU` enquanto mantém orçamentos térmicos. Isso requer coordenação próxima entre subsistemas de gerenciamento de energia de `CPU` e `GPU` e demonstra as implicações de sistema da computação heterogênea.

**Sistemas de gerenciamento térmico** enfrentam desafios sem precedentes com aceleradores de IA modernos. Pesquisas mostram racks de IA empurrando densidades de 60-120kW requerendo **resfriamento por imersão líquida**, materiais de mudança de fase e designs avançados de dissipadores de calor. **Daemon thermald do Linux** previne superaquecimento da `CPU` através de P-states, T-states e drivers de grampo de energia Intel, mas gerenciamento térmico de `GPU` requer soluções específicas do fornecedor com throttling automático de frequência.

**Características térmicas de `TPU`** diferem significativamente de perfis térmicos de `GPU`. `TPU` v1 opera a 700 MHz com resfriamento especializado para arrays sistólicos, enquanto `TPU`s modernas implementam escalonamento de frequência consciente de temperatura com sensores térmicos integrados nos próprios elementos de processamento.

**Algoritmos de escalonamento eficientes em energia** como UEJS e H-PSO demonstram economias de energia de 20-35% em clusters heterogêneos integrando DVFS com decisões de escalonamento de tarefas. Esses algoritmos devem balancear requisitos de performance, restrições de deadline e consumo de energia através de tipos diversos de processadores com diferentes perfis de consumo de energia.

## Pesquisa acadêmica impulsiona inovações fundamentais

O **projeto Gdev** (UC Santa Cruz, Universidade de Nagoya) representa pesquisa inovadora em gerenciamento de recursos de `GPU` de primeira classe, fornecendo ecossistemas completos de driver de dispositivo de `GPU` e biblioteca de runtime de código aberto. O **escalonador BAND** do Gdev alcança virtualização de `GPU` com speedup de 2x para sistemas de arquivos criptografados e demonstra gerenciamento de memória virtual com swapping de dados para demandas excessivas de memória.

**Contribuições da Universidade de Stanford** incluem linguagem de programação Sequoia para gerenciamento de hierarquia de memória e desenvolvimento de linguagem de shading de `GPU` Slang, enquanto seu supercomputador Marlowe com 248 `GPU`s NVIDIA H100 habilita pesquisa de ponta. **Projeto Sia da Carnegie Mellon** foca em escalonamento de cluster ML consciente de heterogeneidade publicado no SOSP 2023.

**ETH Zurich Systems Group** lidera pesquisa europeia com a **plataforma HEROv2** fornecendo infraestrutura de computação heterogênea de código aberto full-stack, **SO multikernel Barrelfish** suportando hardware diverso incluindo `GPU`s, e **cluster ETHZ-HACC** para pesquisa multi-core, `GPU` e FPGA através de parcerias AMD Xilinx University.

**Pesquisa de segurança** de instituições líderes demonstra a importância crítica de ambientes de execução confiáveis. O projeto **Graviton da Microsoft Research** alcança execução confiável de `GPU` com overhead de performance de 17-33%, enquanto **computação confidencial NVIDIA H100** fornece performance quase nativa com garantias de segurança baseadas em hardware.

## Conclusão

A integração de `GPU`s, `TPU`s e `LPU`s em **Sistemas Operacionais** representa a transformação arquitetural mais significativa desde a introdução do gerenciamento de memória virtual. **Sistemas Operacionais** modernos evoluíram de modelos dispositivo-periférico para plataformas de computação heterogênea sofisticadas que coordenam coerência de memória através de tipos diversos de processadores, implementam isolamento de segurança em nível de hardware e fornecem interfaces de programação unificadas para aplicações abrangendo múltiplas arquiteturas de aceleradores.

A complexidade técnica de suportar **espaços de endereçamento virtual unificados**, **enclaves de segurança acelerados por hardware** e **gerenciamento de energia multi-dispositivo** enquanto mantém estabilidade e performance do sistema representa um desafio de engenharia contínuo. A pesquisa demonstra que **Sistemas Operacionais** ainda estão se adaptando aos requisitos de computação heterogênea, com inovação contínua necessária em portabilidade entre plataformas, garantias de escalonamento em tempo real e arquiteturas de aceleradores emergentes.

A colaboração entre instituições de pesquisa acadêmica e indústria demonstra a maturidade deste campo, com implementações concretas como Gdev, `GPU`fs e HEROv2 fornecendo fundações para sistemas comerciais. À medida que `LPU`s e outros processadores especializados emergem, **Sistemas Operacionais** exigir evolução arquitetural contínua para suportar a próxima geração de cargas de trabalho de computação heterogênea. a transformação de design de **Sistema Operacional** centrado na `CPU` para nativo em aceleradores está fundamentalmente mudando como pensamos sobre arquitetura de software de sistema na era da computação especializada.

## Referências

ACM. In-Datacenter Performance Analysis of a Tensor Processing Unit. **Proceedings of the 44th Annual International Symposium on Computer Architecture**, 2017. Disponível em: https://dl.acm.org/doi/10.1145/3079856.3080246.

AMD. Getting started with Virtualization. **AMD Instinct Virtualization Driver**. Disponível em: https://instinct.docs.amd.com/projects/virt-drv/en/latest/userguides/Getting_started_with_MxGPU.html.

APPLE DEVELOPER. Metal Overview. Disponível em: https://developer.apple.com/metal/.

ARCHLINUX. `CPU` frequency scaling. **ArchWiki**. Disponível em: https://wiki.archlinux.org/title/CPU_frequency_scaling.

ARCHLINUX. Intel GVT-g. **ArchWiki**. Disponível em: https://wiki.archlinux.org/title/Intel_GVT-g.

ARXIV. **GPUVM: GPU-driven Unified Virtual Memory**. arXiv:2411.05309, 2024. Disponível em: https://arxiv.org/abs/2411.05309.

ARXIV. Beyond the Bridge: Contention-Based Covert and Side Channel Attacks on Multi-GPU Interconnect. Disponível em: https://arxiv.org/html/2404.03877v1.

ETH ZURICH. Databases on Heterogene s Architectures. **Institute for Computing Platforms - Systems Group**. Disponível em: https://systems.ethz.ch/research/data-processing-on-modern-hardware/database-acceleration.html.

ETH ZURICH. Heterogene s Accelerated Compute Cluster. **Institute for Computing Platforms - Systems Group**. Disponível em: https://systems.ethz.ch/research/data-processing-on-modern-hardware/hacc.html.

FREEDESKTOP. DRM Memory Management. **The **Linux** `kernel` documentation**. Disponível em: https://dri.freedesktop.org/docs/drm/gpu/drm-mm.html.

GOOGLE CLOUD. Introduction to Cloud TPU. Disponível em: https://cloud.google.com/tpu/docs/intro-to-tpu.

GOOGLE CLOUD. TPU architecture. Disponível em: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm.

IEEE. CPU–GPU Utilization Aware Energy-Efficient Scheduling Algorithm on Heterogene s Computing Systems. **IEEE Journals & Magazine**. Disponível em: https://ieeexplore.ieee.org/document/9045988/.

INTEL. Intel® VTune™ Profiler for `CPU` and GPU profiling. Disponível em: https://amrdocs.intel.com/docs/2.1/dev_guide/system_integrator/benchmark_profiling/vtune-profiler.html.

KERNEL.ORG. DRM Memory Management. **The **Linux** `kernel` documentation**. Disponível em: https://docs.kernel.org/gpu/drm-mm.html.

KERNEL.ORG. Heterogene s Memory Management (HMM). **The **Linux** `kernel` documentation**, v.5.0. Disponível em: https://www.kernel.org/doc/html/v5.0/vm/hmm.html.

MARKTECHPOST. What is the Language Processing Unit (LPU)? Its Role in AI Hardware. Disponível em: https://www.marktechpost.com/2024/04/22/what-is-the-language-processing-unit-lpu-its-role-in-ai-hardware/.

MICROSOFT. GPU Virtual Memory in WDDM 2.0. **Windows drivers**. Disponível em: https://learn.microsoft.com/en-us/windows-hardware/drivers/display/gpu-virtual-memory-in-wddm-2-0.

MICROSOFT. Introduction to DirectML. Disponível em: https://learn.microsoft.com/en-us/windows/ai/directml/dml.

MICROSOFT RESEARCH. Barrelfish - Overview. Disponível em: https://www.microsoft.com/en-us/research/project/barrelfish/overview/.

MICROSOFT RESEARCH. Graviton: Trusted Execution Environments on GPUs. Disponível em: https://www.microsoft.com/en-us/research/publication/graviton-trusted-execution-environments-on-gpus/.

NVIDIA. Accelerated Virtual Desktops for Mobile and Office Workers. **NVIDIA Virtual GPU Solutions**. Disponível em: https://www.nvidia.com/en-us/data-center/virtual-gpu-technology/.

NVIDIA. Confidential Computing on NVIDIA H100 GPUs for Secure and Trustworthy AI. **NVIDIA Technical Blog**. Disponível em: https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/.

NVIDIA. NVIDIA Visual Profiler. **NVIDIA Developer**. Disponível em: https://developer.nvidia.com/nvidia-visual-profiler.

NVIDIA. Performance Analysis Tools. **NVIDIA Developer**. Disponível em: https://developer.nvidia.com/performance-analysis-tools.

NVIDIA. Simplifying GPU Application Development with Heterogene s Memory Management. **NVIDIA Technical Blog**. Disponível em: https://developer.nvidia.com/blog/simplifying-gpu-application-development-with-heterogene s-memory-management/.

PURE STORAGE. What Is a Language Processing Unit (LPU)? Disponível em: https://www.purestorage.com/knowledge/what-is-lpu.html.

RESEARCHGATE. CPU-GPU Utilization Aware Energy-Efficient Scheduling Algorithm on Heterogene s Computing Systems. Disponível em: https://www.researchgate.net/publication/340145446_CPU-GPU_Utilization_Aware_Energy-Efficient_Scheduling_Algorithm_on_Heterogene s_Computing_Systems.

RESEARCHGATE. Gdev: First-class GPU resource management in the operating system. Disponível em: https://www.researchgate.net/publication/244311014_Gdev_First-class_GPU_resource_management_in_the_operating_system.

ROCM BLOGS. Performance Profiling on AMD GPUs – Part 1: Foundations. Disponível em: https://rocm.blogs.amd.com/software-tools-optimization/profiling-guide/intro/README.html.

SCIENCEDIRECT. Deep learning based data prefetching in CPU-GPU unified virtual memory. Disponível em: https://www.sciencedirect.com/science/article/abs/pii/S0743731522002490.

SCIENCEDIRECT. Enabling PoCL-based runtime frameworks on the HSA for OpenCL 2.0 support. Disponível em: https://www.sciencedirect.com/science/article/abs/pii/S1383762117301121.

SECURITYWEEK. Academics Devise Side-Channel Attack Targeting Multi-GPU Systems. Disponível em: https://www.securityweek.com/academics-devise-side-channel-attack-targeting-multi-gpu-systems/.

TOM'S HARDWARE. What Are HBM, HBM2 and HBM2E? A Basic Definition. Disponível em: https://www.tomshardware.com/reviews/glossary-hbm-hbm2-high-bandwidth-memory-definition,5889.html.

USENIX. Gdev: First-Class GPU Resource Management in the Operating System. Disponível em: https://www.usenix.org/conference/atc12/technical-sessions/presentation/kato.

USENIX. Graviton: Trusted Execution Environments on GPUs. Disponível em: https://www.usenix.org/conference/osdi18/presentation/volos.

WIKIPEDIA. Direct Rendering Manager. Disponível em: https://en.wikipedia.org/wiki/Direct_Rendering_Manager.

WIKIPEDIA. Heterogene s System Architecture. Disponível em: https://en.wikipedia.org/wiki/Heterogene s_System_Architecture.

WIKIPEDIA. High Bandwidth Memory. Disponível em: https://en.wikipedia.org/wiki/High_Bandwidth_Memory.

WIKIPEDIA. Metal (API). Disponível em: https://en.wikipedia.org/wiki/Metal_(API).

WIKIPEDIA. Non-uniform memory access. Disponível em: https://en.wikipedia.org/wiki/Non-uniform_memory_access.

WIKIPEDIA. Systolic array. Disponível em: https://en.wikipedia.org/wiki/Systolic_array.

WIKIPEDIA. Tensor Processing Unit. Disponível em: https://en.wikipedia.org/wiki/Tensor_Processing_Unit.

WIZ. NVIDIA AI vulnerability: Deep Dive into CVE 2024-0132. **Wiz Blog**. Disponível em: https://www.wiz.io/blog/nvidia-ai-vulnerability-deep-dive-cve-2024-0132.