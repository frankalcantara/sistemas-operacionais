# Processos de Sistema: Linux vs. Windows

Enquanto cada processo de usuário reside em seu próprio Espaço de Endereçamento Virtual (**VAS**) isolado, o `Kernel` do **Sistema Operacional** opera em um contexto de memória privilegiado e fundamentalmente diferente. O espaço de endereçamento do `Kernel` é uma porção do **VAS** que é persistentemente mapeada em todos os processos do sistema. Essa arquitetura permite que o `Kernel` acesse eficientemente suas próprias estruturas de dados e o hardware do sistema, independentemente de qual processo de usuário está atualmente em execução. No entanto, a organização e o gerenciamento desse espaço do `Kernel` diferem significativamente entre sistemas como **Linux** e Windows, refletindo filosofias de design distintas. A @fig-sysproc1 mostra uma visão geral do espaço de endereçamento do `Kernel` em comparação com o modelo de processos de usuários. 

:::: {#fig-sysproc1}
![](/images/memory_architecture_svg.webp)

Comparação entre arquiteturas de espaço virtual de endereçamento para processos de sistema e processos de usuário. Os processos de sistema (esquerda) possuem acesso privilegiado ao `kernel` space e executam com permissões estendidas no user space, permitindo interação direta com hardware e estruturas críticas do sistema. Os processos de usuário (direita) operam em ambiente completamente isolado com layout padronizado de memória virtual, nas quais o `kernel` space permanece protegido e inacessível, garantindo isolamento através da MMU e proteção via page tables.
:::

A arquitetura predominante em **Sistemas Operacionais** de $64 bits$ como $x86-64$ é uma divisão canônica do espaço de endereçamento de $2^{64}$ bytes. Devido a limitações práticas de hardware, apenas $48$ bits são tipicamente usados para endereçamento, resultando em um **VAS** total de $256TB$. Este espaço é dividido em duas metades:

1. **Espaço de Usuário (User-space)**: A metade inferior, endereços canônicos baixos, tipicamente de `0x0000000000000000` a `0x00007FFFFFFFFFFF`) é privada para cada processo. Ela contém o código, dados, pilha e `heap` do processo, como descrito anteriormente na seção @sec-vasproc1.

2. **Espaço do `kernel` (Kernel-space)**: a metade superior, endereços canônicos altos, tipicamente de `0xFFFF800000000000` em diante, é a região do `Kernel`. Esta parte do mapa de endereços é (majoritariamente) global e compartilhada entre todos os processos. Quando o código do modo de usuário tenta acessar um endereço nesta faixa, o hardware da **MMU** gera uma falha de proteção, garantindo o isolamento. a transição do modo de usuário para o modo de `Kernel` (via _system call_ ou interrupção) é o que concede à `CPU` o privilégio de acessar esta região.

### O Layout do Espaço de Endereçamento do **Linux** `kernel` Space **Linux** x86_64

O `Kernel` **Linux** organiza sua parte do espaço de endereçamento em várias zonas com propósitos e características de mapeamento distintos, otimizando o desempenho para diferentes tipos de alocação de memória. A @fig-kernelmemorylinux mostra esta estrutura de dados.

::: {#fig-kernelmemorylinux}
![](/images/kernel_memory_layout.webp)

Layout completo de memória virtual **Linux** $x86_64$ demonstrando a organização do espaço de endereçamento em 48 bits. À esquerda, a visão geral com separação entre `kernel` space, gap não-canônico e user space. À direita, o detalhamento das regiões específicas do `kernel` space incluindo mapeamento direto, área vmalloc, módulos e código do `kernel`.
:::

O `Kernel` **Linux** organiza sua parte do espaço de endereçamento virtual em zonas especializadas, cada uma com propósitos e características de mapeamento distintos (ver Documentation/x86/x86_64/mm.rst). Esta divisão otimiza o desempenho para diferentes tipos de alocação de memória e operações do sistema.

Como eu disse antes, este é um capítulo árido. A corajosa leitora pode, e deve conferir as referências diretas ao código online, sem precisar clonar o [repositório do `kernel`](https://github.com/torvalds/linux) no  site [Elixir Bootlin](https://elixir.bootlin.com/linux/v6.15.4/) 

#### 1. Mapeamento Direto de Memória Física (Direct-mapping region)

Esta divisão constitui a região mais significativa, talvez a mais importante, do espaço de endereçamento do `Kernel`. O **Linux** mapeia toda,  a maior parte, da `RAM` física do sistema em um `range` contíguo de endereços virtuais, começando em um `offset` conhecido como `PAGE_OFFSET`(ver arch/x86/include/asm/page_64.h). A relação matemática é simples, elegante e eficiente:

$$\text{Endereço Virtual} = \text{Endereço Físico} + \text{PAGE\_OFFSET}$$

Este mapeamento $1:1$, denominado _linear map_ na documentação, permite que o `Kernel` acesse qualquer endereço físico através de uma simples operação aritmética binária, uma soma,  subtração de endereços, evitando completamente o custo computacional de modificar tabelas de páginas na maioria dos acessos à memória (ver arch/x86/mm/init_64.c).

**Características técnicas**:

- **Range de endereços**: `0xffff888000000000` - `0xffffc87fffffffff`;
- **Função de alocação**: `kmalloc()` e variantes (ver mm/slab.h);
- **Contiguidade**: garantia de páginas fisicamente contíguas;
- **Performance**: acesso de latência mínima, ideal para operações críticas;
- **Uso principal**: operações de **D**irect **M**emory **A**ccess (**DMA**), estruturas de dados do `kernel` que requerem baixa latência, `buffers` de `E/S` (ver include/linux/dma-mapping.h).

#### 2. Área vmalloc

Nem todas as alocações do `Kernel` podem ser satisfeitas com memória fisicamente contígua, especialmente alocações de grande porte em sistemas que permanecem em execução por períodos extensos. A fragmentação da memória física devido a estes regimes extremos torna impossível encontrar blocos contíguos suficientemente grandes. Para resolver este problema o `Kernel` **Linux** introduz a área `vmalloc`, que permite alocações de memória virtualmente contíguas, mas fisicamente dispersas na memória (ver mm/vmalloc.c).

O `Kernel` constrói explicitamente as entradas da tabela de páginas (**PTEs**) necessárias para mapear essas páginas físicas não contíguas em um bloco virtual que aparece como contínuo.

**Características técnicas**:

- **Range de endereços**: `0xffffc90000000000` - `0xffffe8ffffffffff`;
- **Função de alocação**: `vmalloc()`, `vzalloc()`, `vmalloc_32()` (ver include/linux/vmalloc.h);
- **Contiguidade**: virtual apenas, fisicamente dispersa;
- **Performance**: custo computacional extra devido ao **TLB** e `_cache_` impactado pela dispersão física dos dados;
- **Uso principal**: grandes buffers, módulos do kernel, mapeamentos de dispositivos.

#### 3. Área de Módulos do Kernel

Quando módulos do `kernel`, arquivos `.ko`, como `drivers` de dispositivos ou sistemas de arquivos, são carregados dinamicamente no **Linux** usando `insmod` ou `modprobe`. Durante o carregamento, o `kernel` aloca memória para os segmentos do módulo, código executável (`.text`) e dados inicializados (`.data`) e não inicializados (`.bss`), em uma região específica do espaço de endereçamento virtual, geralmente dentro ou próxima à área `vmalloc` (ex.: `0xffffffffa0000000` a `0xffffffffff5fffff`). Essa área étratada separadamente devido às necessidades únicas dos módulos do `kernel`: **carregamento dinâmico** permite inserir ou remover módulos a qualquer momento, exigindo alocação/desalocação flexível; **relocação de símbolos** ajusta referências a funções ou variáveis do `kernel` (ex.: `printk`) para endereços reais; e **proteções de segurança**, como o bit **NX**, garantem que apenas o `.text` seja executável, protegendo contra exploits. Essas operações são gerenciadas pelo `kernel` em `kernel/module.c`, usando mecanismos semelhantes ao `vmalloc` (como mapeamento de páginas não contíguas), mas com lógica especializada para módulos (ver `kernel/module.c`).

::: callout-note
**Bit NX (No eXecute): Proteção de Segurança em Nível de Hardware**

O **bit NX** é um mecanismo de segurança implementado em hardware nas arquiteturas $x86_64$ modernas que **previne a execução de código em páginas de memória marcadas como não executáveis**. Cada entrada na tabela de páginas contém um bit específico (bit 63) em que `NX = 0` permite execução de instruções e `NX = 1` gera uma exceção do tipo `page fault` caso a `CPU` tente executar código naquela página. Este mecanismo implementa o princípio **D**ata **E**xecution **P**revention, **DEP**  fundamental para a segurança do sistema.
:::

Durante o carregamento de módulos `.ko`, o `kernel` configura cuidadosamente as permissões de cada segmento usando o bit **NX**. O segmento `.text` recebe permissões `R-X` com `NX = 0` para permitir execução do código, enquanto os segmentos `.data` e `.bss` recebem permissões `RW-` com `NX = 1` para impedir execução acidental ou maliciosa de dados como código. Esta separação rigorosa entre código executável e dados protege contra ataques de _code injection_, _buffer overflow_ e técnicas **ROP**/**JOP** em que atacantes tentam executar `shellcode` injetado em `buffers` de dados.

O `kernel` **Linux** implementa esta proteção automaticamente através das macros `PAGE_KERNEL_EXEC` e `PAGE_KERNEL` definidas em `arch/x86/include/asm/pgtable_types.h`, garantindo que apenas o código legitimamente carregado no segmento `.text` possa ser executado pela CPU, mantendo a integridade e segurança do espaço do `kernel`.


**Características técnicas da Área de Módulos do Kernel**:

- **Range de endereços**: `0xffffffffa0000000` - `0xffffffffff5fffff`;
- **Carregamento**: via `insmod`, `modprobe`,  carregamento automático (ver kernel/module.c);
- **Relocação**: suporte a relocação dinâmica de símbolos;
- **Proteção**: segmentos executáveis com proteções apropriadas (**NX bit**) (ver arch/x86/include/asm/pgtable.h);
- **Uso principal**: drivers de dispositivo, sistemas de arquivos, protocolos de rede.

#### 4. `kernel` Text/Data (vmlinux)

Esta área contém o núcleo permanente do **Sistema Operacional** , incluindo todo o código essencial do `Kernel` compilado estaticamente no binário `vmlinux` (ver arch/x86/kernel/vmlinux.lds.S). Diferentemente dos módulos carregáveis, este código é carregado durante a inicialização do sistema e permanece residente durante toda a execução. 

Este processo de carregamento durante a inicialização do sistema é feito pelo **bootloader** (GRUB, systemd-boot, etc.). O **bootloader** carrega o binário `vmlinux` da mídia de armazenamento para a memória física, tipicamente em endereços baixos como `0x1000000` ($16MB$). Se o `kernel` estiver comprimido, `bzImage`, o **bootloader** carrega primeiro um `stub` de descompressão que extrai o `vmlinux` real para a memória. Durante esta fase, o sistema ainda opera em **modo real** ou **modo protegido** de $32 bits$, sem paginação ativa.

a transição crítica ocorre quando o `kernel` assume controle e configura seu próprio ambiente de execução. O código em `arch/x86/kernel/head_64.S` estabelece as tabelas de páginas iniciais, ativa o **modo longo** ($64 bits$) e configura o mapeamento virtual inicial. Neste ponto o `kernel` executa sua própria relocação, mapeando as seções `.text`, `.data`, `.bss` e `.rodata` do `vmlinux` para os endereços virtuais definitivos na região `0xffffffff80000000` - `0xffffffffa0000000`. Esta relocação é necessária porque o `linker`definiu quais endereços virtuais específicos deveriam usadnos no script `arch/x86/kernel/vmlinux.lds.S`, mas o `kernel` foi inicialmente carregado em endereços físicos diferentes.

O processo final envolve a ativação das proteções de memória através da configuração dos bits de permissão nas **PTEs**. O segmento `.text` recebe permissões de leitura e execução com o bit **NX** desabilitado, o segmento `.rodata` fica somente leitura, e os segmentos `.data`/`.bss` recebem permissões de leitura/escrita com o bit **NX** habilitado. Uma vez completado este mapeamento, o `kernel` possui seu espaço de endereçamento virtual completamente configurado e pode iniciar a execução normal do sistema, incluindo a inicialização de subsistemas e o carregamento posterior de módulos dinâmicos. Ou seja, neste ponto, os processos exclusivos do `kernel` estão prontos para serem executados,  em execução, e o sistema pode iniciar a execução de processos de usuário.

**Características técnicas**:

- **Range de endereços**: `0xffffffff80000000` - `0xffffffffa0000000`;
- **Segmentos incluídos**: 
  - `.text`: Código executável do kernel;
  - `.data`: Dados inicializados estaticamente;
  - `.bss`: Dados não inicializados (zerados na inicialização);
  - `.rodata`: Dados somente leitura.
- **Carregamento**: Durante boot, pelo bootloader e early kernel;
- **Proteção**: Segmentação rigorosa com bits **NX** e **WP**.

#### Recursos de Segurança e Randomização

O **Linux** moderno implementa várias técnicas de hardening para proteger o espaço do `kernel` contra exploits:

**KASLR (Kernel Address Space Layout Randomization) (ver arch/x86/boot/compressed/kaslr.c)**:

- Randomiza a base dos endereços do `kernel` a cada boot
- Dificulta ataques baseados em endereços conhecidos
- Implementado através de offset aleatório aplicado durante carregamento

**SMEP/SMAP (Supervisor Mode Execution/Access Prevention)**:

- **SMEP**: Previne execução de código em páginas de usuário pelo `kernel` (ver arch/x86/include/asm/processor-flags.h);
- **SMAP**: Previne acesso a dados de usuário sem autorização explícita;
- Suportado por hardware moderno $x86_64$.

**Proteções adicionais**:

- **Stack canaries**: Detecção de buffer overflow em funções do `kernel` (ver include/linux/stackprotect.h);
- **KPTI**: `kernel` Page Table Isolation para mitigação de Meltdown (ver arch/x86/mm/pti.c);
- **Control Flow Integrity**: Proteção contra ataques ROP/JOP (ver arch/x86/kernel/cfi.c).

#### Considerações de Performance

A organização do espaço do `kernel` reflete trade-offs fundamentais entre performance e flexibilidade:

- **Mapeamento direto**: Máxima performance para operações críticas;
- **vmalloc**: Flexibilidade para grandes alocações com custo de performance;
- **Fragmentação**: Balanceamento entre utilização eficiente da memória e contiguidade física;
- **_cache_ locality**: Organização que favorece localidade espacial e temporal.

### O Layout do Espaço de Endereçamento do `kernel` Windows

O `Kernel` do **Windows NT**, usado em todas as versões modernas, do **Windows XP** ao **Windows 11** e Server, também usa um modelo de memória dividida, mas com sua própria terminologia e organização interna.

1. **Paged Pool e Non-paged Pool**: estas são as duas principais áreas de `heap` do `Kernel` para alocação dinâmica.

   - **Non-paged Pool**: contém estruturas de dados do `Kernel` que não podem, em nenhuma circunstância, ser paginadas para o disco. Isso inclui dados que podem ser acessados em um `Interrupt Request Level` (**IRQL**) elevado (quando falhas de página não são permitidas), como **D**eferred **P**rocedure **C**alls, **DPC** ou dados essenciais para o próprio gerenciador de memória. Alocações aqui são feitas via `ExAllocatePoolWithTag(NonPagedPool, ...)` e são garantidas para residir na `RAM` física.
   - **Paged Pool**: Contém dados do `Kernel` que podem ser paginados para o disco (movidos para o _page file_) em momentos de alta pressão de memória. O acesso seguro a esses dados é governado pelo mecanismo de prioridade de interrupção do Windows, o **I**nterrupt **R**equest **L**evel (`IRQL`). O `IRQL` determina a prioridade do código em execução e quais operações são permitidas.

   Dados no _Paged Pool_ só podem ser acessados quando a `CPU` está em `IRQL = 0`, um estado conhecido como `PASSIVE_LEVEL`. A razão para esta restrição é que um acesso a dados que foram paginados para o disco gera uma _page fault_. Em `PASSIVE_LEVEL`, o `Kernel` pode gerenciar essa falha de forma segura: ele pode suspender a `thread` atual, instruir o Gerenciador de Memória a carregar a página necessária do disco para a `RAM` e, em seguida, retomar a `thread`. Em contraste, em `IRQLs` mais altos (como `DISPATCH_LEVEL` ou superior), o escalonador de `threads` está desativado para permitir que tarefas mais urgentes sejam concluídas sem interrupção. Se uma _page fault_ ocorresse nesse estado, o sistema não poderia esperar pela demorada operação de leitura do disco, resultando em uma falha crítica do sistema (um _Bug Check_ ou _Tela Azul da Morte_), tipicamente com o código `IRQL_NOT_LESS_OR_EQUAL`.

2. **System _cache_ Working Set**: esta região é usada pelo _Cache Manager_ para mapear `views` de arquivos em _cache_. Em vez de ler dados de disco diretamente em `buffers` do `Kernel`, o **Windows** mapeia porções de arquivos no espaço de endereçamento do `Kernel`, permitindo que o acesso aos dados do arquivo seja tratado como acessos à memória.

3. **System PTEs (Page Table Entries)**: uma área reservada de entradas de tabela de páginas que o `Kernel` usa para mapear dinamicamente coisas como `buffers` de ``E/S`` para `DMA`,  para mapear temporariamente seções de memória de processos de usuário no espaço do `Kernel` durante _system calls_.

4. **Hyperspace (obsoleto em x64, mas conceitualmente importante)**: Em sistemas de $32 bits$, esta era uma área especial usada para mapear temporariamente o _working set_ de um processo no espaço do `Kernel` para manipulá-lo. Em x64, com um espaço de endereçamento muito maior, essa técnica não é mais necessária da mesma forma.

5. **Código do `kernel` e Drivers**: contém a imagem do `Kernel` (`ntoskrnl.exe`), a `Hardware Abstraction Layer` (**HAL** - `hal.dll`) e todos os _drivers_ (`.sys` files) carregados no sistema.

A tabela @fig-kernel-mem-comp resume as principais regiões análogas entre **Linux** e Windows.

::: {#fig-kernel-mem-comp}
![](/images/kernel_memory_comparison_svg.webp)

Organização do espaço de endereçamento dos kernels **Linux** e **Windows** NT. O **Linux** (esquerda) emprega mapeamento direto 1:1 com a memória física para máxima eficiência, complementado por área vmalloc para alocações virtualmente contíguas. O **Windows** NT (direita) utiliza pools diferenciados (paged/non-paged) com controle rigoroso via IRQL, _cache_ manager dedicado e PTEs dinâmicas. As regiões especializadas refletem filosofias distintas: simplicidade e performance no **Linux** versus controle granular e robustez no **Windows** NT
:::

::: callout-note
**Um Mergulho Técnico na Proteção do Kernel: KPTI e a Mitigação de Meltdown**

A simples divisão do espaço de endereçamento com um bit de permissão Supervisor/Usuário na **PTE** foi considerada segura por décadas. No entanto, a vulnerabilidade de hardware _Meltdown_ ([CVE-2017-5754](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-5754)) quebrou essa premissa. _Meltdown_ explora a execução especulativa em processadores modernos para ler dados do espaço do `Kernel` a partir do modo de usuário. Mesmo que a instrução de acesso final falhe e gere uma exceção, os dados do `Kernel` já foram trazidos para o `_cache_` da `CPU` e podem ser inferidos através de um ataque de _side-channel_.

A solução de software para esse problema de hardware é o **K**ernel **P**age **T**able **I**solation**, **KPTI**, também conhecido como `kaise_r_`. O **KPTI** reforça radicalmente o isolamento entre o `user-space` e o `kernel-space`, quebrando o modelo de um único conjunto de tabelas de páginas compartilhado.

Em vez de um único conjunto de tabelas de páginas, o **KPTI** mantém **dois conjuntos**:

1. **Conjunto de Tabelas de Páginas de Usuário**: este é o conjunto ativo quando o código está sendo executado em modo de usuário. Ele contém todos os mapeamentos para o `user-space` do processo, mas inclui apenas uma porção mínima e essencial do `kernel-space`. Essa porção mínima contém apenas o código necessário para lidar com a transição de, e para o modo `Kernel`, transições chamadas de _trampolines_ de _system call_ e interrupção, e nenhuma informação sensível do `Kernel`. A grande maioria da memória do `Kernel` simplesmente não está mapeada aqui.

2. **Conjunto de Tabelas de Páginas do Kernel**: Este é o conjunto completo e tradicional de tabelas de páginas, contendo os mapeamentos tanto para o `user-space` do processo atual quanto para todo o `kernel-space`.

Este mecanismo opera da seguinte forma:

- **Execução em Modo Usuário**: o registrador `CR3`, que aponta para a base da hierarquia de tabelas de páginas, aponta para o **conjunto de tabelas de páginas de usuário**. Qualquer tentativa de acesso especulativo a endereços do `Kernel` falhará em encontrar um mapeamento válido, mitigando o _Meltdown_.

- **Transição para o Modo `kernel` (System Call/Interrupt)**: assim que uma transição para o `Kernel` ocorre, uma das primeiras ações executadas pelo código da _trampoline_ é **trocar o valor do registrador CR3** para que ele aponte para o **conjunto de tabelas de páginas do Kernel**. Agora, o `Kernel` tem acesso total a toda a memória, como esperado.

- **Retorno para o Modo Usuário**: antes de retornar ao modo de usuário, via `sysret` ou `iret`, o `Kernel` troca o `CR3`, para que ele aponte novamente para o conjunto de tabelas de páginas de usuário.

O custo computacional extra do **KPTI** vem dessa troca constante do registrador `CR3` e do _TLB flush_ que ela acarreta. Cada transição de/para o `Kernel` se torna mais cara. Para otimizar isso, processadores mais novos introduziram o **P**rocess-**C**ontext **ID**entifier, **PCID**, que permite que entradas da **TLB** sejam marcadas com um ID. Com o **PCID**, o `Kernel` pode alternar entre os dois contextos de tabela de páginas (**PCID** para usuário, **PCID** para `Kernel`) sem precisar invalidar todo o conteúdo da **TLB** a cada transição, reduzindo significativamente a sobrecarga de desempenho. Tanto o **Linux** quanto o **Windows** implementaram o **KPTI**,  sua variante específica para mitigar o _Meltdown_.

Até este ponto em que estamos do livro, o **KPTI** é o melhor exemplo do laço de realimentação positiva que eu citei antes, algumas vezes. Neste caso, tivemos uma necessidade, um problema de segurança, que provocou uma mudança no **Sistema Operacional** . Que, por sua vez, provocou uma mudança no hardware. Para fraseando [Fred Guiol](https://en.wikipedia.org/wiki/Fred_Guiol): assim caminha a tecnologia!
:::

## Projeto 2: Monitor de Hierarquia de Processos e Análise de Memória

Este projeto tem como objetivo demonstrar as diferenças fundamentais entre as arquiteturas de processos nos **Sistemas Operacionais** Linux** e Windows. Implementado em C++23, o sistema combina análise de hierarquia de processos, layout de memória e interações entre espaço de usuário e kernel, permitindo que esforçada leitora possa compreender conceitos complexos de forma interativa.

### Aspectos Técnicos

Um sistema em **C++23** que utilize `std::variant` para gerenciar dados específicos de cada plataforma, `std::unique_ptr` para garantir um gerenciamento seguro de memória e `std::chrono` para medições precisas de tempo. O sistema deve integrar **API**s específicas para **Linux**, como `/proc`, `ptrace` e `sysfs`, a fim de analisar `threads` do `kernel` e `vDSO`, e para Windows, utilizando `PSAPI`, `ToolHelp` e `NtQuerySystemInformation` para coletar informações sobre processos e memória. Implemente um tratamento robusto de erros para lidar com falhas específicas, como permissões negadas ou **API**s indisponíveis. Além disso, adote um design modular que permita a adição de suporte a novos **Sistemas Operacionais** ou funcionalidades no futuro.

### Objetivos

Consolidar os conceitos de **Sistemas Operacionais**, como gerenciamento de processos, alocação de memória e interações kernel-usuário. Fazendo uma comparação entre as arquiteturas de processos entre **Linux** e Windows.Fornecer visualizações e demonstrações interativas de conceitos como **K**ernel **A**ddress **S**pace **L**ayout **R**andomization, **KASLR** e **K**ernel **P**age **T**able **I**solation. Em uma ferramenta que seja extensível para análises avançadas e futuras expansões.

A esperança é que a amável leitora consiga: algum aprendizado prático com a interação com dados reais do **Sistema Operacional** reforçando os conceitos teóricos; pensamento crítico sobre **Sistemas Operacionais**, graças a análise das diferenças arquiteturais entre **Linux** e Windows; e uma compreensão mais profunda de como os **Sistemas Operacionais** modernos gerenciam processos e memória.

O programa deverá ser capaz de mostrar as diferenças Arquiteturais entre o **Linux** e o Windows, e como essas diferenças afetam o gerenciamento de processos e memória. As principais funcionalidades incluem:

**Linux**:

- **Kernel Threads**: Identificados automaticamente (`[kthreadd]`, `[migration/0]`)
- **Hierarquia do init**: Todos os processos descendem do `systemd` (PID 1)
- **vDSO**: Virtual Dynamic Shared Object detectado
- **KASLR**: `kernel` Address Space Layout Randomization ativo

**Windows**:

- **System Process**: Processo fundamental (PID 4) que não é um thread
- **Session Isolation**: Separação clara entre sessions 0 (services) e usuário
- **No `kernel` Threads**: **Windows** usa threads dentro de processos, não processos `kernel` independentes
- **VAS Split**: Divisão clara user/kernel space

Além disso, o programa deve demonstrar:

- **Mapeamento de memória virtual** específico de cada plataforma
- **Detecção de proteções** (KPTI, KASLR)
- **Análise de regiões** (text, data, heap, stack, libraries)
- **Diferenças de addressing** (48-bit vs 32-bit)
- **Árvore genealógica** completa de processos
- **Categorização automática** (User, System, Kernel, Service)
- **Estatísticas comparativas** entre tipos de processo
- **Visualização ASCII** educativa

### Fases do Projeto

O Projeto está dividido em $5$ fases:

#### Fase 1: Arquitetura Central e Implementação Básica para Linux

**Por que?**

- Estabelecer uma base modular que suporte implementações para **Linux** e Windows.
- Iniciar com **Linux** devido à facilidade de acesso a informações de processos via `/proc`.

**Como?**

- Desenvolver a classe `SystemProcessMonitor` como núcleo do sistema, coordenando as funcionalidades.
- Criar uma interface abstrata `PlatformInterface` para abstrair lógica específica de cada plataforma.
- Implementar `LinuxPlatform` para enumerar processos e construir a hierarquia inicial usando `/proc`.

**Definição da Classe**:

```cpp
class SystemProcessMonitor {
public:
    enum class Platform { **Linux**, **Windows** };
    enum class ProcessType { 
        User, KernelThread, SystemProcess, Driver, Service 
    };
    
private:
    Platform current_platform_;
    std::unique_ptr<PlatformInterface> platform_impl_;
    std::unique_ptr<MemoryAnalyzer> memory_analyzer_;
    std::unique_ptr<HierarchyMapper> hierarchy_mapper_;
};
```

**Explicação**: A classe `SystemProcessMonitor` é o ponto central do sistema. O enum `Platform` define as plataformas suportadas, enquanto `ProcessType` categoriza tipos de processos (usuário, thread do kernel, etc.). Os ponteiros inteligentes (`std::unique_ptr`) garantem gerenciamento seguro de memória para os componentes `PlatformInterface`, `MemoryAnalyzer` e `HierarchyMapper`. O uso de C++23 assegura modernidade e robustez.

**Motivação**:

- A modularidade facilita a adição de novas plataformas ou funcionalidades.
- Começar com **Linux** permite explorar rapidamente conceitos como hierarquia de processos.

**Entregáveis**:

- Enumeração de processos no Linux.
- Hierarquia básica de processos.
- Estrutura inicial de dados para processos.

#### Fase 2: Implementação para **Windows** e Interface Cross-Platform

**Por que?**

- Expandir o sistema para suportar Windows, permitindo comparações entre os dois sistemas.
- Garantir que a interface cross-platform seja consistente e robusta.

**Como?**

- Implementar `WindowsPlatform` usando `APIs`do Windows, como `CreateToolhelp32Snapshot`, para enumerar processos.
- Atualizar `ProcessInfo` para incluir dados específicos do Windows, como nível de elevação e sessão.
- Refinar `PlatformInterface` para suportar métodos consistentes em ambas as plataformas.

**Definição da Classe**:

```cpp
class PlatformInterface {
public:
    virtual ~PlatformInterface() = default;
    virtual std::vector<ProcessInfo> enumerateProcesses() = 0;
    virtual MemoryLayout analyzeMemoryLayout(pid_t pid) = 0;
    virtual KernelSpaceInfo getKernelSpaceInfo() = 0;
    virtual ProcessHierarchy buildHierarchy() = 0;
};

class LinuxPlatform : public PlatformInterface {
    std::vector<ProcessInfo> enumerateProcesses() override;
    MemoryLayout analyzeMemoryLayout(pid_t pid) override;
};

class WindowsPlatform : public PlatformInterface {
    std::vector<ProcessInfo> enumerateProcesses() override;
    MemoryLayout analyzeMemoryLayout(pid_t pid) override;
};
```

**Explicação**: A `PlatformInterface` é uma classe abstrata que define métodos virtuais puros para enumeração de processos, análise de memória, obtenção de informações do `kernel` e construção de hierarquias. `LinuxPlatform` usa `/proc` para coletar dados, enquanto `WindowsPlatform` utiliza `APIs`nativas do Windows. Essa abstração permite que o mesmo código cliente funcione em ambas as plataformas.

**Motivação**:

- A comparação direta entre **Linux** e **Windows** destaca diferenças como a separação de sessões no **Windows** e os cgroups no Linux.
- A interface unificada simplifica o desenvolvimento e a manutenção.

**Entregáveis**:

- Enumeração de processos no Windows.
- Estrutura `ProcessInfo` unificada para ambas as plataformas.
- Hierarquia de processos cross-platform.

#### Fase 3: Análise de Memória e Integração de TLB

**Por que?**

- Aprofundar a análise com detalhes sobre layouts de memória e desempenho do TLB (Translation Lookaside Buffer).
- Demonstrar como **Linux** e **Windows** gerenciam memória de forma diferente.

**Como?**

- Implementar `MemoryAnalyzer` para analisar regiões de memória (e.g., `/proc/self/maps` no **Linux**, `VirtualQueryEx` no Windows).
- Simular traduções de endereços virtuais e desempenho do TLB com `TLBSimulator`.
- Incluir suporte para recursos específicos, como KASLR (Linux) e KPTI (Windows).

**Definição da Classe**:

```cpp
class MemoryAnalyzer {
public:
    struct MemoryRegion {
        uintptr_t start_address;
        uintptr_t end_address;
        std::string permissions;
        std::string region_type;  // text, data, heap, stack, vdso, etc.
        bool is_kernel_accessible;
        std::optional<std::string> backing_file;
    };
    
    struct MemoryLayout {
        std::vector<MemoryRegion> regions;
        VirtualAddressSpace vas_info;
        KernelMapping kernel_mapping;
        TLBStats tlb_performance;
    };
    
    MemoryLayout analyzeProcessMemory(pid_t pid);
    void demonstrateKernelUserSplit();
    VirtualAddressTranslation simulatePageTranslation(uintptr_t virtual_addr);
};
```

- **Explicação**: A classe `MemoryAnalyzer` analisa o layout de memória de um processo, identificando regiões como texto, heap e stack. A estrutura `MemoryRegion` armazena detalhes de cada região (endereços, permissões, tipo). `MemoryLayout` agrega informações sobre o espaço de endereçamento virtual (`VirtualAddressSpace`), mapeamentos do `kernel` e estatísticas do TLB. Métodos como `analyzeProcessMemory` e `simulatePageTranslation` permitem explorar dinamicamente a memória e simular outraduções de endereços.

**Motivação**:

- A análise de memória é essencial para entender como os **Sistemas Operacionais** gerenciam recursos.
- Simulações de TLB ajudam a compreender o impacto do hardware na tradução de endereços.

**Entregáveis**:

- Relatórios detalhados de layout de memória.
- Simulações de tradução de endereços e desempenho do TLB.
- Documentação de **KASLR** e **KPTI**.

#### Fase 4: Demonstrações e Análise Comparativa

**Por que?**

- Criar demonstrações interativas para ensinar conceitos complexos de `kernel` de forma acessível.
- Comparar métricas entre **Linux** e **Windows** para destacar diferenças arquiteturais.

**Como?**

- Desenvolver `KernelConceptDemo` para demonstrar conceitos como divisão de espaço de endereçamento e **KPTI/KASLR**.
- Implementar `ComparativeAnalyzer` para gerar relatórios comparativos sobre memória, processos e design do `kernel`.
- Criar uma interface CLI interativa com `EducationalCLI`.

**Definição da Classe**:

```cpp
class KernelConceptDemo {
public:
    void demonstrateAddressSpaceSplit();
    void showKernelVsUserProcesses();
    void explainMMUTranslation();
    void visualizeKPTI();  // Windows
    void showKASLR();      // Linux
    
    struct Demo {
        std::string concept_name;
        std::string description;
        std::function<void()> demonstration;
        std::vector<std::string> educational_notes;
    };
};
```

**Explicação**: A classe `KernelConceptDemo` organiza demonstrações, como a divisão entre espaços de usuário e `kernel` (`demonstrateAddressSpaceSplit`) e a visualização de **KPTI** (`visualizeKPTI`). Cada demonstração é encapsulada em uma estrutura `Demo`, que inclui um nome, descrição, função executável e notas, facilitando a interação com o usuário.

```cpp
class ComparativeAnalyzer {
public:
    struct PlatformComparison {
        struct Metric {
            std::string name;
            std::variant<double, size_t, std::string> linux_value;
            std::variant<double, size_t, std::string> windows_value;
            std::string interpretation;
        };
        std::vector<Metric> metrics;
        std::string summary;
    };
    
    PlatformComparison compareMemoryArchitectures();
    PlatformComparison compareProcessModels();
};
```

**Explicação**: A `ComparativeAnalyzer` gera relatórios comparativos, como `compareMemoryArchitectures`, que contrastam métricas (e.g., uso de memória, eficiência de processos) entre **Linux** e Windows. A estrutura `PlatformComparison` armazena métricas com valores para cada plataforma.

**Motivação**:

- Demonstrações interativas tornam conceitos abstratos mais concretos.
- Relatórios comparativos ajudam a entender as escolhas de design de cada **Sistema Operacional** .

**Entregáveis**:

- Interface CLI interativa para demonstrações.
- Relatórios comparativos de arquiteturas de memória e processos.

#### Fase 5: Visualizações Avançadas e Monitoramento em Tempo Real

**Por que?**

- Melhorar a experiência de entendimento com visualizações intuitivas e monitoramento dinâmico.
- Permitir que os usuários observem processos e memória em tempo real.

**Como?**

- Implementar `ASCIIVisualizer` para criar representações gráficas de hierarquias e layouts de memória.
- Adicionar monitoramento em tempo real na `EducationalCLI` para exibir atualizações dinâmicas.
- Otimizar visualizações para grandes hierarquias de processos.

**Definição da Classe**:

```cpp
class ASCIIVisualizer {
public:
    std::string createProcessTreeVisualization(const ProcessHierarchy& hierarchy);
    std::string createMemoryLayoutDiagram(const MemoryLayout& layout);
    std::string createKernelUserSplitDiagram(const VirtualAddressSpace& vas);
    std::string createTLBPerformanceChart(const TLBStats& stats);
};
```

**Explicação**: A classe `ASCIIVisualizer` gera representações visuais em arte ASCII, como árvores de processos (`createProcessTreeVisualization`) e diagramas de memória (`createMemoryLayoutDiagram`). Essas visualizações são cruciais para tornar dados complexos acessíveis a estudantes.

**Motivação**:

- Visualizações facilitam a compreensão de dados complexos, especialmente para aprendizes visuais.
- Monitoramento em tempo real demonstra a natureza dinâmica dos **Sistemas Operacionais**.

**Entregáveis**:

- Visualizações ASCII de hierarquias e layouts de memória.
- Interface de monitoramento em tempo real.
- Algoritmos otimizados para visualizações complexas.

### Exemplo de Saída

#### Visualização de Hierarquia de Processos

```shell
init (PID: 1)
├── systemd (PID: 123)
│   ├── sshd (PID: 456)
│   └── cron (PID: 789)
└── kernel_thread (PID: 2, kworker/0:1)
    └── ksoftirqd (PID: 3)
```

- **Explicação**: mostra a relação pai-filho entre processos, destacando processos do sistema e threads do `kernel`.

#### Diagrama de Layout de Memória

```shell
Processo: firefox (PID: 1234)
Espaço de Usuário [0x0000000000000000 - 0x00007FFFFFFFFFFF]
├── Segmento de Texto [0x400000-0x401000] (r-x) /usr/bin/firefox
├── Heap             [0x1234000-0x1235000] (rw-) [heap]
└── Pilha            [0x7fff12346000] (rw-) [stack]
Espaço do `kernel` [0xFFFF800000000000 - 0xFFFFFFFFFFFFFFFF]
├── Mapeamento Direto [0xFFFF888000000000-...]
└── Área de Módulos   [0xFFFFFFFF80000000-...]
```

- **Explicação**: Apresenta as regiões de memória de um processo, incluindo permissões e arquivos associados.

#### Exemplo de Saída Completo

#### **Linux** (Ubuntu 22.04)

```shell
=== Monitor de Hierarquia + Memory Analysis ===
Plataforma: Linux

Enumerando processos...
Encontrados 267 processos

Construindo hierarquia...
=== Árvore de Processos ===

systemd (PID: 1, Children: 145)
├── kthreadd (PID: 2, Children: 89)
│   ├── rcu_gp (PID: 3, Children: 0)
│   ├── rcu_par_gp (PID: 4, Children: 0)
│   ├── slub_flushwq (PID: 5, Children: 0)
│   ├── migration/0 (PID: 12, Children: 0)
│   ├── ksoftirqd/0 (PID: 13, Children: 0)
│   └── watchdog/0 (PID: 14, Children: 0)
├── systemd-journal (PID: 156, Children: 0)
├── systemd-udevd (PID: 189, Children: 0)
├── dbus-daemon (PID: 623, Children: 0)
├── gnome-shell (PID: 1847, Children: 12)
└── firefox (PID: 2341, Children: 8)

Estatísticas:
├── Total de processos: 267
├── Processos de usuário: 89
├── Processos de sistema: 89
└── Threads do kernel: 89

Analisando memória do processo systemd (PID: 1)...
=== Layout de Memória Virtual ===

Modo de endereçamento: 48-bit
KASLR: ATIVO | KPTI: ATIVO
Tamanho da página: 4096 bytes

Espaços de endereçamento:
├── User Space:   0x0000000000000000 - 0x00007FFFFFFFFFFF
└── `kernel` Space: 0xFFFF800000000000 - 0xFFFFFFFFFFFFFFFF

Regiões de memória mapeadas:
├── [0x000055555555-0x000055555578] r-x Text Segment    92 KB
├── [0x000055555777-0x000055555779] r-- Data Segment     8 KB
├── [0x000055555779-0x00005555577A] rw- Data Segment     4 KB
├── [0x00005555577A-0x00005555579C] rw- Anonym s       34 KB
├── [0x000055555B2A-0x000055555B4B] rw- Heap           132 KB
├── [0x00007FFFF7A0-0x00007FFFF7C2] r-x Mapped File    137 KB
├── [0x00007FFFF7F8-0x00007FFFF7FA] r-x vDSO            8 KB
├── [0x00007FFFF7FA-0x00007FFFF7FC] r-- Mapped File     8 KB
├── [0x00007FFFF7FC-0x00007FFFF7FD] rw- Mapped File     4 KB
└── [0x00007FFFFFFDE-0x000080000000] rw- Stack         136 KB

Uso de memória:
├── Virtual total: 563.00 MB
└── Residente: 12.45 MB

Conceitos específicos da plataforma:

Conceitos **Linux** demonstrados:
├── `kernel` threads identificados automaticamente
├── Mapeamento /proc filesystem
├── Detecção de vDSO (virtual Dynamic Shared Object)
├── KASLR (Kernel ASLR): ATIVO
└── KPTI (Meltdown mitigation): ATIVO

Análise concluída com sucesso!
Este programa demonstra diferenças fundamentais entre
arquiteturas de processo **Linux** e Windows.
```

#### **Windows** 11

```shell
=== Monitor de Hierarquia + Memory Analysis ===
Plataforma: Windows

Enumerando processos...
Encontrados 312 processos

Construindo hierarquia...
=== Árvore de Processos ===

System (PID: 4, Children: 89)
├── Registry (PID: 120, Children: 0)
├── MemCompression (PID: 1544, Children: 0)
└── services.exe (PID: 812, Children: 67)
    ├── svchost.exe (PID: 1024, Children: 0)
    ├── spoolsv.exe (PID: 1892, Children: 0)
    └── SearchIndexer.exe (PID: 3421, Children: 2)
├── explorer.exe (PID: 2847, Children: 23)
│   ├── msedge.exe (PID: 4521, Children: 15)
│   └── notepad.exe (PID: 5632, Children: 0)
└── winlogon.exe (PID: 756, Children: 0)

Estatísticas:
├── Total de processos: 312
├── Processos de usuário: 123
├── Processos de sistema: 12
└── Threads do kernel: 0

Analisando memória do processo System (PID: 4)...
=== Layout de Memória Virtual ===

Modo de endereçamento: 48-bit
KASLR: INATIVO | KPTI: ATIVO
Tamanho da página: 4096 bytes

Espaços de endereçamento:
├── User Space:   0x0000000000000000 - 0x00007FFFFFFFFFFF
└── `kernel` Space: 0xFFFF800000000000 - 0xFFFFFFFFFFFFFFFF

Regiões de memória mapeadas:
├── [0x000000140000-0x000000142000] rwx Image          8192 KB
├── [0x000000142000-0x000000144000] rw- Private        8192 KB
├── [0x000000780000-0x000000790000] rw- Mapped        65536 KB
├── [0x00007FF7C0000000-0x00007FF7C0100000] r-x Image  1024 KB
└── [0x00007FFFFFFFE000-0x000080000000000] rw- Private   8 KB

Uso de memória:
├── Virtual total: 82817.00 MB
└── Residente: 256.12 MB

Conceitos específicos da plataforma:
Conceitos **Windows** demonstrados:
├── System processes vs User processes
├── Session isolation
├── Process elevation levels
├── VAS Split: User (0x0 - 0x7FFFFFFFFFFF)
└── KPTI: ATIVO
```

