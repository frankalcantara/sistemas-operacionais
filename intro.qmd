# Desvendando o Invisível: Uma Introdução aos Sistemas Operacionais

::: {.content-hidden when-format="pdf"}
:::{.callout-tip}
Está sem tempo? Leia o [Expresso](.\ex\intro-expresso.html).
:::
:::

Em cada computador, smartphone ou dispositivo inteligente que utilizamos hoje quase como extensões do nosso próprio corpo, existe um software fundamental que orquestra silenciosamente todas as operações: o **Sistema Operacional** , que, assim como o capitão pilota um navio, coordena cada pequeno subsistema da máquina criando harmonia perfeita, integrando os sistemas de hardware e software em um ambiente computacional funcional e eficiente. Contudo, ao contrário do capitão, cuja presença é evidente, inestimável e inevitável, a natureza ubíqua dos **Sistemas Operacionais** torna-os quase invisíveis para a maioria dos usuários. Quando a cuidadosa leitora salva um arquivo, executa um programa ou conecta-se à internet, o faz sem se dar conta dos mecanismos complexos que tornam essas ações possíveis. Mais importante, além de não se dar conta, ela não precisa se preocupar com isso. Por trás da aparente simplicidade das nossas máquinas, reside uma das criações mais sofisticadas e interessantes da engenharia de software: um sistema capaz de gerenciar recursos limitados, coordenar atividades concorrentes, garantir segurança e fornecer uma interface amigável, de acesso simultâneo,transparente e eficiente.

Compreender os **Sistemas Operacionais** não é apenas uma questão de curiosidade acadêmica, mas uma necessidade fundamental para qualquer profissional que deseje trabalhar com tecnologia de forma competente. Esta necessidade é maximizada em tempos de crescente complexidade tecnológica, impulsionada por avanços nos campos da Inteligência Artificial, Computação Quântica e dispositivos móveis. Novas tecnologias estão surgindo e mudando toda a relação que temos com a computação e talvez as bases da nossa sociedade. Eu não sei como serão as máquinas, sistemas e o mercado de trabalho no futuro próximo, mas aposto meu chapéu que estas máquinas usarão **Sistemas Operacionais** sofisticados.

Os **Sistemas Operacionais** formam as pontes entre o hardware bruto e as aplicações que utilizamos, definindo como os recursos computacionais são utilizados e como as tarefas destes sistemas são executadas. Neste ponto, a atenta leitora deve ter percebido que este conhecimento é essencial para o desenvolvimento de softwares eficientes, a resolução de problemas de desempenho e a compreensão das limitações e possibilidades dos sistemas computacionais. Se a tecnologia avança, também avançam os **Sistemas Operacionais**.

Neste texto, vou guiá-la em uma jornada pela evolução histórica dos **Sistemas Operacionais**, desde as primeiras máquinas programáveis até os sistemas modernos que gerenciam servidores e redes complexas. Exploraremos as funções fundamentais que todos os **Sistemas Operacionais** devem realizar, as diferentes perspectivas por meio das quais podemos compreendê-los, e os princípios arquiteturais que orientam o seu design. Nossa meta é construir uma compreensão sólida que sirva como fundação para estudos mais avançados em ciência da computação e engenharia de software. Prepare-se, esta não será uma jornada para fracos de coração ou vontade.

Escrevo com a esperança e ambição de que ao final desta jornada, a persistente leitora não apenas compreenda os **Sistemas Operacionais**, mas que seja capaz de utilizar esse conhecimento para resolver problemas práticos, otimizar sistemas e contribuir para o avanço da tecnologia.

Que o céu esteja azul e que os ventos sejam justos!

## Uma Jornada no Tempo: Evolução Histórica dos Sistemas Operacionais

A história dos **Sistemas Operacionais** foi impulsionada pela constante evolução do hardware, pelas crescentes demandas dos usuários e, principalmente pela criatividade aplicada à solução de novos problemas ou pela busca de novos recursos. Para que a atenta leitora tenha um vislumbre desta história, a linha do tempo apresentada na @fig-timeline1 ilustra as principais eras da evolução dos **Sistemas Operacionais** segundo esse pretensioso autor.

::: {#fig-timeline1}
![Linha de tempo da evolução dos Sistemas Operacionais](/images/timeline_evolutivo_so.webp)

Linha temporal mostrando as eras (1940s-presente) com marcos tecnológicos importantes para o entendimento da evolução dos **Sistemas Operacionais**, destacando os sistemas mais representativos de cada era e as inovações de hardware correspondentes.
:::

A @fig-timeline1 mostra que a evolução dos **Sistemas Operacionais** pode ser dividida em eras, marcadas por inovações tecnológicas, fomentadas por mudanças nas necessidades dos usuários. Por outro lado, a @fig-timeline2 destaca um aspecto importante: a evolução do hardware, que teve impacto direto na evolução dos **Sistemas Operacionais**, seja criando uma nova oportunidade de evolução, seja forçando esta evolução.

::: {#fig-timeline2}
![Evolução do Hardware e Impact nos Sistemas Operacionais](/images/hardware_evolution_svg.webp)

Linha temporal destacando a relação entre a evolução do hardware e a dos **Sistemas Operacionais**.
:::

A visão destas eras é interessante, mas carece de detalhamento. Talvez um resumo das características que atribuímos a cada uma destas eras permita que a atenta leitora possa entender como chegamos aqui.

### O Estágio Nascente: Máquinas Nuas e Programação Direta (1940s - início dos 1950s)

Os primórdios da computação, _a era do bit lascado_, foram caracterizados por máquinas colossais que utilizavam **válvulas termiônicas e painéis de conexão**, em inglês chamados de _plugboard_, operando sem qualquer forma de sistema supervisor, gerenciador ou de controle. Estas máquinas primitivas, verdadeiras máquinas nuas, em inglês _bare machines_, exigiam que os programadores interagissem diretamente com o hardware. Nestas máquinas, cada instrução era codificada manualmente em formato binário e as funções que a máquina deveria executar eram criadas por meio da fiação física interligando circuitos para montar a estrutura capaz de executar as instruções em binário. Assim, se a operação requeria uma soma, o circuito do somador precisava ser montado e incluído no ciclo de processamento, manualmente. As máquinas precisavam ser montadas, fisicamente configuradas, para cada tarefa específica. Nesse tempo, muitos programadores eram especialistas em eletrônica, capazes de entender e manipular o hardware diretamente, atuando em conjunto com matemáticos, capazes de simplificar as equações que seriam executadas.

A atenta leitora deve estar imaginando que esse modo de operação era ineficiente, cansativo e tedioso. Os programadores precisavam se inscrever em uma lista de controle, um diretório, para conseguir direito de usar um intervalo de máquina. Neste intervalo, o programador teria que construir os circuitos que precisaria, incluir manualmente, em binário, as instruções que seriam executadas e, finalmente, executar o programa. A configuração era um processo demorado e propenso a erros. Entretanto, a tecnologia evolui.

A introdução dos cartões e fitas perfurados para a entrada e saída de dados representou uma melhora no processo. Não era mais necessário incluir os comandos manualmente em binário. Entretanto, os circuitos necessários à execução de um programa específico ainda precisavam ser manualmente criados e configurados. Os dados e as instruções entravam e saíam da máquina mais rápido, mas a operação continuava predominantemente manual, tediosa e demorada. O **ENIAC**, um dos computadores desse período, poderia levar dias ou até semanas para ser configurado e programado para a realização de uma tarefa que seria realizada em horas, talvez minutos. A @fig-eniacfoto é uma foto do **ENIAC**.

::: {#fig-eniacfoto}
![foto do eniac mostrando os paineis de cabeamento](/images/ENIAC2.webp)

O **ENIAC**, um dos primeiros computadores programáveis, com painéis de conexão e cartões perfurados. Uma máquina colossal. Imagem artificialmente colorizada (WIKIMEDIA COMMONS, 2025).
:::

::: callout-note
**ENIAC**

O **E**lectronic **N**umerical **I**ntegrator **A**nd **C**omputer, **ENIAC**, é considerado um dos primeiros computadores digitais eletrônicos de grande escala. Este equipamento foi desenvolvido durante a Segunda Guerra Mundial e concluído em 1945. Projetado por [John Presper Eckert](https://pt.wikipedia.org/wiki/John_Presper_Eckert) e [John William Mauchly](https://pt.wikipedia.org/wiki/John_Mauchly) na Universidade da Pensilvânia para ajudar no esforço de guerra. O **ENIAC** foi criado para calcular trajetórias balísticas para o exército dos Estados Unidos da América. No entanto, sua utilidade transcendeu esse propósito inicial, marcando o início da era dos computadores eletrônicos. _Tratava-se de uma máquina colossal, ocupando uma área de aproximadamente $167$ metros quadrados e pesando cerca de $30$ toneladas_. Apesar de seu tamanho e complexidade, talvez graças ao seu tamanho e complexidade, o **ENIAC** foi capaz de realizar cálculos em velocidades sem precedentes na sua época, revolucionando a forma como problemas matematicamente complexos e trabalhosos poderiam ser resolvidos.

Há aqui uma curiosidade interessante. Graças ao trabalho e dedicação de [Kathy Kleiman](https://www.amazon.com/Proving-Ground-Untold-Programmed-Computer/dp/1538718286) nós sabemos que o **ENIAC** foi programado por um grupo de mulheres matemáticas: Kathleen "Kay" (McNulty); Mauchly Antonelli; Jean "Betty" (Jennings) Bartik; Frances "Betty" (Snyder) Holberton; Marlyn Wescoff Meltzer; Frances "Fran" (Bilas) Spence; e Ruth (Lichterman) Teitelbaum. Em 1997 estas mulheres foram reconhecidas pelo Congresso dos Estados Unidos como as primeiras programadoras de computadores do mundo. Elas desempenharam um papel crucial na programação do **ENIAC**, desenvolvendo algoritmos complexos e configurando a máquina para realizar cálculos específicos. O trabalho dessas mulheres pioneiras foi fundamental para o sucesso do **ENIAC** e para o avanço da computação como um todo. No livro, Kleiman conta que, como os matemáticos da universidade não acreditavam na tecnologia, relegaram a máquina às mulheres, as computadoras.

O **ENIAC** utilizava uma tecnologia revolucionária para o seu tempo, baseada em válvulas termiônicas, tubos de vácuo capazes de executar comutações e operações analógicas simples, em vez de componentes puramente mecânicos como os relés. _Com aproximadamente $17.468$ válvulas, $7.200$ diodos de cristal, $1.500$ relés, $70.000$ resistores e $10.000$ capacitores_, o **ENIAC** representava o ápice da engenharia eletrônica mundial. As válvulas termiônicas permitiam que o **ENIAC** realizasse cálculos a uma velocidade muito maior do que qualquer máquina anterior. Mesmo assim, a programação do **ENIAC** ainda era feita por meio de painéis de conexão e chaves manuais, interligando conjuntos genéricos de válvulas para criar os circuitos necessários a uma computação específica. O **ENIAC** estabeleceu as bases para o desenvolvimento de computadores mais avançados e acessíveis, pavimentando o caminho para a revolução digital que viria a seguir.
:::

O **ENIAC** é um marco importante na história da computação. Mas não foi o primeiro computador, nem o único. Outros computadores notáveis dessa era incluem o [**Colossus**](https://en.wikipedia.org/wiki/Colossus_computer) anterior à Máquina de Turing, usado para decifrar códigos durante a Segunda Guerra Mundial, o [**EDVAC**](https://en.wikipedia.org/wiki/EDVAC), que introduziu o conceito de armazenar programas na memória. E, o mais surpreendente de todos, o **Z3**, que graças aos problemas da segunda guerra mundial, ficou esquecido. Relegado à poeira do preconceito e medo.

O filme, O Jogo da Imitação, induz ao erro, dando a impressão que [Turing](https://en.wikipedia.org/wiki/Alan_Turing) construiu sozinho uma supermáquina, o primeiro computador. Na verdade, a grande máquina que a equipe de Turing constrói no filme, e que eles ficcionalmente nomeiam de "Christopher", é uma representação cinematográfica da **Bombe**. O objetivo da **Bombe**, uma máquina eletromecânica, era decifrar os códigos da máquina Enigma. o trabalho de Turing foi, de fato, aperfeiçoar o projeto de uma máquina polonesa anterior para criar a **Bombe** britânica, que foi fundamental para a quebra da Enigma. Foi Turing quem recrutou [Tommy Flowers](https://en.wikipedia.org/wiki/Tommy_Flowers) para o centro de criptoanálise em Bletchley Park. [Max Newman](https://en.wikipedia.org/wiki/Max_Newman), um matemático e colega de Turing, liderava a seção conhecida como "Newmanry", responsável por automatizar a criptoanálise da [cifra de Lorenz](https://en.wikipedia.org/wiki/Lorenz_cipher). o trabalho teórico de Turing sobre computação, notavelmente o seu artigo de 1936 sobre ["Números Computáveis"](https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf) que introduziu o conceito da Máquina de Turing, forneceu a base teórica para a ideia de que uma máquina poderia resolver problemas lógicos complexos. Enquanto isso, completamente isolado pela distância, política e guerra estava [Konrad Zuse](https://pt.wikipedia.org/wiki/Konrad_Zuse). Zuse era um engenheiro civil e queria automatizar os cálculos tediosos e complexos de engenharia, como os de estática para projetos de aeronaves.

O processo de redescoberta e reconhecimento da relevância do [**Z3**](https://en.wikipedia.org/wiki/Z3_(computer)) começou a ganhar força na década de 1990. Um marco importante ocorreu após a morte de Konrad Zuse, em 1995, quando um renovado interesse em seu trabalho reacendeu os debates sobre qual foi o primeiro computador da história. Finalmente, em 1998, foi demonstrado que o **Z3** era Turing-completo. Contudo, para isso foi preciso um pequeno truque de programação descoberto por [Raúl Rojas](https://en.wikipedia.org/wiki/Ra%C3%BAl_Rojas) em 1998, já que o **Z3** não possuía uma estrutura de desvio condicional. Com isso foi provado que o **Z3** era capaz de realizar qualquer cálculo que um computador moderno pode fazer, desde que devidamente programado e com tempo suficiente. Essa demonstração solidificou a posição do **Z3** como um avanço fundamental na evolução da computação. Para muitos, o primeiro computador moderno.

::: callout-note
O Zuse **Z3**

O Zuse **Z3**, criado pelo engenheiro civil alemão [Konrad Zuse](https://pt.wikipedia.org/wiki/Konrad_Zuse) em 1941, é reconhecido como o primeiro computador programável e totalmente automático do mundo. Desenvolvido em Berlim, o **Z3** representou uma inovação significativa na capacidade computacional, utilizando relés eletromecânicos para realizar cálculos complexos com entrada e saída de dados automáticas usando fitas perfuradas. Zuse construiu o **Z3** para resolver problemas de engenharia, notadamente problemas de mecânica estática. Existem registros de que o **Z3** tenha sido usado em cálculos estruturais e aerodinâmicos.

O **Z3** utilizava cerca de $2.600$ relés eletromecânicos para realizar suas operações lógicas e aritméticas, uma tecnologia avançada para a época, porém limitada em comparação com os computadores puramente eletrônicos que surgiriam nos EUA e Reino Unido. O **Z3** operava com uma frequência de _clock_ de aproximadamente $5 Hz$, cinco pulsos por segundo, o que, embora lento pelos padrões atuais, era uma conquista notável para a tecnologia da época. Este projeto introduziu conceitos fundamentais da computação moderna, como a separação entre programa e dados. Além disso, o **Z3** era capaz de realizar operações de ponto flutuante armazenando dados em memória. Uma memória limitada, mas ainda assim, suficiente para resolver eficientemente as tarefas da época. O **Z3** foi destruído durante um bombardeio aliado em 1943.
:::

Observe que o **ENIAC** usava válvulas termiônicas, enquanto o **Z3** utilizava relés eletromecânicos. Isto parece implicar que o **ENIAC** era muito mais rápido. Contudo, o **Z3** era montado por fitas perfuradas, enquanto o **ENIAC** utilizava painéis de conexão. O que significa que o **Z3** era mais flexível e fácil de programar. Ou seja, este pobre autor acredita que o tempo entre a definição do problema e a solução do mesmo era menor no **Z3** do que no **ENIAC**. O que, em última análise, é o que importa. A @fig-eniacz3 apresenta as características do **ENIAC** em comparação com o **Z3**.

::: {#fig-eniacz3}
![gráfico de blocos mostrando os sistemas dos dois computadores](/images/eniac_z3_comparison_svg.webp)

Comparação entre o **ENIAC** e o **Z3**, destacando as diferenças em tecnologia, programação e velocidade.
:::

Nesta altura da nossa jornada, a atenta leitora deve focar em compreender que, **mesmo sem qualquer sistema de gerência de hardware ou software, os computadores já eram capazes de realizar tarefas complexas**. No entanto, a falta de abstração e automação tornava o processo trabalhoso, dolorosamente tedioso  e propenso a erros. Nesse ponto da história, parece inevitável perceber que existe a necessidade de uma camada extra de tecnologia, entre o hardware e o software, que permitisse automatizar as tarefas básicas de operação, como configurar os circuitos necessários para resolver problemas computacionais, sem a necessidade de intervenção manual constante. E começamos a pensar em lotes. Digo, `batches`.

### A Revolução Batch: Automatizando o `throughput` (final dos 1950s - meados dos 1960s)

::: callout-note
**Throughput é uma palavra horrível**

A palavra `throughput` da língua inglesa, não tem uma tradução direta para o português. Pode ser entendida como vazão, taxa de transferência ou capacidade de processamento dependendo da área da ciência onde é aplicada. No contexto de **Sistemas Operacionais**, **refere-se à quantidade de trabalho que um sistema computacional pode realizar em um determinado período de tempo**.trata-se de uma métrica importante para avaliar a eficiência e o desempenho de um sistema, especialmente em ambientes de computação nos quais múltiplas tarefas são executadas simultaneamente.

Neste livro, eu vou usar o termo `throughput` livremente, na esperança de que Cecília Meireles e Fernando Pessoa me perdoem o estrangeirismo. Pecado que, inevitavelmente, cometerei muitas vezes ao longo deste livro. Pobre autor pecador. A compassiva leitora há de ter paciência comigo.

Como o desempenho de sistemas é um conceito complexo, para que a esforçada leitora possa entendê-lo, vamos começar esclarecendo uma confusão comum. No dia a dia, usamos o termo **velocidade** de forma genérica, mas ele, na verdade, é o resultado de duas métricas distintas: `throughput` e `latência`. Esta distinção fica evidente quando saímos da computação para a fabricação. Considere uma fábrica de carros. A **velocidade** percebida da fábrica depende de dois fatores: O `throughput` representa quantos carros a fábrica produz por dia, a vazão total. A `latência` indica quanto tempo leva para que um carro específico passe por toda a linha de montagem, do início ao fim.

Uma fábrica pode ter um `throughput` altíssimo, produzindo milhares de carros por dia, mas uma `latência` elevada, onde cada carro individualmente leva semanas para ficar pronto. Ambas as métricas são essenciais para entender a real eficiência da operação.
:::

Estamos na era dos transistores. _A substituição das válvulas por transistores tornou os computadores menores, mais confiáveis, rápidos e práticos_. Apesar disso, as máquinas da época eram extremamente caras e, por isso mesmo, gerenciadas centralmente. Estas grandes máquinas ficaram conhecidas como **mainframes**.

O termo em inglês `main frame`, escrito assim: com espaço entre as palavras, era usado para descrever a estrutura física principal que abrigava os componentes centrais de um computador, como a `CPU`, em inglês **C**entral **P**rocessing **U**nit, e a memória. Nos primeiros computadores de grande porte, os componentes de uma máquina eram montados em grandes gabinetes metálicos, chamados em inglês de `frames`. Neste caso, o `main frame` era o mais importante desses gabinetes. Com o tempo, o termo passou a ser escrito sem espaços como `mainframe` e passou a designar não apenas a estrutura física central, mas todo o sistema computacional de grande porte.

Os `mainframes` eram operadas por equipes de especialistas e utilizados principalmente para tarefas vitais e estratégicas em grandes organizações, como bancos, institutos militares e universidades. Essas máquinas enfrentavam um problema importante: a **subutilização da unidade central de processamento**.

A `CPU` ficava ociosa enquanto esperava por operações de Entrada/Saída (`E/S`),  pela conclusão de outros processos, resultando em desperdício de recursos. Tanto na era dos `mainframes` quanto nos dias atuais, as operações de `E/S` são os pontos de menor velocidade e `Throughput` e de maior latência em todo o processo computacional. Entretanto, em um `mainframe`, deixar a `CPU` parada representava um custo muito alto. A solução emergiu na forma de **Sistemas Batch**.

Os sistemas `batch`, uma palavra em inglês que pode ser traduzida por lote, apresentavam características distintas das abordagens anteriores usadas na computação. Uma das suas principais inovações era a capacidade de agrupar tarefas com necessidades similares, formando lotes,  `batchs` de processamento, que eram executados de maneira sequencial, permitindo a utilização mais eficiente dos recursos computacionais. _Esses sistemas contavam com um monitor residente, um componente precursor dos **Sistemas Operacionais** modernos, que tinha a função de automatizar o sequenciamento dos trabalhos, eliminando a necessidade de intervenção manual anterior à execução de cada tarefa_.

Observe, atenta leitora, a sentença: **... eliminando a necessidade de intervenção manual anterior à execução de cada tarefa**. Aqui está o segredo do sucesso da arquitetura `batch`. Entretanto, a evolução tecnológica abriu caminhos para a automação do processo de execução de tarefas, permitindo que os sistemas `batch` fossem programados para executar automaticamente uma sequência de tarefas sem intervenção manual. Aqui surgem as linguagens de programação de domínio específico.

Para controlar esse processo, foi desenvolvida a linguagem **JCL**, em inglês **J**ob **C**ontrol **L**anguage, em português: linguagem de controle de trabalhos. A **JCL** é uma linguagem específica, hoje chamamos de linguagem de domínio específico, que permitia instruir um sistema de monitoramento para processar os trabalhos computacionais definindo parâmetros e sequências de execução. Em resumo, a **JCL** era uma linguagem que permitia programar a ordem de execução dos trabalhos, permitindo, por exemplo, priorizar ou excluir trabalhos de acordo com os resultados anteriores. Além disso, os sistemas `batch` introduziram o conceito de processamento `offline`, Aqui, o termo `offline` está relacionado com a saída dos trabalhos estar direcionada para fitas magnéticas, muito mais rápidas que as impressoras. O sistema grava em fita magnética, muito rápido, e volta à `CPU` para outra tarefa. Desta forma, libera memória, já que deixa a impressora lendo a fita e imprimindo no seu próprio ritmo. Os sistemas `batch`representaram um avanço na automação da operação dos computadores, é possível programar os trabalhos computacionais, aumentando consideravelmente a utilização da `CPU` e o `throughput` dos sistemas. A @fig-throughput ilustra essa evolução em uma situação fictícia.

::: {#fig-throughput}
![Diagrama mostrando uma situação fictícia de aumento de throughput](/images/throughput-comparison.webp)

Diagrama mostrando uma situação fictícia de aumento de throughput. Mais trabalhos realizados em menos tempo. Nessa ilustração simples conseguimos $4\times$ mais `throughput` com o mesmo hardware!
:::

#### Sistemas influentes da Era Batch

Dois Sistemas `batch`, merecem destaque:

1. **FMS**, em inglês, **F**ortran **M**onitor **S**ystem. O **FMS** foi um dos primeiros sistemas de monitoramento desenvolvido especificamente para programas escritos em **FORTRAN**, em inglês: **FOR**mula **TRAN**slation, uma das primeiras linguagens de programação, utilizada para aplicações científicas e de engenharia de alto desempenho. O **FMS** permitia que _múltiplos programas **FORTRAN** fossem executados em sequência sem a necessidade de intervenção manual entre cada execução_. O **FMS** introduziu os conceitos básicos de gerenciamento de tarefas e alocação de recursos, que se tornariam fundamentais para os **Sistemas Operacionais**. O **FMS** facilitava a compilação e execução de programas **FORTRAN**, tornando o processo de desenvolvimento mais eficiente e menos propenso a erros.

::: callout-note  
Nesta terceira década do século XXI, o **FORTRAN** é amplamente utilizado em áreas onde o desempenho computacional é essencial.trata-se de uma linguagem muito comum em simulações numéricas e aplicações científicas, como no [VASP](https://www.vasp.at/wiki/index.php/The_VASP_Manual), e [Quantum ESPRESSO](https://www.quantum-espresso.org/), usados em química e física computacional. Na meteorologia e climatologia, modelos como o WRF, em inglês [Weather Research and Forecasting Model](https://github.com/wrf-model/) e o UFS, em inglês [Unified Forecast System](https://github.com/ufs-community/ufs-weather-model/wiki). Agências como a [NASA](https://www.nasa.gov/), a [NOAA](https://www.noaa.gov/) e o [IMPE](https://www.gov.br/inpe/pt-br) mantêm códigos extensos em **FORTRAN**, especialmente em simulações aeroespaciais e modelagem climática. Por fim, é preciso não esquecer que o **FORTRAN** é amplamente usado em computação de alto desempenho (HPC), com suporte a paralelismo via [MPI](https://www.open-mpi.org/) e [OpenMP]. Sua permanência como linguagem relevante se deve ao alto desempenho em cálculos numéricos, à grande base de código legado e à maturidade de suas bibliotecas científicas. Coloque ênfase em maturidade.
:::

2. **IBSYS**: sistema `batch` para o **IBM 7094**. Este sistema de monitoramento e gestão estabeleceu alguns conceitos que permanecem importantes no processo computacional. O **IBSYS** introduziu técnicas de gerenciamento de memória e escalonamento de tarefas, permitindo que múltiplos trabalhos fossem processados de maneira eficiente. Além disso, implementou mecanismos de proteção de memória, garantindo que um programa não interferisse na execução de outros. Este isolamento de memória é um conceito fundamental e indispensável para a estabilidade e confiabilidade dos sistemas computacionais. Complementarmente, o **IBSYS** oferecia suporte a dispositivos de entrada e saída diversos, incluindo leitores de cartões, impressoras e unidades de fita magnética, permitindo mais flexibilidade na manipulação de dados.

Os sistemas `Batch` foram concebidos para maximizar a utilização da `CPU` e aumentar o `throughput`. _A execução de mais tarefas em um mesmo intervalo de tempo torna as máquinas economicamente viáveis_. Depois de voltar à definição de `throughput`, só por via das dúvidas, a esforçada leitora deve registrar que esta era marcou o **primeiro passo na automação dos processos de operação das máquinas de computação** e impulsionou os conceitos de abstração que usamos hoje para representar o hardware.

### Malabarismo de Recursos: O Advento da multiprogramação (meados dos 1960s - 1970s)

A introdução dos circuitos integrados marcou outro passo significativo na evolução dos sistemas computacionais, empurrando o desenvolvimento dos **Sistemas Operacionais**, resultando em computadores mais poderosos, compactos e acessíveis. Nesta era de circuitos integrados, mesmo com a eficiência aprimorada dos sistemas `batch`, um problema persistia: a `CPU` permanecia ociosa durante as operações de entrada e saída de dados (`E/S`). O gargalo devido à velocidade dos dispositivos de entrada e saída resistia.

Nesse contexto surgiu a **multiprogramação**. **Essa técnica propõe manter múltiplos processos na memória principal simultaneamente**. A ideia era simples, mas transformadora: se um programa em execução precisasse realizar uma operação de `E/S`, um sistema de gestão poderia rapidamente comutar a `CPU` para outro programa que estivesse pronto para ser executado, em vez de esperar o término da lenta operação de `E/S`. Multiprogramação, vários programas em memória e sendo colocados em execução sempre que a `CPU` estivesse ociosa. Parece bom.

A abordagem da multiprogramação aumentou drasticamente a utilização da `CPU`, reduziu o tempo ocioso e revolucionou a forma como os recursos computacionais são gerenciados. 

Antes de continuarmos, podemos fazer uma pequena definição: **chamamos de processo os programas que estejam em memória, estejam sendo executados, ou não**. Depois, voltaremos a essa definição com mais profundidade. Por agora basta para continuarmos.

#### Características Fundamentais da multiprogramação

A multiprogramação introduziu conceitos que se tornariam os pilares dos **Sistemas Operacionais** modernos. Notadamente a capacidade de manter vários programas carregados simultaneamente na memória principal. Diferentemente dos sistemas `batch`, onde apenas um programa residia na memória por vez, a multiprogramação permitia que múltiplos programas coexistissem na memória principal, aguardando sua vez de utilizar a `CPU`. Neste contexto os processos só cedem controle da `CPU` **voluntariamente** ou quando se tornam bloqueados devido a operações de `E/S`. Chamamos essa regra de comutação de **Troca não-preemptiva** indicando que não há interrupção forçada por um fator temporal. No paradigma da multiprogramação o sistema simplesmente aproveita os momentos naturais de espera para maximizar a utilização dos recursos computacionais. Esta estratégia maximiza a utilização da `CPU`. Quando um processo executa uma operação de `E/S` e se torna bloqueado, o **Sistema Operacional** seleciona automaticamente outro processo que esteja pronto para execução. Um exemplo de como este _chaveamento de processos_ em memória, chamado em inglês de _context switching_ pode ser visto na @fig-multiprogramação.

::: {#fig-multiprogramação}
![](/images/multiprocessing-diagram.webp)

Funcionamento da multiprogramação com _context switching_, a troca de processos em memória. Demonstração do ciclo de execução onde Processo A bloqueia em operação de `E/S`(t1), o agendador do **Sistema Operacional** realiza a troca de contexto para Processo B (t2-t3), e Processo A retorna à fila de processos prontos após conclusão do `E/S` (t4-t5).
:::

::: callout-note
O termo **preemptivo** chegou ao português a partir do inglês `preemptive`. Este termo tem origem na expressão latina _praeemptio_ que significa: comprar antes. Usada quando uma pessoa tem direito de preferência na compra de um bem. A partir do século XIX o sentido de `preemptive` se expandiu para um sentido mais geral: agir antes. Durante a guerra fria a expressão `preemptive strike` se popularizou significando um ataque feito para impedir que o inimigo realize um ataque iminente. No contexto dos **Sistemas Operacionais**, o termo `preemptive` refere-se a uma técnica onde o **Sistema Operacional** pode interromper um processo em execução para dar prioridade a outro processo, garantindo que todos os processos tenham uma chance justa de serem executados. Há um verbo se esforçando para emergir dos confins do estrangeirismo relacionado a esta palavra no português: preemptar. Eu vou usar preemptivo, mas me recuso a usar preemptar como verbo. Deus me livre!
:::

Um fator interessante de multiprogramação é que sua eficácia pode ser modelada matematicamente. Se um processo gasta uma fração $p$ do seu tempo esperando por operações de `E/S`, a probabilidade de $n$ processos, todos residentes na memória, estarem simultaneamente esperando por `E/S` é $p^n$. No paradigma da multiprogramação a `CPU` só estará ociosa se todos os processos estiverem esperando. Portanto, a utilização da `CPU` é a probabilidade de que pelo menos um processo não esteja esperando por `E/S`, o que pode ser expresso pela fórmula:

$$\text{Utilização da CPU} = 1 - p^n$$

Nesta equação, temos:

- $p$ representa a fração de tempo que um processo gasta em operações de `E/S`;
- $n$ é o número total de processos mantidos na memória.

::: callout-note
**Demonstração Prática: O Poder da multiprogramação**

A fórmula $\text{Utilização da CPU} = 1 - p^n$ pode parecer abstrata, mas seus resultados são impressionantes. Vamos considerar um cenário fictício. Neste cenário, os processos gastam $50\%$ do tempo em `E/S`. Ou seja, $p = 0.5$. Logo:

| Processos (n) | $p^n$ | Utilização da `CPU` | Melhoria |
|:-------------:|:-----:|:-----------------:|:--------:|
| 1 | $0.5$ | $50\%$ | - |
| 2 | $0.25$ | $75\%$ | $+50\%$ |
| 4 | $0.0625$ | $93.75\% | $+87.5\%$ |
| 8 | $0.0039$ | $99.61\%$ | $+99.2\%$ |

: Simulação do ganho de `throughput` com a multiprogramação. {#tbl-simul}

**Observação Importante**: com apenas $4$ processos na memória, é possível obter quase $94\%$ de utilização da `CPU`, mesmo quando cada processo passa metade do tempo esperando `E/S`! Considere, no entanto, que há uma limitação prática neste cenário: esta análise assume que sempre há pelo menos um processo pronto para executar e não considera o custo computacional da troca de contexto. Esse custo é o custo inerente, na forma de ciclos de máquina e acessos à memória, para tirar um processo da `CPU` e colocar outro.
:::

A esperta leitora deve observar que, mesmo com um valor de $p$ relativamente alto, por exemplo o valor de $0.5$ que usamos para indicar que os processos passam metade do tempo em `E/S`, aumentar o número de processos $n$ na memória faz com que o termo $p^n$ diminua, levando a utilização da `CPU` para perto de $100\%$. A @fig-multiprog1 representa claramente a ideia de multiprogramação, onde múltiplos processos estão na memória, cada um em diferentes estados (`CPU`, `E/S`, `waiting`), e o cronograma temporal demonstra como a `CPU` alterna entre processos durante operações de `E/S` de outros.

::: {#fig-multiprog1}
![](/images/multiprogramacao_CPU.webp)

Representação da alocação de múltiplos processos em memória e os estados onde eles se encontram.
:::

O advento do chaveamento de contexto, essencial para manter a `CPU` ocupada quando um processo realizava operações lentas de `E/S`, tornou a gestão de memória eficiente um pré-requisito indispensável. Múltiplos processos precisavam ser mantidos na memória simultaneamente, exigindo que o Sistema Operacional os alocasse e protegesse de forma rigorosa. Uma gestão de memória eficaz era, portanto, o alicerce que permitia ao agendador de tarefas realizar a troca de processos de forma rápida e confiável.

Outro avanço desta era que merece nossa atenção foi o _spooling_, em inglês **S**imultane s **P**eripheral **O**peration **O**n-**L**ine**, uma técnica que utiliza o disco como `buffer` intermediário para operações de `E/S`. Isso permitiu que a `CPU` e os dispositivos de `E/S` operassem de forma concorrente, melhorando a eficiência geral do sistema. Um exemplo marcante dessa era é o **OS/360 da IBM**, anunciado em 1964. O **OS/360** era um sistema de multiprogramação que estabeleceu muitos dos conceitos ainda utilizados nos **Sistemas Operacionais** modernos. O termo **OS/360** refere-se a uma família de **Sistemas Operacionais** desenvolvidos pela IBM para sua linha de mainframes **System/360**. Nesta linha de computadores, a IBM introduziu a multiprogramação como um recurso central e fundamental do sistema. O **OS/360** foi projetado para suportar uma ampla gama de aplicações, desde processamento de dados até computação científica, e estabeleceu padrões que influenciaram profundamente o desenvolvimento de **Sistemas Operacionais** subsequentes[^1].

::: callout-note
**Definindo Buffer**
O termo `buffer` é outro desses termos em inglês que tomaram de assalto o vocabulário da computação no Brasil. Talvez, algum pesquisador de língua inglesa tenha se lembrado de uma passagem da infância,  de alguma outra área da vida e trouxe o termo para a computação e hoje vivemos com ele. O termo `buffer` deriva do verbo em inglês antigo `buff`, que significava golpear ou amortecer um golpe. Esse sentido inicial estava ligado à ideia de suavizar ou absorver um impacto físico, como uma pancada. Na física, um `buffer` é um dispositivo que reduz o impacto ou choque, como os amortecedores usados em trens e carros para suavizar colisões com o meio. Em inglês o termo `buffer` também pode se referir a algo,  alguém, que funciona como uma barreira protetora. Em computação um `buffer` é uma área de armazenamento temporário para dados,  instruções, usada enquanto eles estão sendo transferidos entre dois lugares,  processos, distintos. Por exemplo, um `buffer` pode guardar informações de um dispositivo rápido, como um processador, antes de enviá-las para um dispositivo mais lento, como uma impressora, ajudando a equilibrar diferenças de velocidade.
:::

Ó poetas mortos da língua portuguesa, perdoai este pobre autor pelos crimes que comete!

[^1]:O pobre autor começou sua vida profissional em um velho mainframe IBM 360/30, com o OS/360 rodando Cobol, PL/1 e RPG. O sistema era tão antigo que o manual de operação era um livro, com mais de $1.000$ páginas, e o computador tinha apenas $32 KB$ de memória. Era o final dos anos 1970 poucos meses antes deste 360/30 ser descomissionado e substituído por um IBM 370/138 que, usando memória virtual chegava a $16 MBytes$ de memória. Imagine! Hoje, minha máquina está rodando $118$ processos em $2 Gbytes$ de memória apenas para editar o arquivo de textos que estou escrevendo.

A curiosa leitora deve notar que a IBM não criou o termo **Sistema Operacional** , mas foi fundamental para sua popularização. O termo já existia na comunidade de computação antes do lançamento do $OS/360$ pela IBM em 1964. Por exemplo, sistemas como o [GM-NAA `E/S`](https://en.wikipedia.org/wiki/GM-NAA_`E/S`), desenvolvido em 1956, e o [CTSS](https://pt.wikipedia.org/wiki/Compatible__time-sharing__System), descrito em 1962, já eram chamados de **Sistemas Operacionais** em contextos acadêmicos e de pesquisa. No entanto, o **OS/360**, marcou um ponto de virada na história da computação. Deste ponto em diante, **podemos usar o termo **Sistema Operacional** para nos referirmos a um software que gerencia recursos de hardware e fornece serviços essenciais para programas de aplicação**. 

::: callout-note
**Sistema Operacional** : George F. Ryckman utilizou o termo _operating system_ em seu artigo de maio de 1960 intitulado [_The computer operation language_](https://dl.acm.org/doi/pdf/10.1145/1460361.1460406). Neste artigo Ryckman discutiu o **Sistema Operacional** SHARE. Esta parece ser a primeira utilização documentada do termo de uma forma familiar à que utilizamos atualmente. Antes do termo **Sistema Operacional** se estabelecer, esses programas residentes em segundo plano eram frequentemente chamados de _monitors_, _monitor-programs_, _supervisor_, _executive_,  _operating executive_.
:::

A multiprogramação e o _spooling_ estruturaram as bases para a abstração de hardware e a automação do gerenciamento de recursos que usamos hoje. Contudo, a utilização de computadores ainda era árida e limitada a especialistas. Na maior parte das vezes, não havia qualquer interação com o usuário. Um especialista escrevia um programa, outro especialista o rodava e um terceiro entregava os resultados.

### Era da Interatividade: Sistemas de Tempo Compartilhado (final dos 1960s - 1980s) {#sec-timesharing}

Os sistemas de tempo compartilhado, em inglês _time-sharing_, representaram o nascimento da **multitarefa interativa moderna**. Sendo uma evolução natural da multiprogramação, seu foco passou a ser a experiência do usuário, sem abrir mão da eficiência da `CPU`. Esses sistemas revolucionaram a computação ao dividir o tempo da `CPU` entre múltiplos usuários interativos quase simultaneamente, criando um ambiente operacional no qual cada usuário tem a impressão de estar utilizando um computador dedicado exclusivamente a ele.

A abordagem de time-sharing marcou uma mudança significativa de paradigma nos sistemas computacionais. Essa transição foi possível graças à implementação do _time slicing_, fatiamento de tempo, uma técnica na qual cada processo recebe uma pequena fatia de tempo fixa para uso da `CPU`. Essa fatia de tempo é frequentemente chamada de quantum ou, mantendo o termo em inglês, _time slice_. O processo acessa a `CPU` e roda durante este intervalo antes de ser temporariamente suspenso para permitir que outros processos sejam executados. Essa abordagem cria a ilusão de que cada usuário tem acesso exclusivo aos recursos do computador, melhorando significativamente a interatividade e a experiência geral do usuário.

O _time-sharing_ possui características distintivas importantes. A **preempção por tempo** é uma delas: diferentemente da multiprogramação, na qual os processos cedem controle voluntariamente ou quando bloqueiam, o _time-sharing_ introduziu a preempção forçada. Cada processo recebe um quantum de tempo fixo, e quando esse tempo expira, o **Sistema Operacional** interrompe forçosamente o processo e concede a `CPU` ao próximo processo na fila. Outra característica é a **interatividade prioritária**: o objetivo principal mudou de maximizar `throughput`, como na multiprogramação, para fornecer responsividade interativa. O sistema é otimizado para garantir que cada usuário receba tempo de `CPU` de forma rápida e regular. Este quantum de tempo pode ser calculado por:

$$\text{Quantum time} = \frac{\text{Total `CPU` time}}{\text{Number of active processes}}$$

A @fig-time1 ilustra o conceito de time-sharing. 

::: {#fig-time1}
![](/images/time-sharing-diagram.webp)

Sistema de time-sharing com escalonamento round-robin. Demonstração do quantum temporal e preempção forçada, onde múltiplos usuários (A, B, C) compartilham a `CPU` através de fatias de tempo fixas, criando a ilusão de uso exclusivo para cada terminal interativo.
:::

::: callout-note
**Round-Robin Scheduling**

O algoritmo **round-robin** é uma técnica de escalonamento preemptivo na qual cada processo recebe uma fatia de tempo igual para executar em `CPU` antes de ser suspenso e enviado para o final da fila de processos prontos. Para tanto, o agendador mantém uma fila circular de processos. Quando o quantum de um processo expira, ele é interrompido e o próximo processo da fila obtém a `CPU`. Isso garante que todos os processos recebam uma oportunidade equitativa de execução. Usando **round-robin** cada processo recebe o mesmo quantum de tempo graças a preempção forçada quando o tempo expira.

O termo _round-robin_ vem em inglês, mas tem sua origem etimológica ligada ao termo francês _rond ruban_ do século XVII. Uma forma de assinar petições em vários círculos concêntricos para que o primeiro a assinar não pudesse ser identificado e punido. Há outra referência, do século XIX, quando o termo foi adotado para torneios nos quais cada participante competia com todos os outros um número igual de vezes, garantindo que todos tivessem a mesma oportunidade de vencer. Quando os cientistas da computação precisaram de um nome para um algoritmo de escalonamento que tratasse todos os processos de forma igualitária, o termo "round-robin" era a analogia perfeita.

O round-robin garante responsividade e justiça. Nenhum processo monopoliza a `CPU`. Como nem sempre os céus são azuis e os mares estão calmos, o tamanho do quantum é crítico. Muito pequeno causa custos extras de processamento por excesso de chaveamento de contexto. Muito grande reduz a responsividade do sistema. A virtude está no equilíbrio!
:::

#### Distinguindo multiprogramação de Time-Sharing

É importante que a atenta leitora compreenda as diferenças fundamentais entre multiprogramação e _time-sharing_. A @tbl-multiprocessamento_timesharing resume as principais distinções entre os dois paradigmas.

| Aspecto  | multiprogramação  | Time-Sharing  |
|-------------------------|-------------------------------------|---------------------------------------------|
| **Objetivo Principal**  | Maximizar utilização de `CPU` | Garantir responsividade interativa |
| **Troca de Contexto**| Apenas quando processo bloqueia  | Por quantum de tempo **ou** quando bloqueia |
| **Preempção**  | Não| Sim (por tempo)  |
| **Tipo de Usuário**  | `batch`, jobs, processamento em lote | Usuários interativos|
| **Custo Computacional** | Mínimo| Maior (context switching frequente)|
| **Aplicação Ideal**  | Sistemas com alta taxa de `E/S`  | Sistemas multiusuário interativos  |

: Comparação entre multiprogramação e Time-Sharing {#tbl-multiprocessamento_timesharing}

::: callout-note
**Context Switching: O Custo da Interatividade**

O **context switching**, troca de contexto, é o mecanismo pelo qual o **Sistema Operacional** salva o estado atual de um processo e carrega o estado de outro processo. Este processo inclui:

1. **Salvamento do estado**: registradores da `CPU`, _program counter_, _stack pointer_;
2. **Atualização de estruturas**: tabelas de processos, controle de memória;
3. **Carregamento do novo estado**: restauração dos registradores do próximo processo.

O custo computacional do chaveamento de contexto é o preço a ser pago pela interatividade. Em sistemas de _time-sharing_, esse custo é compensado pela melhor experiência do usuário, enquanto na multiprogramação pura, as trocas são minimizadas para maximizar `throughput`.
:::

#### Sistemas Influentes em Time-Sharing

Entre os sistemas de tempo compartilhado mais influentes da história da computação, destacaremos três cujas características são importantes para os objetivos deste pretensioso autor:

1. **CTSS**: o  **C**ompatible **T**ime-**S**haring **S**ystem foi desenvolvido no Massachusetts Institute of Technology (MIT). Este sistema foi pioneiro no uso de compartilhamento de tempo com interrupções, uma técnica que permitia a múltiplos usuários compartilharem os recursos de um computador de maneira operacionalmente mais eficiente. O [CTSS](https://people.csail.mit.edu/saltzer/Multics/CTSS-Documents/CTSS-Documents.html#overview) estabeleceu muitos dos conceitos fundamentais que formaram a estrutura dos **Sistemas Operacionais** interativos modernos, **incluindo mecanismos de alocação de recursos e gerenciamento de processos que garantiam uma experiência de usuário mais responsiva e interativa**.

2. **MULTICS**: o **Mult**iplexed **I**nformation and **C**omputing **S**ervice foi o resultado de um projeto colaborativo entre o MIT, a General Electric e os Bell Labs. O **MULTICS** introduziu os conceitos de memória de nível único, simplificando o gerenciamento de memória, a ligação dinâmica de código, que permitiu maior flexibilidade na execução de programas, e um sistema de arquivos hierárquico, que permitiu a organização de dados de forma mais intuitiva. O [MULTICS](https://web.mit.edu/multics-history/) tinha um forte foco em segurança, introduzindo mecanismos avançados de proteção de dados e controle de acesso. Embora o **MULTICS** tenha tido um sucesso comercial limitado, sua influência no desenvolvimento dos **Sistemas Operacionais** subsequentes foi relevante, estabelecendo padrões que ainda são seguidos.

3. **UNIX**, desenvolvido nos Bell Labs por [Ken Thompson](https://pt.wikipedia.org/wiki/Ken_Thompson) e [Dennis Ritchie](https://en.wikipedia.org/wiki/Dennis_Ritchie), merece destaque especial. Inspirado pelo **MULTICS**, o **UNIX** foi criado com uma filosofia de simplicidade e elegância que o tornou extremamente popular. Diferente de seu predecessor, o [**UNIX**](https://unix.org/) foi escrito na **Linguagem C**, o que lhe conferiu uma portabilidade notável, permitindo que fosse executado em uma grande variedade de plataformas de hardware. O **UNIX** também se destacou por seu ambiente multiusuário e multitarefa, permitindo que múltiplos usuários trabalhassem simultaneamente no mesmo sistema, cada um executando várias tarefas ao mesmo tempo. **Os sistemas **UNIX**, versão de 1993, apresentavam pilhas `TCP/IP` maduras, com `sockets BSD` desde 1983, memória virtual sofisticada com paginação por demanda, sistemas de arquivos avançados e capacidades de computação distribuída por meio de NFS e RPC. Acrescente a isso que o **UNIX** introduziu um sistema de arquivos hierárquico e um `shell` de comando, que oferecia aos usuários uma interface flexível e eficiente para interagir com o sistema**. O **UNIX** estabeleceu um padrão de design que influenciou profundamente o desenvolvimento dos **Sistemas Operacionais** subsequentes, incluindo, obviamente o **Linux**, o **macOS** e até mesmo o **Windows** e continua a ser uma referência importante.

::: {.callout-note}
**O **UNIX** e o C**
O nome `**UNIX**` é uma brincadeira derivada de **Multics**. O **Multics** foi um projeto ambicioso, mas complexo e pesado. Quando Ken Thompson e Dennis Ritchie começaram a desenvolver um sistema mais simples e eficiente, chamaram-no de **UNIX** como um trocadilho, sugerindo algo mais unitário e simplificado. O nome também pode ser interpretado como uma abreviação de _**UNI**ple**X**ed Information and Computing Service_, embora esta interpretação seja mais uma explicação retroativa e racionalização do que a intenção original.
:::

A **Linguagem C** foi criada por Dennis Ritchie na Bell Labs entre 1972 e 1973. A **linguagem C** foi desenvolvida especificamente para facilitar o desenvolvimento do **Sistema Operacional UNIX**. Dennis Ritchie descreveu o **C** como _uma linguagem de implementação de sistema para o nascente **Sistema Operacional UNIX**_. A @fig-unixtimeline ilustra a relação entre o **UNIX** e a **Linguagem C**.

::: {#fig-unixtimeline}
![](/images/unix_c_timeline.webp)

A evolução histórica que levou a **UNIX**, **Linux** e **Windows NT**.
:::

Para facilitar o desenvolvimento do **UNIX**, a **Linguagem C** foi projetada para ser uma linguagem de programação de sistemas, com foco em eficiência, portabilidade e expressividade. Essa linguagem permitiu que o **UNIX** fosse reescrito, de forma mais concisa e legível, facilitando a manutenção e evolução do sistema. Enquanto o **Sistema Operacional** era reescrito na **Linguagem C**, a portabilidade também aumentava, permitindo que o **UNIX** rodasse em diferentes arquiteturas de computador. A **Linguagem C** foi criada com o objetivo de mover o código do `Kernel` **UNIX** da **Linguagem Assembly** para uma linguagem de alto nível, que realizaria as mesmas tarefas com menos linhas de código. Como Dennis Ritchie construiu a **Linguagem C** sobre a **Linguagem B**, a **Linguagem C** herdou a sintaxe concisa de Thompson que possuía uma poderosa mistura de funcionalidades de alto nível com os recursos específicos necessários para criar um **Sistema Operacional** que fosse portável entre diferentes plataformas de hardware. E esta foi a vantagem competitiva do **UNIX**.

O estudo da história do **UNIX** destaca um princípio importante no design de sistemas: **soluções pragmáticas e focadas muitas vezes ganham maior adoção do que aquelas excessivamente ambiciosas e complexas**. A portabilidade do **UNIX**, a capacidade de rodar em hardwares diferentes, foi um divisor de águas, permitindo a disseminação deste **Sistema Operacional** por plataformas de hardware tão diversas quanto _mainframes_ e dispositivos portáteis.

::: {.callout-note}
**Kernel vs Sistema Operacional**

Chamamos de `Kernel` o componente central, e o mais importante do **Sistema Operacional** . A melhor tradução de `kernel` é núcleo. O `Kernel` será a primeira parte do **Sistema Operacional** que será carregada na memória durante o `boot` e conterá as funções necessárias para atuar como uma ponte entre hardware e software. O `Kernel` opera no nível mais baixo do sistema, gerenciando recursos fundamentais como: processos, decidindo qual programa usa a `CPU` e por quanto tempo; memória `RAM`, controlando alocação e proteção entre programas; dispositivos de hardware, por meio de `drivers`; e fornecendo interfaces para que aplicações solicitem serviços do sistema. O `Kernel` é essencialmente invisível ao usuário comum, operando em modo privilegiado para garantir estabilidade e segurança do sistema.

O **Sistema Operacional** , por sua vez, é o conjunto completo de software que inclui o `Kernel` mais todos os componentes que tornam o computador útil e amigável para o usuário final. Além do `Kernel`, um **Sistema Operacional** engloba: interfaces de usuário; ambientes para interpretação de comandos, que chamaremos de `shell`; sistema de arquivos para organização de dados; utilitários de sistema como gerenciadores de arquivos e painéis de controle; e bibliotecas de funções de sistema que fornecem APIs, em inglês **A**pplication **P**rogramming **I**nterface, para desenvolvimento de aplicações. Todos esses componentes trabalham em conjunto, geralmente invisíveis, para criar uma experiência coesa, funcional e eficiente.

_Pessoas mais inteligentes que eu dizem que a tecnologia e a civilização avançam em rampas e degraus. Existe sempre uma rampa positiva de crescimento com inclinações variáveis e diferentes. De tempos em tempos, uma inovação,  descoberta significativa, ocorre criando um salto qualitativo, o degrau, na capacidade tecnológica. O par **UNIX** e **C** é, claramente, um destes degraus._
:::

### A Democratização da Computação: Era dos Computadores Pessoais (final dos 1970s - presente)

A invenção e popularização dos **microprocessadores**, impulsionadas pelos avanços em **L**arge **S**cale **I**ntegration, **LSI**, a expressão em inglês para integração em larga escala, e em **V**ery **L**arge **S**cale **I**ntegration, **VLSI**, integração em escala muito grande em inglês, levaram ao surgimento de computadores pessoais acessíveis, em quase todo o mundo. No Brasil, no final dos anos 1980, era mais fácil importar um computador que comprar um telefone.

As novas tecnologias permitiram a miniaturização e a redução de custos, tornando possível a criação de computadores suficientemente baratos para caberem em um gabinete pouco maior que uma caixa de sapatos e serem adquiridos por indivíduos e pequenas empresas. Esses computadores pessoais,  **PCs** como a IBM chamava, eram, e são, milhares de vezes mais baratos que os `mainframes`. Estes computadores eram portáteis, substituíveis e capazes de evoluir muito mais rapidamente. De repente, os computadores estavam em todos segmentos da sociedade, da contabilidade à pesquisa científica, da torradeira ao avião. A popularização dos **PCs** levou ao desenvolvimento de **Sistemas Operacionais** mais acessíveis e amigáveis. Neste ponto, o laço de realimentação positiva, o hardware evolui e requer software mais sofisticado, e o software sofisticado requer hardware mais potente, se torna evidente. O ciclo de desenvolvimento de **Sistemas Operacionais** e hardware acelera. Tornando a computação ubíqua e quase tão indispensável quanto o ar que respiramos. Nuvens negras apareceram no horizonte. Em um dado momento, o ciclo de evolução contínua atingiu uma barreira. 

Circuitos integrados, mais rápidos e menores, encontraram um limite físico/econômico. Máquinas menores e mais rápidas implicavam em altas velocidades de `_clock_`, relógio em inglês. O `_clock_` é o sinal que comuta todos os dispositivos da `CPU` e sincroniza as operações do sistema como um metrônomo ajuda o estudante de música a seguir o ritmo. Aumentar a frequência do `_clock_` aumenta o calor gerado. O calor é um inimigo mortal da eletrônica. Circuitos derretem e pegam fogo. Na virada do milênio, o aumento da temperatura exigia sistemas de resfriamento mais complexos, aumentando o custo e a complexidade dos sistemas. Neste ponto, a miniaturização dos circuitos integrados tornou-se cada vez mais difícil devido às limitações físicas dos materiais semicondutores. O limite estava próximo dos $3 Ghz$. Uma barreira a ser  transposta ou um beco sem saída? A resposta estava no horizonte, mas não era óbvia.

Neste ponto da história, o processamento em paralelo, várias máquinas trabalhando juntas, já era uma solução viável. No entanto, o processamento em paralelo exigia que os programas fossem escritos de forma a tirar proveito dessa capacidade, o que não era trivial. A maioria dos programas existentes não era projetada para serem executados em múltiplos processadores simultaneamente. Além disso, as máquinas precisavam ser interligadas por redes físicas o que limitava a velocidade do processamento. Alguém pensou: já que podemos integrar circuitos em uma escala tão grande, por que não colocar duas `CPUs` no mesmo chip?

Entram em cena os _chips multicore_.

Os _chips multicore_, como são popularmente conhecidos, integram dois ou mais núcleos de processamento colocados em um único circuito integrado, rodando a velocidades mais baixas, esquentando menos, porém com um `throughput` maior. Novamente, a evolução do hardware exigiu uma evolução do software mudando a forma como os programas eram escritos e executados. O software precisava ser adaptado para tirar proveito dos múltiplos núcleos de processamento, o que exigia novas técnicas de programação e algoritmos de escalonamento mais sofisticados. A @fig-multicore ilustra a evolução dos processadores multicore.

::: {#fig-multicore}
![](/images/multicore_timeline.webp)

Evolução dos microprocessadores multi-core de 2001 a 2025, demonstrando a transição da era single-core para sistemas many-core. A linha do tempo destaca marcos tecnológicos fundamentais desde o IBM POWER4, primeiro dual-core comercial, até as tendências futuras com arquiteturas híbridas, _chiplet design_ e integração de aceleradores de Inteligência Artificial. As fases evolutivas evidenciam a progressão do número de cores, redução dos processos de fabricação, de 180nm para 3nm, e o aumento exponencial da densidade de transistores, consolidando o paradigma de paralelismo em hardware moderno.
:::

#### A Era dos Multiprocessadores: Paralelismo Real

Primeiro, a IBM. Lançado em 2001, o **IBM POWER4** foi o primeiro microprocessador a integrar dois núcleos de processamento em um único chip de silício. Ele foi projetado para uso em servidores e sistemas de alta performance, muito antes da tecnologia se tornar padrão em desktops e notebooks. A arquitetura do **POWER4** apresentava características avançadas, como o suporte a multiprocessamento simétrico, em inglês **S**ymmetric **M**ulti**P**rocessing, **SMP**, no próprio circuito integrado, permitindo que os dois núcleos trabalhassem de forma conjunta e eficiente em tarefas complexas. Quando a IBM lançou esta tecnologia voltada para o mercado empresarial, sinalizou uma oportunidade para a Intel.

A Intel deu o primeiro passo na era da computação multicore para o consumidor final em maio de 2005, com o lançamento do processador **Pentium D** com o codenome _Smithfield_. O **Pentium D** representou a resposta da Intel à crescente demanda por maior desempenho e capacidade de multitarefa imposta por um número cada vez maior de usuários de computação. A abordagem inicial da Intel com o **Pentium D** consistiu em unir dois núcleos de **Pentium 4** em uma pastilha de silício, permitindo que o **Sistema Operacional** e os aplicativos executassem múltiplos processos simultaneamente. Pouco tempo depois, em janeiro de 2006, a Intel lançou a linha **Core Duo**, com o codinome `Yonah`. Embora lançado posteriormente, o **Core Duo** é frequentemente considerado o primeiro processador multicore verdadeiro da Intel, devido à sua arquitetura mais integrada e eficiente em comparação com o **Pentium D**. No **Core Duo**, os dois núcleos compartilhavam o mesmo `_cache_ L2`, o que permitiu uma comunicação mais rápida e eficiente entre eles, resultando em um desempenho superior e menor consumo de energia. E temos o mesmo **throughput** das máquinas anteriores, mas com menos calor e consumo de energia.

Com o avanço da tecnologia de semicondutores e a busca por maior performance, surgiu uma nova dimensão na computação: **multiprocessamento**. Diferentemente da multiprogramação e do time-sharing, que simulam execução simultânea em processadores únicos, o multiprocessamento oferece **paralelismo verdadeiro** através de múltiplas unidades de processamento físicas. Neste novo paradigma temos **Paralelismo real**: múltiplas instruções podem executar **simultaneamente** em processadores diferentes, não sequencialmente como nos sistemas anteriores. Cada processo pode ter acesso exclusivo de um processador físico separado, eliminando a necessidade de compartilhamento temporal da `CPU`. Dessa forma,  implicando em um aumento de performance, de forma quase linear, em relação ao número de processadores. A otimista leitora deve se segurar. Não é tão bom assim. O ganho de eficiência, velocidade e `throughput` é grande, mas existem limites arquiteturais que estudaremos depois. A @fig-coreduo ilustra a relação entre as tecnologias da Intel para estes circuitos integrados.

::: {#fig-coreduo}
![](/images/evolucao_processadores.webp)

A figura mostra a progressão arquitetural do Pentium 4 single-core para o **Pentium D** dual-core e **Core Duo**. O **Pentium D** implementou multiprocessamento através de duas pastilhas de silício separadas com _caches L2_ independentes e comunicação limitada à velocidade do barramento compartilhado, resultando em alta latência inter-cores e consumo energético elevado, 95-130W. Em contraste, o **Core Duo** introduziu uma pastilha de silício unificada com _Smart _cache_ L2_ compartilhado de $2 MB$ e interconexão direta entre _cores_, estabelecendo os fundamentos para o multiprocessamento eficiente que exigiu novos algoritmos de agendamento e gerenciamento de memória nos **Sistemas Operacionais** modernos.
:::

Agora que a atenta leitora já foi apresentada ao conceito de multiprocessamento, vamos comparar os três paradigmas fundamentais: multiprogramação, time-sharing e multiprocessamento. A @tbl-multishamulti resume as principais diferenças entre estes três paradigmas:

| Aspecto| Multiprogramação| Time-Sharing | Multiprocessamento|
|-----------------------|-----------------------------|-----------------------------------|-------------------------------|
| **Hardware** | Processador único  | Processador único  | Múltiplos processadores |
| **Execução** | Sequencial cooperativa| Sequencial preemptiva | Paralela real  |
| **Troca de contexto** | Apenas quando bloqueado  | Por quantum de tempo  | Comunicação entre CPUs  |
| **Preempção**| Não | Sim | Sim (aplicada em cada processador)  |
| **Objetivo** | Utilização eficiente de `CPU` | Responsividade interativa| Máximo `throughput`  |
| **Overhead** | Mínimo | Context switching frequente | Sincronização entre processos |
| **Aplicação**| Sistemas `batch`,com `E/S`  | Sistemas interativos multiusuário | Computação paralela intensiva |

: Comparação entre Multiprogramação, Time-Sharing e Multiprocessamento. {#tbl-multishamulti}

Os paradigmas que vimos até o momento são didaticamente interessantes e históricos, mas não são mutuamente exclusivos. Os sistemas modernos implementam uma combinação desses paradigmas para otimizar o desempenho e a responsividade resultando em sistemas híbridos que aproveitam o melhor de cada abordagem.

#### Sistemas Híbridos: A Convergência dos Paradigmas

Os **Sistemas Operacionais** modernos combinam elementos de multiprogramação, time-sharing e multiprocessamento para otimizar o desempenho e a responsividade. Essa convergência é essencial para atender às demandas crescentes de computação em ambientes complexos que vão desde servidores de alta performance até dispositivos móveis portáveis e vestíveis.

A evolução dos paradigmas de computação provoca a emersão do conceito de **S**ymmetric **M**ulti**P**rocessing, **SMP**, como uma solução híbrida que combina os três paradigmas fundamentais. O **SMP** permite que múltiplos processadores compartilhem a mesma memória e os mesmos recursos do sistema, enquanto cada processador executa processos de forma independente e simétrica. Diferentemente de arquiteturas assimétricas onde processadores têm funções específicas, no **SMP** todos os processadores são funcionalmente idênticos e podem executar qualquer tarefa do **Sistema Operacional** , incluindo rotinas do `kernel`.

Esta arquitetura simétrica possibilita que o **Sistema Operacional** implemente tanto multiprogramação quanto _time-sharing_ em cada processador individual, criando uma matriz de execução onde $n$ processadores podem simultaneamente gerenciar $m$ processos através de fatias de tempo. O resultado é uma otimização multiplicativa do uso da `CPU` que melhora dramaticamente a responsividade do sistema e o `throughput` geral. Isso, porém, cria novos desafios de software para os **Sistemas Operacionais**. Entre eles estão o agendamento de tarefas, o balanceamento de carga e sincronização e coerência de dados.

1. **Agendador de Tarefas em Sistemas Híbridos**: o **agendador de tarefas**, em inglês _task scheduler_ em sistemas **SMP** representa uma evolução significativa em relação aos agendadores que existiam nos sistemas tradicionais com apenas um núcleo de processamento. Enquanto agendadores clássicos como o `Round Robin` ou o `Shortest Job First`, expressão em inglês para o trabalho menor primeiro, operavam em uma única fila de processos prontos. Os agendadores **SMP** devem coordenar múltiplas filas de execução distribuídas entre processadores. Em linhas gerais, já que vamos estudar isso com profundidade mais adiante, o agendador **SMP** deve lidar com as seguinte hierarquia de decisões:

   - **Escalonamento Local**: cada processador mantém sua própria fila de processos prontos e aplica algoritmos de _time-sharing_, tais como **C**ompletely **F**air **S**cheduler, **CFS**, o agendador antigo do **Linux**, para determinar qual processo executar durante o próximo quantum de tempo;

   - **Escalonamento Global**: o sistema periodicamente avalia a distribuição de carga entre processadores e toma decisões de migração de processos para equilibrar a utilização;

   - **Escalonamento de Afinidade**: o agendador considera a afinidade de `_cache_` e memória, preferindo manter processos no mesmo processador em que foram executados recentemente para minimizar os `_cache_ misses`. Erros que ocorrem quando o dado desejado não está no `_cache_`.

2. **Balanceamento de Carga**: a arquitetura **SMP** induziu a aplicação do conceito de **balanceamento de carga**, em inglês _load balancing_ para distribuir processos inteligentemente entre os processadores disponíveis. O algoritmo de balanceamento opera em múltiplas dimensões. A saber:

   - **Balanceamento Quantitativo**: distribui o número de processos ativos uniformemente entre processadores;
   - **Balanceamento Qualitativo**: considera a intensidade computacional dos processos, evitando concentrar tarefas _CPU-intensive_, tarefas que requerem primordialmente o uso de `CPU`, em um único processador;
   - **Balanceamento Temporal**: ajusta dinamicamente a distribuição com base em padrões de execução históricos.

   No domínio do balanceamento de carga duas tecnologias prevalecem: o balanceamento pode ser _push-based_, no qual processadores sobrecarregados migram processos proativamente. Neste caso, processadores ociosos solicitam trabalho ao agendador. Sistemas modernos frequentemente implementam abordagens híbridas que combinam ambas as estratégias.

3. **Desafios de Sincronização e Coerência**: a convergência de paradigmas em sistemas **SMP**, além da complexidade do agendamento de tarefas e do balanceamento de cargas, introduziu novos graus de complexidade na sincronização entre processos e dados. Quando múltiplos processadores acessam estruturas de dados compartilhadas, tais como filas de processos, tabelas de páginas,  o próprio agendador, os mecanismos de sincronização requerem atenção extra. Dessa forma, os **Sistemas Operacionais** modernos implementam um conjunto diversificado de mecanismos de sincronização para garantir a integridade dos dados e evitar condições de corrida. Condições de corrida são estados em que dois ou mais processos acessam o mesmo dado com direito de escrita e modificação. Em condições de corrida é difícil manter a integridade e coerência do dado. Entre os métodos usados estão:

   - **Locks de Spin**: representam o mecanismo mais fundamental para proteção de seções críticas de duração extremamente curta em ambientes **SMP**. Estes mecanismos são Implementados por meio de operações atômicas como _compare-and-swap_ ou _test-and-set_, os `spinlocks` mantêm o processador em um laço ativo verificando continuamente o estado do `lock` até que seja liberado. Esta abordagem é particularmente eficiente quando o tempo de espera é menor que o custo computacional de uma troca de contexto, tipicamente para seções críticas que executam em menos de $100$ microssegundos. Em sistemas _multicore_, os `spinlocks` aproveitam a latência reduzida da comunicação _inter-core_ usando o `_cache_` compartilhado, evitando o custo de chamados às funções do sistema e mudanças de estado do processo. Contudo, em cenários de alta contenção ou seções críticas longas, podem causar desperdício significativo de ciclos de `CPU` e degradação de performance.

   - **Mutexes**: constituem o mecanismo padrão para sincronização de seções críticas de duração moderada a longa, implementando semântica de exclusão mútua com semântica de propriedade em `threads`. Mutexes modernos implementam algoritmos como `priority inheritance`, herança de prioridade, em que um `thread` de baixa prioridade que detém um `mutex` temporariamente herda a prioridade do `thread` de alta prioridade que está bloqueado esperando o `lock`.

   - **Semáforos**: implementam um mecanismo generalizado de sincronização baseado em contadores que controla o acesso a recursos limitados em quantidade específica. Introduzidos por [Dijkstra](https://en.wikipedia.org/wiki/Edsger_W._Dijkstra), os semáforos mantêm um contador interno que representa o número de recursos disponíveis, permitindo que múltiplos `threads` acessem simultaneamente até o limite estabelecido.

   - **Read-Write Locks**: representam uma otimização para cenários com padrões de acesso predominantemente de leitura, permitindo concorrência entre múltiplos processos leitores enquanto garantem exclusividade para processos escritores. Esta abordagem é fundamental em sistemas **SMP** para estruturas de dados como tabelas de páginas e diretórios de sistemas de arquivos nos quais operações de leitura são muito mais frequentes que modificações. 

4. **Métricas de Eficiência em Sistemas Híbridos**: conceitualmente podemos acreditar que os sistemas híbridos sejam mais eficientes. É até possível que a atenta leitora concorde com isso, apenas olhando sua própria máquina e verificando os processos que estão em execução. Contudo, fé e observação não definem a ciência nem garantem performance. Precisamos de métricas para análise. A eficiência total de um sistema híbrido pode ser expressa como:

    $$\text{System Efficiency} = \sum_{i=1}^{n} \text{CPU}_i \times \text{utilization}_i \times (1 - \text{overhead}_i)$$

    na qual $n$ é o número de processadores, $\text{utilization}_i$ é a taxa de utilização do processador $i$, e $\text{overhead}_i$ representa o custo computacional de sincronização e gerenciamento.

    Esta equação captura a realidade de que a eficiência não escala linearmente com o número de processadores devido aos custos computacionais da própria sincronização. O termo $(1 - \text{overhead}_i)$ torna-se particularmente significativo em sistemas com muitos núcleos de processamento, nos quais a contenção por `locks` e invalidações de `_cache_` podem degradar a performance. Para otimizar a eficiência, sistemas modernos implementam técnicas como **NUMA awareness**, em inglês **N**on-**U**niform **M**emory **A**ccess, nas quais o agendador considera a topologia de memória ao tomar decisões de agendamento, e **_cache_-affinity scheduling**, expressão em inglês para agendamento por afinidade de `_cache_`, que minimiza migrações desnecessárias de processos entre núcleos de processamento.

::: {#fig-smp1}
![](/images/arquitetura_smp.webp)

A figura ilustra quatro processadores simétricos compartilhando memória principal através de um barramento comum, onde cada `CPU` mantém sua própria fila de processos locais e implementa time-sharing com quantum de $100 ms$,  paradigmas de multiprogramação e _time-sharing_. O _Global Load Balancer_, balanceador de carga global, coordena a migração de processos entre `CPU`s com cargas desbalanceadas, `CPU` $0$ com $80\%$ vs. `CPU` $3$ com $20\%$, exemplificando o paradigma de multiprocessamento. A hierarquia de `_cache_`, `L1`/`L2` privados, `L3` compartilhado, e os mecanismos de sincronização representam os desafios fundamentais de coerência e coordenação que **Sistemas Operacionais** que implementem o **SMP** devem gerenciar para otimizar performance mantendo a coerência dos dados entre múltiplos processadores rodando múltiplos processos.
:::

##### Sistemas Influentes Na Era dos Sistemas Híbridos

Na era dos sistemas híbridos, destacamos cinco sistemas influentes que exemplificam a convergência dos paradigmas de multiprogramação, _time-sharing_ e multiprocessamento:

1. **CP/M (Control Program for Microcomputers)**: desenvolvido por [Gary Kildall](https://pt.wikipedia.org/wiki/Gary_Kildall) na Digital Research em 1974, o **CP/M** foi um marco importante na evolução dos **Sistemas Operacionais** de microcomputadores. Este sistema estabeleceu convenções duradouras para a organização de arquivos e comandos que influenciariam profundamente o desenvolvimento posterior de sistemas como o **MS-DOS**. O **CP/M** introduziu o conceito da **B**asic **I**nput/**O**utput **S**ystem, **BIOS**, uma camada de abstração entre o hardware e o **Sistema Operacional** que permitiu maior portabilidade entre diferentes microcomputadores baseados nos processadores **Intel 8080** e **Zilog Z80**. A estrutura modular do **CP/M**, com _**C**ommand **C**onsole **P**rocessor,_ **CCP**, _**B**asic **D**isk **O**perating **S**ystem, **BDOS** e a  **BIOS**, tornou-se um modelo arquitetônico para sistemas posteriores. Durante o final dos anos 1970 e início dos 1980, o **CP/M** dominou o mercado de microcomputadores comerciais, estabelecendo padrões para nomenclatura de drives (`A:`, `B:`, `C:`) e comandos básicos de sistema que persistem até hoje[^2][^3].

[^2]: O pobre autor teve que comprar uma placa de expansão para rodar o CP/M no seu Apple II. A placa tinha um processador [Zilog - Z80](https://www.zilog.com/docs/z80/um0080.pdf), $64 KB$ de memória e um drive de disquete de $5.25$ polegadas. O CP/M rodava em modo texto, mas permitia o uso de programas como o [WordStar](https://en.wikipedia.org/wiki/WordStar) e o [dBase II](https://en.wikipedia.org/wiki/DBase), que eram muito populares na época. O **CP/M** foi um dos primeiros **Sistemas Operacionais** a permitir a execução de múltiplos programas simultaneamente, embora não fosse multitarefa no sentido moderno. Mais importante, eu tinha, em casa, à disposição, uma máquina que podia ser programada na **Linguagem C**. Finalmente me livrando do Basic infernal do Apple II.

[^3]: Há aqui uma outra curiosidade. A pedido do meu diretor imediato que atendia uma solicitação do presidente da empresa, este pobre autor foi levado, em março de 1981 se não me falha a memória, a atender um dentista. O [Dr. Olympio Faissal Pinto](https://www.odontologiafaissol.com.br/historia.html). O objetivo era ajudar o bom Dr. a criar uma conexão via modem com um computador de uma universidade em Miami, na Flórida. O Dr. Olympio queria enviar para uma universidade na Califórnia a foto de um dente. A máquina era um TRS-80 modelo III rodando o TRS-DOS e o CP/M. O modem era um Radio Shack Modem I, com velocidade de $300 bps$. A conexão era feita por uma linha telefônica analógica. A imagem digitalizada tinha resolução de $640X480$. E, dada a complexidade de modens, placas de captura e protocolos, o Dr. precisava de ajuda. Passamos $4$ horas resolvendo problemas e mais algumas horas enviando. Este pobre autor não tem como provar, mas tem a esperança que esta tenha sido a primeira conexão internet do Brasil. O computador do Dr. acessou a internet usando o computador de Miami como _gateway_. Para o Dr. deve ter sido algo comum, mas o jovem técnico, nunca esqueceu essa tarde. Mesmo, devo admitir, sem ter a menor ideia que o que estávamos conectando era a internet.

2. **MS-DOS (**M**icro**S**oft **D**isk **O**perating **S**ystem)**: originado como uma adaptação do **Q**uick and **D**irty **O**perating **S**ystem,  [**QDOS**](https://en.wikipedia.org/wiki/86-DOS), desenvolvido por [Tim Paterson](https://pt.wikipedia.org/wiki/Tim_Paterson) na Seattle Computer Products. O **MS-DOS** foi adquirido pela Microsoft em 1981 para atender à demanda da IBM por um **Sistema Operacional** necessário ao seu novo **P**ersonal **C**omputer, **PC**. O sistema mantinha compatibilidade conceitual com o **CP/M**, facilitando a migração de aplicações, mas foi otimizado para o processador Intel 8086/8088. Sua interface de linha de comando, embora aparentemente simples, oferecia recursos poderosos como redirecionamento de entrada/saída, processamento em lote por meio de arquivos `.BAT`, e suporte a dispositivos por meio de drivers carregáveis. O **MS-DOS** evoluiu significativamente ao longo de suas versões, introduzindo suporte a discos rígidos, neste caso na versão 2.0, estruturas de diretórios hierárquicas, e eventualmente suporte limitado à memória estendida. Sua natureza monotarefa e arquitetura de $16 bits$, embora limitantes, proporcionaram estabilidade e previsibilidade que contribuíram para o estabelecimento do padrão **IBM PC** como plataforma dominante na computação pessoal por mais de uma década[^4].

[^4]: Este, o pobre autor, rodava **MS-DOS** em um PC-386, com co-processador matemático, comprado em consórcio e construído pela Cobra Informática, uma empresa brasileira que importava componentes e montava computadores pessoais no Brasil. O **MS-DOS** era o **Sistema Operacional** padrão para PCs compatíveis com **IBM PC**, e eu o utilizava para rodar programas como o WordPerfect e o Lotus 1-2-3. Usava o Borland C++ para programar usando a **Linguagem C++**, e o Borland Turbo Pascal para programar em Pascal. mas este último só para atender eventuais clientes ou como forma de autopunição.

3. **Apple Macintosh OS (Classic Mac OS)**: lançado em 1984, o **Sistema Operacional** do Macintosh representou uma revolução na interação humano-computador, popularizando conceitos que hoje consideramos fundamentais. Inspirado no trabalho pioneiro realizado nos Laboratórios Xerox Alto e Star, o **Mac OS** implementou de forma comercialmente viável a metáfora da área de trabalho. Nesta metáfora arquivos eram representados como documentos físicos e pastas como contêineres organizacionais. O sistema aproveitou as ideias dos Laboratórios da Xerox e introduziu o _m se_ como dispositivo primário de navegação, implementou o conceito de **WYSIWYG**, abreviatura do _What You See Is What You Get_, na edição de documentos, e estabeleceu padrões de interface como menus suspensos, caixas de diálogo modais, e manipulação direta de objetos gráficos. Tecnicamente, o Mac OS original baseava-se em um núcleo de processamento cooperativo que, embora não oferecesse proteção robusta de memória ou multitarefa preemptiva, proporcionava uma experiência de usuário fluida e intuitiva. Sua arquitetura de recursos permitia a incorporação de elementos gráficos, sonoros e de interface diretamente nos arquivos executáveis, facilitando a localização e personalização de aplicações.

4. **Microsoft Windows** - Iniciado em 1985 como um ambiente gráfico executado sobre o **MS-DOS**, o Windows passou por três fases distintas de desenvolvimento. As versões 1.0 a 3.11 funcionavam essencialmente como ambientes gráficos e não como **Sistemas Operacionais**, oferecendo uma interface visual para o **MS-DOS** subjacente, mas mantendo as limitações fundamentais de um sistema de $16 bits$. Nesta fase, apenas o Windows 3.11 teve sucesso comercial muito devido ao sistema de configuração de redes locais que dispensava produtos de terceiros e ao lançamento do Corel Draw que trouxe a editoração gráfica para o desktop. O Windows 95 marcou um ponto de inflexão na evolução dos **Sistemas Operacionais**, introduzindo multitarefa preemptiva de $32 bits$, um sistema de arquivos mais robusto, e uma interface redesenhada que incorporava elementos como a barra de tarefas e o menu Iniciar. Paralelamente, a linha **Windows NT**, iniciada em 1993 sob a liderança de [Dave Cutler](https://en.wikipedia.org/wiki/Dave_Cutler), representou uma abordagem completamente nova: um **Sistema Operacional** construído desde a concepção com arquitetura de $32 bits$, `microkernel` híbrido, e recursos avançados de segurança baseados em _**A**ccess **C**ontrol **L**ists_, **ACL**s, e domínios. O **NT** introduziu conceitos  importados do padrão **POSIX**, como `threading` avançado, proteção de memória, e suporte nativo a redes, estabelecendo as bases arquitetônicas que persistem nas versões modernas do Windows. Desde o Windows XP, lançado em 2001, todas as versões do Windows são baseadas no `kernel` do **Windows NT 4.0**.

::: callout-note
**A Relação entre o Windows NT, o POSIX e o UNIX**
O Windows **NT** representou uma adaptação sofisticada de princípios estabelecidos da ciência da computação que já estavam padronizados ou implementados em sistemas **UNIX**. O **POSIX.1** foi ratificado em 1988 precisamente quando [Dave Cutler](https://en.wikipedia.org/wiki/Dave_Cutler) iniciou o desenvolvimento do **NT** na Microsoft, criando uma convergência histórica única. O **POSIX.1b-1993**, publicado poucos meses depois do lançamento do **NT** em julho de 1993, já havia padronizado extensões avançadas de tempo real incluindo escalonamento de prioridade, travamento de memória, objetos de memória compartilhada, filas de mensagens e temporizadores de alta resolução, capacidades que o subsistema **POSIX** mínimo do **NT** ignorou completamente.

Em 1993, os padrões **POSIX** definiam capacidades sofisticadas de multiprocessamento com escalonamento de prioridade de $32$ níveis, arquivos mapeados em memória, semáforos e sinais de tempo real. O Windows **NT** implementou apenas a revisão básica **POSIX.1-1990** para conformidade com contratos governamentais.

O uso de `threading` é particularmente revelador da influência do **POSIX**: enquanto o **NT** foi lançado com `threading` nativo em 1993, o [Mach](https://en.wikipedia.org/wiki/Mach_(`Kernel`)) havia introduzido `threads` em sistemas semelhantes ao **UNIX** em 1985, oito anos antes. Os padrões de `threading` **POSIX** estavam em desenvolvimento durante a criação do **NT** e foram publicados como **POSIX.1c** em 1995. O **POSIX.1b-1993** especificou interfaces de travamento de memória, arquivos mapeados em memória, escalonamento preemptivo de prioridade fixa com mínimo de $32$ níveis de prioridade, semáforos nomeados e não-nomeados, filas de mensagens e temporizadores de precisão de nanossegundos, tudo antes do lançamento do **NT**.

A implementação **POSIX** do **NT** foi deliberadamente mínima, suportando apenas chamadas de sistema básicas sem utilitários de `shell`, interfaces de `threading` ou extensões de tempo real. O subsistema **POSIX** da Microsoft foi o resultado de uma caixa de seleção para conformidades de contratos, não um esforço sério de compatibilidade com o **UNIX**.

As inovações genuínas do **NT** foram em integração arquitetônica em vez de avanços fundamentais. O design de `microKernel` híbrido suportando múltiplas personalidades de **Sistema Operacional** ,  **Win32**, **POSIX**, **OS/2**, simultaneamente foi arquitetonicamente inovador. A implementação abrangente da camada de abstração de hardware, em inglês **H**ardware **A**bstraction **L**ayer, **HAL**, excedeu as abordagens de portabilidade contemporâneas. A arquitetura de segurança integrada com controle de acesso baseado em capacidades representou um avanço genuíno sobre o modelo mais simples baseado em usuário/grupo/outros do **UNIX**. Finalmente, o design unificado para multiprocessamento desde a primeira versão, constituiu uma vantagem arquitetônica e competitiva. Enquanto os sistemas **UNIX** estavam gradualmente adaptando suporte **SMP**, o agendador de tarefas centrado em `threads` do **NT** e a preempção integrada do `Kernel` representaram engenharia superior de conceitos estabelecidos ao invés de inovação. No entanto, o **UNIX** mantinha vantagens significativas: **os sockets BSD tiveram uma década de refinamento até 1993, fornecendo interfaces de programação de rede maduras e comprovadas**. Os sistemas **UNIX** ofereciam ambientes de desenvolvimento sofisticados, estabilidade comprovada por meio de implantação em produção e serviços de rede abrangentes como **NFS** e soluções integradas de computação distribuída.
:::

5. **Linux**: da sua concepção em 1991, por [Linus Torvalds](https://pt.wikipedia.org/wiki/Linus_Torvalds) como um hobby pessoal para criar um sistema semelhante ao [MINIX](https://www.minix3.org/) para computadores pessoais usando o Intel 386, o **Linux** evoluiu para se tornar um dos projetos de software livre mais bem-sucedidos da história. Sua arquitetura monolítica permite a incorporação dinâmica de funcionalidades por meio de módulos carregáveis dinamicamente, oferecendo flexibilidade sem comprometer performance. O desenvolvimento do **Linux** seguiu um modelo colaborativo distribuído sem precedentes, conhecido como `bazar` segundo [Eric Raymond](https://pt.wikipedia.org/wiki/Eric_S._Raymond). Nesse modelo milhares de desenvolvedores contribuem simultaneamente para diferentes aspectos do sistema. Tecnicamente, o **Linux** implementa recursos avançados como gerenciamento de memória virtual, multiprocessamento simétrico (**SMP**), agendamento de tarefas em tempo real, e suporte extensivo a sistemas de arquivos como ([ext4](https://www.`Kernel`.org/doc/html/latest/admin-guide/ext4.html), [Btrfs](https://btrfs.readthedocs.io/en/latest/), [ZFS](https://docs.freebsd.org/en/books/handbook/zfs/)). Sua natureza de código aberto, e a **Linguagem C**, permitiram adaptações deste código para uma variedade extraordinária de plataformas, desde supercomputadores até dispositivos embarcados, smartphones, e sistemas de tempo real.

A [Linux Foundation](https://www.linuxfoundation.org/), criada em 2000, garante sua sustentabilidade do projeto e mantém uma história de evolução contínua. O **Linux** não é apenas um **Sistema Operacional** ; é uma plataforma que impulsiona a inovação em áreas dispares que vão desde supercomputadores até dispositivos móveis. Porém, do ponto de vista do desenvolvimento de **Sistemas Operacionais** dizemos que o Projeto **Linux** desenvolve o `Kernel`,  núcleo. Neste caso, um `Kernel` de arquitetura monolítica e modular.

::: {#fig-doadores}
![](/images/linus-donors.webp)

A figura mostra uma coleção curiosa dos maiores doadores da Fundação Linux. A figura foi tirada do site da [Linux Foundation](https://www.linuxfoundation.org/) em junho de 2025 e destaca a Microsoft como um dos maiores doadores da fundação. A Microsoft, que já foi uma das maiores concorrentes do Linux, agora é um dos principais apoiadores do projeto. Esta mudança de postura da Microsoft em relação ao **Linux** reflete a evolução do ecossistema de software e a crescente importância do **Linux** em ambientes corporativos e de nuvem.  
:::

A era do **IBM PC** provocou a simplificação dos recursos do **Sistema Operacional** , `CPU`, memória, `E/S`, em comparação com os **Sistemas Operacionais** de `mainframe`. Entretanto, esta simplificação durou pouco. O laço de realimentação positiva que existe entre software e hardware gradualmente reintroduziu a complexidade no sistema graças às novas aplicações cada dia mais sofisticadas e exigentes.

## Fronteiras Modernas

As últimas décadas foram marcadas por níveis de exigência de funcionalidades e performance e pela criação de novos paradigmas computacionais. Esta evolução  transform  a forma como os seres humanos interagem com a tecnologia, graças à expansão  das capacidades de processamento, armazenamento e comunicação de dados, assentando os alicerces para a revolução do uso da Inteligência Artificial. Desde os dispositivos móveis que se tornaram extensões de nossas vidas cotidianas, quase como órteses vitais, até as vastas infraestruturas de computação em nuvem, os complexos sistemas distribuídos que sustentam a economia digital global, e os supercomputadores científicos. Cada avanço tecnológico representa um novo horizonte de possibilidades e tem impacto profundo nas tecnologias que usamos para desenvolver os **Sistemas Operacionais**.

Tecnologias como **computação móvel**, **sistemas distribuídos**, **computação em nuvem** e, mais recentemente, **computação quântica** e **Inteligência Artificial** moldam os **Sistemas Operacionais** modernos. Ao mesmo tempo em que só podem existir e evoluir graças à existência destes mesmos **Sistemas Operacionais**. Se olharmos apenas para as inovações, relacionadas à convergência entre **L**arge **L**anguage **M**odel, **LLMs**, modelos de linguagem de grande escala, representante dos avanços em Inteligência Artificial e a computação quântica veremos que existe uma urgência econômica incentivando a criação de uma nova geração de **Sistemas Operacionais** capaz de atender o hardware de alto desempenho que suporta estas tecnologias.

Nesta seção, nossa jornada nos leva às fronteiras da ciência e da tecnologia. Começando pelo campo da tecnologia que será integrada a tudo que foi desenvolvido até o momento nas nossas vidas cotidianas. Os sistemas embarcados. Aperte o cinto e respire fundo.

### Sistemas Operacionais Embarcados e IoT: Conectando o Mundo Físico

Graças a internet, a capacidade de integração de dispositivos eletrônicos e as novas tecnologias que desenvolvemos em cima delas, a humanidade criou um termo para englobar todos os dispositivos conectados e portadores de alguma inteligência computacional a **I**nternet **o**f **T**hings, **IoT**, internet das coisas em português. Com a convergência entre avanço deste conjunto de tecnologias, o baixo custo, e a proliferação de dispositivos conectados, os **Sistemas Operacionais** embarcados, _embedded_ em inglês, tornaram-se os pilares da infraestrutura tecnológica em meados da segunda década do século XXI. Esses sistemas especiais são projetados para operar em hardware com recursos limitados em memória, capacidade de processamento, velocidade e integração com o mundo externo. Do ponto de vista da arquitetura de **Sistemas Operacionais**, isso representa um desafio fascinante: como aplicar os princípios de abstração de hardware, gerenciamento de recursos e concorrência em um ambiente tão restritivo?

Esta categoria de sistemas computacionais abrange desde os sistemas baseados em microcontroladores controlando uma rede de sensores usados, como os usados em marca-passos cardíacos, até sistemas completos em um único circuito integrado usados para controlar robôs, drones, automóveis e sistemas de controle industrial rodando [SCADA](https://en.wikipedia.org/wiki/SCADA). Estes últimos chamados de `SoC`, em inglês **S**ystem **o**n **C**hip, frequentemente, operando em tempo real. A ubiquidade dos **Sistemas Operacionais** embarcados é um testemunho do sucesso desta tecnologia e da sua importância na era moderna.

As limitações do hardware e do ambiente de uso de sistemas embarcados tensionaram o desenvolvimento de **Sistemas Operacionais Embarcados**. Ou, em outras palavras, diferentemente dos **Sistemas Operacionais** criados originalmente para _mainframes_, servidores e computadores pessoais, os **Sistemas Operacionais Embarcados** priorizam baixo consumo de energia, tamanho reduzido de código e resposta determinística a eventos externos.

#### Características e Desafios dos **Sistemas Operacionais Embarcados**

Os **Sistemas Operacionais Embarcados**  distinguem-se fundamentalmente de seus congêneres tradicionais por especificidades moldadas pelas limitações de recursos e pelos requisitos operacionais únicos de seus ambientes de implantação. _A eficiência energética constitui uma das preocupações primordiais nestes sistemas_. Trata-se de uma extensão crítica da função de gerenciamento de recursos, que em um **Sistema Operacional** tradicional foca no tempo de `CPU` e memória, mas aqui se inclui gerenciamento ativo de consumo de energia. Adicionando um porto extra à rota da performance. 

O gerenciamento ativo de consumo de energia é, como não poderia deixar de ser, implementado em algoritmos sofisticados de controle do hardware. Estas técnicas permitem ao sistema, por exemplo, ajustar dinamicamente a tensão e frequência de operação da `CPU` com base na carga de trabalho atual. Desligando circuitos, reduzindo a temperatura do circuito integrado e otimizando o uso do hardware. Para dispositivos alimentados por bateria, como sensores **IoT** distribuídos em ambientes remotos, esta capacidade de otimização energética pode representar a diferença entre operação contínua por meses ou falha prematura em dias. Em dispositivos como drones,  robôs autônomos, o uso de energia define a fronteira entre útil e de volta à prancheta. O consumo de energia é um requisito recente, limitante e novo de **Sistemas Operacionais Embarcados**. Mas não é a única preocupação.

A otimização de espaço constitui outro imperativo arquitetural. Enquanto um **Sistema Operacional** de propósito geral utiliza mecanismos como memória virtual e paginação para gerenciar `gigabytes` de `RAM`, um **Sistema Operacional Embarcado** deve operar sem esses luxos, exigindo que o código seja meticulosamente refinado para ocupar apenas alguns `kilobytes` de memória. Esta compactação é uma necessidade absoluta quando operando em microcontroladores com memória limitada, frequentemente restrita a apenas $32 KB$ de `RAM` comuns em dispositivos domésticos e industriais de monitoramento e acionamento. Neste ambiente, os desenvolvedores são forçados a dominar técnicas sofisticadas de otimização, incluindo compilação agressiva, eliminação de código não utilizado e implementação de algoritmos específicos para ambientes com recursos escassos que são diferentes para plataformas diferentes. **Nesse campo de desenvolvimento, o diferencial competitivo está no conhecimento profundo de linguagens formais e técnicas de compilação e otimização de código**. Muitas vezes somos obrigados a trocar espaço em memória por `throughput`. Principalmente em sistemas que devem reagir ao universo físico em tempo real.

Em sistemas de tempo real a natureza crítica das aplicações embarcadas eleva o determinismo temporal ao status de requisito fundamental. Isso impacta diretamente o núcleo do **Sistema Operacional** , especificamente seu algoritmo de agendamento de tarefas, _scheduler_. O resultado desta tensão é a classificação de numerosos sistemas embarcados como **R**eal-**T**ime **O**perating **S**ystems, **RTOS**. Quase um sonho inatingível: um sistema computacional, composto de hardware e software, que deve responder a eventos do mundo físico no mesmo tempo em que eles ocorrem, o tempo real. 

O termo **RTOS** refere-se a sistemas operacionais projetados para garantir que tarefas vitais para seres humanos e máquinas sejam executadas dentro de prazos temporais rigorosos, com previsibilidade e confiabilidade. Esses sistemas são encontrados em aplicações onde atrasos podem resultar em falhas catastróficas, como em dispositivos médicos implantáveis, controladores automotivos de sistemas de frenagem,  sistemas de controle de voo e robótica em geral. Os **sistemas de tempo real devem garantir respostas dentro de prazos temporais estritos, previsíveis e muitas vezes, imutáveis**. 

O determinismo temporal transcende a simples velocidade de processamento, exigindo garantias matemáticas de que operações críticas serão completadas dentro de janelas temporais específicas, mesmo sob condições de carga máxima do sistema. A @fig-rtos1 mostra o diagrama em blocos de um sistema **RTOS**.

::: {#fig-rtos1}
![](/images/embedded_os_architecture.webp)

Um exemplo prático introdutório, sem Sistema Operacional, mas muito interessante, pode ser visto no desenvolvimento de softwares para [seguidores de linha](https://frankalcantara.com/linefollower/). Um desenvolvimento inicial obrigatório para todos os interessados em sistemas de tempo real e robótica.

Arquitetura em camadas de um **Sistema Operacional** de Tempo Real (RTOS) para dispositivos **IoT** embarcados. A estrutura apresenta quatro camadas principais: aplicação, com algoritmos [TinyML](https://arxiv.org/abs/2403.19076) e processamento determinístico, `kernel` **RTOS**, incluindo agendador preemptivo, gerenciamento de memória estática, e gestão de energia, camada de abstração de hardware (HAL) e plataforma física. As restrições típicas incluem $32KBytes$ de `RAM`, processador de $80MHz$ e requisitos de tempo real com latência inferior a **1ms** para tarefas críticas. A base de hardware abrange microcontroladores ARM Cortex-M4, sensores MEMS, módulos de comunicação (WiFi, BLE, LoRaWAN) e sistemas de energia. Os protocolos de comunicação **IoT** ([MQTT](https://mqtt.org/), [CoAP](https://coap.space/), [6LoWPAN](https://www.ti.com/lit/wp/swry013/swry013.pdf?ts=1730905735016), [Thread](https://openthread.io/guides/thread-primer?hl=pt-br), [Zigbee](https://en.wikipedia.org/wiki/Zigbee), [NB-IoT](https://www.3gpp.org/news-events/3gpp-news/nb-iot-complete)) facilitam a integração em redes distribuídas com diferentes requisitos de largura de banda e alcance.
:::

A @fig-rtos1 tenta apresentar que a conectividade em sistemas embarcados manifesta-se por meio de suporte a protocolos de rede especificamente projetados para ambientes com limitações em recursos e em largura de banda. Protocolos como o [MQTT](https://mqtt.org/), de **M**essage **Q**ueuing **T**elemetry **T**ransport, e o [CoAP](https://coap.space/), de **Co**nstrained **A**pplication **P**rotocol, foram desenvolvidos para facilitar comunicação em redes **IoT**, operando eficazmente mesmo com largura de banda severamente limitada e conectividade intermitente. Estes protocolos implementam mecanismos de compressão de dados, estratégias de reconexão automática e algoritmos de `retry` que permitem operação confiável em ambientes de rede hostis típicos de dispositivos **IoT**. Com implicação direta na arquitetura dos **Sistemas Operacionais Embarcados**. Se temos dispositivos conectados em rede, temos problemas de segurança. Esta matriz de conexão cria superfícies de ataque expandidas.

As considerações de segurança, que já são um pilar no design de qualquer **Sistema Operacional** , adquirem novos níveis de complexidade com a proliferação de dispositivos conectados à internet. Por exemplo, a implementação de mecanismos como inicialização segura, em inglês _secure boot_, garante que apenas código autenticado e não adulterado seja executado durante o processo de inicialização, estabelecendo a cadeia de confiança desde o primeiro momento de operação. Complementarmente, a criptografia embarcada deve ser implementada considerando os recursos limitados, e requer algoritmos adaptados e específicos. Requerendo especializações extras em performance para os arquitetos destes algoritmos. Finalmente, dispositivos remotos requerem atualização remota. Esta necessidade de atualizações **o**ver-**t**he-**a**ir, **OTA**, representa tanto uma necessidade operacional quanto um desafio de segurança, permitindo manutenção remota de dispositivos implantados em localizações inacessíveis, mas introduzindo vetores de ataque que devem ser rigorosamente protegidos por meio de assinatura digital, criptografia de canal e verificação de integridade.

Não bastasse os problemas de segurança, performance e conectividade, os **Sistemas Operacionais Embarcados** enfrentam desafios relacionados à falta de padronização de hardware, enquanto que todo o mercado de servidores e computadores pessoais está limitado a uma dezena de plataformas diferentes criadas e mantidas por meia dúzia de empresas, o mercado de **Sistemas Operacionais Embarcados** é caracterizado por uma diversidade extraordinária de microcontroladores, sensores e módulos de comunicação

#### Desafios de Arquitetura e Operação {#sec-desafios-arquitetura-operacao}

A ausência de padrões universais,  mercadológicos, de hardware em dispositivos embarcados introduz um problema fundamental de fragmentação que permeia todos os aspectos do desenvolvimento de **Sistemas Operacionais Embarcados**. A diversidade de plataformas de hardware força os **Sistemas Operacionais Embarcados** a adotar arquiteturas definitivamente modulares impondo aos projetistas uma atenção especial à camada de abstração de hardware, em inglês **H**ardware **A**bstraction **L**ayer, um componente do `kernel` dos **Sistemas Operacionais** cuja função é isolar as aplicações das especificidades da plataforma física.

Esta fragmentação manifesta-se na diversidade extraordinária de microcontroladores, desde arquiteturas [**ARM Cortex-M4**](https://www.arm.com/products/silicon-ip-cpu/cortex-m/cortex-m4) de baixo consumo até processadores [RISC-V](https://riscv.org/) emergentes, cada um com características específicas de memória, conjuntos de instruções e periféricos integrados. A variabilidade estende-se aos sensores incorporados, abrangendo desde simples sensores de temperatura analógicos até complexos sistemas `MEMS` multi-eixo, cada um exigindo drivers específicos e protocolos de comunicação distintos.

::: callout-note
***O Que São MEMS Multi-eixo?**

Sistemas Microeletromecânicos (`MEMS`) multi-eixo são sensores microscópicos, integrados em um chip de silício, que medem o movimento e a orientação de um objeto no espaço. A sua capacidade é definida pelo número de eixos ou `graus de liberdade`, em inglês **D**egree **o**f **F**reedom, que monitoram:

- **3-Eixos**: utiliza um **acelerômetro** para medir a aceleração linear nos eixos $X$, $Y$ e $Z$, detectando inclinação e deslocamento;
- **6-Eixos (IMU)**: uma unidade de medição inercial, em inglês **I**nertial **M**easurement **U**nit, adiciona um **giroscópio** de 3 eixos para medir a velocidade de rotação, permitindo um rastreamento completo de movimento;
- **9-Eixos (IMU)**: Integra um **magnetômetro** de 3 eixos que atua como uma bússola digital, fornecendo uma referência de direção absoluta e corrigindo desvios para uma orientação precisa.

Esses dispositivos de baixo consumo de energia são a tecnologia por trás da rotação de tela em smartphones, da estabilização de imagem em câmeras de vídeo, dos sistemas de segurança em automóveis, airbags, controle de estabilidade, e da navegação de drones. Para ficar nos poucos casos que lembrei sem muito esforço.
:::

A integração crescente de capacidades de Inteligência Artificial, particularmente por meio de modelos leves como [TinyML](https://pll.harvard.edu/course/fundamentals-tinyml), **Tiny** **M**achine **L**earning*, introduz um novo porto na jornada de complexidade dos **Sistemas Operacionais Embarcados**. Estes modelos de aprendizado de máquina, embora otimizados para operação em ambientes com recursos limitados, ainda impõem demandas computacionais significativas que devem ser cuidadosamente balanceadas contra as limitações de processamento e energia dos dispositivos hospedeiros. Novamente destacando a necessidade de uma camada extra de especialização à formação dos arquitetos e engenheiros de inteligência artificial e **Sistemas Operacionais**.

A complexidade e os desafios dos **Sistemas Operacionais Embarcados** é maior e mais relevante quando voltamos nossa análise para áreas de desenvolvimento que incluem aplicações médicas, militares, aeronáutica e financeiras. Nestes setores, a confiabilidade e a segurança são não apenas desejáveis, mas absolutamente essenciais. A certificação de conformidade com normas rigorosas, como as da **FDA** para dispositivos médicos ou as normas de segurança automotiva **ISO 26262**, exige que os **Sistemas Operacionais Embarcados** implementem práticas de desenvolvimento rigorosas, incluindo testes extensivos, validação formal e documentação meticulosa. A complexidade adicional introduzida por estas exigências normativas não apenas aumenta o custo e o tempo de desenvolvimento, mas também eleva o nível de especialização necessário para os engenheiros envolvidos. Isso sem falar, que estas áreas da economia tem impacto social e ético.

O design de sistemas embarcados para estas aplicações exige um equilíbrio delicado entre inovação tecnológica e responsabilidade social que transcende considerações puramente técnicas. _Esta responsabilidade manifesta-se na necessidade de transparência algorítmica, especialmente em sistemas que incorporam Inteligência Artificial, auditabilidade de decisões críticas, e capacidade de operação ética mesmo em situações não previstas durante o desenvolvimento_. Assim, o estudo dos **Sistemas Operacionais Embarcados**  transcende a especialização técnica, representando a fronteira moderna da teoria de **Sistemas Operacionais**, onde os princípios clássicos de gerenciamento e abstração são testados contra os limites da física, da conectividade e da ética

#### Exemplos de **Sistemas Operacionais Embarcados**

1. **FreeRTOS**: criado por Richard Barry em 2003, adquirido pela Amazon em 2017, o [FreeRTOS](https://www.freertos.org/) é um **RTOS** de código aberto amplamente utilizado em dispositivos embarcados sejam eles **IoT** ou não. Sua arquitetura modular suporta microcontroladores de baixa potência, como os da família **ARM Cortex-M**. O **FreeRTOS** oferece escalonamento preemptivo, comunicação inter-tarefa via filas e semáforos, e integração com o [AWS IoT Core](https://aws.amazon.com/pt/iot-core/) para conectividade em nuvem. Em 2025, é comum encontrar sistemas **FreeRTOS** em dispositivos como termostatos inteligentes e sensores industriais.

2. **Zephyr**: mantido pela **Linux** Foundation, o [Zephyr](https://docs.zephyrproject.org/latest/index.html) um **RTOS** projetado para **IoT**, com suporte a uma ampla gama de arquiteturas, entre elas: **ARM**, **RISC-V** e **x86**. Ele se destaca por sua escalabilidade, permitindo uso em dispositivos com apenas $8 KB$ de `RAM`, e por sua pilha de rede completa, incluindo [**B**luetooth **L**ow **E**nergy (**BLE**)](https://www.bluetooth.com/learn-about-bluetooth/tech-overview/) e **Thread**. O **Zephyr** é usado em dispositivos vestíveis, como smartwatches, e em redes de sensores para cidades inteligentes.

3. **QNX**: o [QNX](https://blackberry.qnx.com/en) é um **RTOS** comercial baseado em micro`Kernel`, amplamente adotado em sistemas críticos, como automóveis e dispositivos médicos. Sua arquitetura modular e certificações de segurança (ex.: ISO 26262 para sistemas automotivos) o tornam ideal para aplicações onde falhas não são toleráveis. Em 2025, o **QNX** é usado em sistemas de assistência ao motorista e em dispositivos **IoT** industriais.

4. **RIOT**: o [RIOT](https://www-riot--os-org.translate.goog/?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=pt&_x_tr_pto=tc) é um **Sistema Operacional** de código aberto voltado para dispositivos **IoT** de baixa potência. Ele suporta múltiplos protocolos de rede, como os citados acima, e é compatível com microcontroladores de $8$ e $32 bits$. Sua comunidade ativa e foco em interoperabilidade o tornam popular em projetos de pesquisa e protótipos de **IoT**, como redes de sensores ambientais.

### Sistemas Operacionais Móveis

Anteriormente, eu falei que os dispositivos eletrônicos estão se tornando órteses para o homem moderno. A culpa disso recai na ascensão dos dispositivos móveis, como _smartphones_ e _tablets_. Esses dispositivos redefiniram a computação pessoal e impulsionaram a criação de **Sistemas Operacionais** especializados. Diferentemente dos **Sistemas Operacionais** desenvolvidos para servidores e computadores pessoais, os **Sistemas Operacionais Móveis** são projetados para operar sobre uma camada de hardware com recursos inerentemente limitados em termos de capacidade de processamento, memória e autonomia de bateria. Por outro lado, são dispositivos que dependem inerentemente de conexões à redes. Olhando $10$ anos para trás, uma comparação com os dispositivos que temos hoje parece indicar que, versão à versão, as limitações estão diminuindo ou mudando de perfil. Esta evolução tensiona o desenvolvimento de **Sistemas Operacionais Móveis** forçando adaptações contínuas.

Desde o lançamento do Apple iPhone (2007), o foco no design dos **Sistemas Operacionais Móveis** centrou-se em interfaces de toque, com a introdução de comandos baseados em gestos multitoque como _swiping e _pinch-to-zoom_, transformando a experiência do usuário em um diferencial comercial definitivo. Os sistemas que não são amigáveis não atingem o ponto de equilíbrio nas vendas e são retirados do mercado, como o [Windows Phone](https://mobiforge.com/timeline/windows-phone-history). Estes dispositivos, e sua interface moderna têm impacto direto na criação de novos conceitos para o desenvolvimento de **Sistemas Operacionais**, como a introdução de _widgets_ e notificações interativas, que permitem uma interação dinâmica e personalizada com cada um dos usuários. Neste mercado, a integração de sensores como acelerômetros, giroscópios e `GPS` ampliou as possibilidades de interação e personalização, permitindo que os dispositivos móveis se transformassem de ferramentas de comunicação e consulta em plataformas multifuncionais para aplicações especializadas substituindo tarefas como navegação no mundo real, monitoramento de saúde, agendamento de compromissos e sistemas de entretenimento.

As duas plataformas predominantes no mercado de **Sistemas Operacionais Móveis** são o [Android](https://www.android.com/intl/pt_br/what-is-android/), desenvolvido pelo Google, e o [IoS](https://www.apple.com/ios/ios-18/), da Apple. 

O **Android** é baseado no `Kernel` **Linux** e ainda adota um modelo de código aberto, que permite personalizem seus dispositivos, embora essa flexibilidade também contribua para a fragmentação do ecossistema. Para a criação do **Android** o Google combina `kernels` **Linux** de suporte de longo prazo com atualizações e correções específicas para o Android, criando o que eles chamam de  **A**ndroid **C**ommon **K**ernels, **ACK**s, em português `kernels` comuns do Android. Para complementar o **Android** utiliza o [SQLite](https://www.sqlite.org/) para armazenamento de dados estruturados e, historicamente, a [máquina virtual Dalvik](https://source.android.com/docs/core/runtime?hl=pt-br), posteriormente substituída pela [Android Runtime - ART](https://source.android.com/docs/core/ota/modular-system/art?hl=pt-br), para a execução de aplicativos desenvolvidos primariamente em **Java** ou **Kotlin**. A diferença nas estruturas do Dalvik e o ART pode ser vista na @fig-dalvik1. Como a conectividade é indispensável em dispositivos móveis, o **Android** oferece suporte extensivo a tecnologias de conectividade, incluindo [GSM/EDGE](https://www.3gpp.org/), [CDMA](https://www.3gpp.org/), [EV-DO](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/ev-do-rev-a-and-b-wireless-broadband-for-the-masses-whitepaper.pdf), [UMTS](https://www.3gpp.org/), LTE, [5G](https://www.3gpp.org/), [Bluetooth](https://www.bluetooth.com/), [Wi-Fi](https://www.wi-fi.org/) e [WiMAX](https://wimaxforum.org/).

::: {#fig-dalvik1}
![](/images/dalvik_vs_art_comparison.webp)

Comparação das arquiteturas de runtime do Android: _Dalvik Virtual Machine_ vs _Android Runtime_. O diagrama ilustra a evolução dos mecanismos de compilação e execução, destacando a transição da interpretação _Just In Time_ do Dalvik para a estratégia híbrida _Ahead of Time / Just In Time_ do _Android RunTime_, com melhorias significativas em _garbage collection_ e gerenciamento de memória. Nós vamos voltar a isso tudo, com calma, no capítulo certo.
:::

O **i**Phone **O**perating **S**ystem, **iOS**, da Apple, é derivado do [macOS](https://www.apple.com/br/macos/macos-sequoia/) e opera em um modelo de plataforma fechada, com uma integração vertical forte entre hardware e software, o que frequentemente resulta em alto desempenho e otimização de recursos. Sua arquitetura é organizada em camadas distintas: o _Core OS_, que inclui o `Kernel` do **Sistema Operacional** , gerenciamento de energia e segurança, o _Core Services_, responsável por serviços como acesso a arquivos, rede e banco de dados SQLite, _Media_, para áudio, vídeo e gráficos, e o  _Cocoa Touch_, que gerencia as interações do usuário, incluindo gestos multitoque e acesso a sensores. A @fig-AndroidIos, apresenta a arquitetura do **iOS** e do Android.

::: {#fig-AndroidIos}
![](/images/android_vs_ios_os_comparison.webp)

Comparação das arquiteturas dos **Sistemas Operacionais** **Android** e **iOS**. O diagrama apresenta a estrutura em camadas de cada sistema, destacando as diferenças fundamentais entre a arquitetura baseada em **Linux** do **Android** e o `kernel` Darwin (XNU) do **iOS**.
:::

A atenta leitora deve ter notado, observando a @fig-AndroidIos que estes **Sistemas Operacionais** não são tão diferentes como olhos puros e inocentes podem supor. Com toda certeza veremos que as diferenças são mais profundas, mas infelizmente não são perceptíveis em um diagrama de blocos tão simples. Ambos os **Sistemas Operacionais Móveis** compartilham princípios fundamentais de design, como a camada abstração de hardware, gerenciamento de recursos e segurança, mas implementam esses conceitos de maneiras que refletem suas filosofias e objetivos distintos. Quando olhamos o diagrama em blocos da @fig-AndroidIos, os sistemas parecem versões da mesma ideia. A curiosa leitora pode aproveitar a oportunidade e comparar estes dois **Sistemas Operacionais** com o **RTOS** da @fig-rtos1.

#### Sistemas Operacionais Móveis: Desafios

Não são poucos desafios enfrentados pelos **Sistemas Operacionais Móveis**. A limitação de recursos é uma preocupação constante, exigindo que os desenvolvedores criem soluções eficientes em uso de memória, processamento, conectividade, segurança e gestão de energia em ambientes muito diferentes dos que encontramos em servidores e computadores pessoais.

A conectividade é uma das pedras angulares dos **Sistemas Operacionais Móveis**. Para que um dispositivo móvel tenha sucesso no mercado ele precisa de um amplo espectro de tecnologias de rede e protocolos de comunicação, incluindo Wi-Fi, redes celulares (3G, 4G e, cada vez mais, 5G), [Bluetooth](https://www.bluetooth.com/) e [NFC](https://nfc-forum.org/), garantindo comunicação constante e acesso a serviços online. A chegada do [5G](https://www.qualcomm.com/5g/what-is-5g), com suas promessas de velocidades significativamente mais altas e latência ultrabaixa, impõe novas demandas aos **Sistemas Operacionais Móveis** para gerenciar e habilitar o uso de novas classes de aplicativos, como os aplicativos de realidade aumentada e interações em tempo real mais ricas. No entanto, se removermos a diversidade de protocolos, a gestão de módulos de rede e conexão não é muito diferente em sistemas embarcados, móveis ou tradicionais. Em todos eles a solução mais adequada foi a criação de um sistema de carregamento de módulos. Assim, o **Sistema Operacional** pode carregar e rodar apenas os módulos necessários otimizando o consumo de energia e as camadas de segurança necessárias. 

Segurança e privacidade também são preocupações importantes no design de **Sistemas Operacionais Móveis**. Estes sistemas implementam modelos de permissão granulares, exigindo consentimento explícito do usuário para que aplicativos acessem recursos sensíveis como câmera, microfone, dados de localização e contatos. O *sandboxing* de aplicativos é uma técnica comum, isolando os processos e dados de cada aplicativo para prevenir interferências maliciosas e limitar o impacto de possíveis vulnerabilidades. O _sandboxing_ é implementado via restrições do `Kernel`, garantindo privacidade e estabilidade. Talvez este termo tenha sido escolhido porque nos EUA crianças pequenas brincam em caixas de areia, e não podem sair delas, tornando estas caixas um lugar seguro.

Para completar as funcionalidades de segurança, precisamos evidenciar a criptografia de dados, tanto para dados em repouso no dispositivo quanto em trânsito pela rede. Algoritmos de criptografia são amplamente utilizadas para proteger informações sensíveis. Apesar desses algoritmos e mecanismos, os **Sistemas Operacionais Móveis** enfrentam desafios contínuos devido à evolução constante de ameaças cibernéticas sofisticadas e à complexidade do ecossistema de aplicativos. A amável leitora deve ter usado um destes **Sistemas Operacionais Móveis** e percebido a ênfase significativa em segurança e privacidade, que estes sistemas aplicam. Novamente, o ambiente restrito e a necessidade de proteger dados sensíveis, como informações pessoais e financeiras, exigem uma abordagem rigorosa e adaptativa que não são muito diferentes das abordagens adotadas em **Sistemas Operacionais Embarcados**. A diferença é que, em um **Sistema Operacional Móvel**, o usuário é o responsável por autorizar o acesso a recursos sensíveis, enquanto que em um **Sistema Operacional Embarcado** o usuário não tem controle sobre o acesso a estes recursos. Nos dois casos, todos os processos, algoritmos e métodos de segurança estão limitados pelas restrições do consumo de energia o que não acontece com **Sistemas Operacionais** de servidores, _mainframes_ e computadores pessoais.

Eu enfatizei as restrições de consumo de energia em dois temas consecutivos para que a atenta leitora perceba que o consumo de energia é um dos principais desafios enfrentados pelos **Sistemas Operacionais Móveis**. A natureza portátil desses dispositivos exige que os **Sistemas Operacionais Móveis** implementem técnicas avançadas de gerenciamento de energia para maximizar a vida útil da bateria. Para enfrentar essa questão, os **Sistemas Operacionais Móveis** empregam estratégias sofisticadas de controle e monitoramento. Estas estratégias incluem o gerenciamento dinâmico de estados de energia dos componentes de hardware, como a `CPU`, que pode operar em modos de baixo consumo ou ser colocada em estados de suspensão, _sleep_, durante períodos de inatividade. 

As estratégias de gerenciamento de energia só são possíveis graças a relação simbiótica entre o **Sistema Operacional** e o hardware. Nesta relação, o **Sistema Operacional** é responsável por monitorar a atividade do usuário e ajustar dinamicamente o desempenho da `CPU` e outros componentes para otimizar o consumo de energia. Enquanto o hardware deve informar dados de consumo, frequência, atividade e operação ao **Sistema Operacional** . Novamente em um laço de realimentação positiva: há uma necessidade, cria-se um hardware que atenda a esta necessidade e o **Sistema Operacional** é adaptado para tirar proveito deste hardware, e voltamos ao início. Esta relação simbiótica força os arquitetos de **Sistema Operacionais Móveis** a criarem soluções novas e específicas.

O Android, por exemplo, implementa seu próprio sistema de gerenciamento de energia sobre o [Linux Power Management](https://docs.`Kernel`.org/power/index.html), utilizando _wake locks_ para permitir que aplicações requisitem recursos da `CPU` apenas quando necessário, garantindo que a `CPU` não consuma energia desnecessariamente se não houver aplicações ou serviços ativos demandando processamento. Um sistema de gestão de energia de servidores e computadores pessoais modificado e otimizado para uso em dispositivos móveis.

É possível que a perceptiva leitora tenha notado, nos últimos anos, a evolução das técnicas de gerenciamento de energia, transcendendo os modos manuais de economia para sistemas adaptativos e preditivos baseados em Inteligência Artificial. O **Android** introduziu recursos como **Adaptive Battery**, que aprende os padrões de uso do usuário para otimizar o consumo de energia, gerenciando o desempenho e a eficiência em segundo plano. Similarmente, o mercado especula que o **iOS**, a partir da versão $19$, deve introduzir otimizações de bateria baseadas em Inteligência Artificial, que aprendem os hábitos de uso de aplicativos, limitam atividades em segundo plano de forma inteligente e preveem necessidades de recarga, com todo o processamento dos algoritmos de aprendizagem de máquina ocorrendo no próprio dispositivo móvel para preservar a privacidade do usuário. Esta é uma transição importante no paradigma de gerenciamento de energia.

A atenta leitora deve ter em mente que esta transição para uma gestão energética proativa, na qual o **Sistema Operacional** antecipa e se adapta às necessidades do usuário de forma quase invisível, usando Inteligência Artificial, tenta aliviar o usuário da carga cognitiva de gerenciar manualmente essas configurações. Tirando a relação entre hardware e software do mundo determinístico da computação imperativa para o mundo probabilístico, mais fluido e adaptativo da Inteligência Artificial. Neste admirável mundo novo o sistema aprenderá e se ajustará continuamente às suas necessidades e não as necessidades de um usuário médio hipotético. Regozijai-vos! Estamos quase lá. Mas existem desafios éticos e de privacidade que devem ser considerados.

A crescente sofisticação dos sistemas de Inteligência Artificial no gerenciamento de energia e na personalização da experiência do usuário, embora traga benefícios evidentes em termos de usabilidade e eficiência, também levanta questões importantes sobre a privacidade dos dados de uso do dispositivo. Mesmo com o processamento ocorrendo localmente no dispositivo, como destacado para o **iOS**, a coleta e análise detalhada de hábitos de uso, quais aplicativos são usados, em que horários, possivelmente inferindo locais e rotinas, são inerentemente sensíveis. 

Neste cenário, surge um dilema entre a conveniência da automação inteligente, que torna o gerenciamento de recursos invisível e contínuo, e a manutenção da transparência e do controle por parte do usuário sobre as operações do seu dispositivo. Este equilíbrio delicado entre personalização avançada, privacidade e controle do usuário continuará a ser um campo de debate e desenvolvimento para os futuros **Sistemas Operacionais Móveis**, podendo influenciar as preferências dos consumidores e até mesmo levar a novas regulamentações sobre os dados utilizados por sistemas de Inteligência Artificial embarcados.

A @tbl-androidios1 apresenta um comparativo entre as duas principais plataformas de **Sistemas Operacionais Móveis**, **Android** e **iOS**, destacando suas características fundamentais e abordagens recentes, especialmente no que tange ao gerenciamento de energia.

| Característica | **Android** | **iOS** |
| :---- | :---- | :---- |
| **Arquitetura Base** | `Kernel` **Linux** | Derivado do macOS, arquitetura em camadas (Core OS, Core Services, Media, Cocoa Touch) |
| **Modelo de Distribuição** | Código aberto, personalizável por fabricantes | Plataforma fechada, proprietária da Apple |
| **Interface Predominante** | Interfaces de toque, alta personalização da UI pelos fabricantes | Interfaces de toque (multitoque, gestos), design de UI consistente e controlado pela Apple |
| **Gerenciamento de Energia** | **Android** Power Management, wake locks, Adaptive Battery, controles granulares (One UI) | Gerenciamento de energia integrado, otimização de bateria baseada em Inteligência Artifical (a partir do **Sistemas Operacionais Móveis** 19) |
| **Conectividade** | Amplo suporte: GSM/EDGE, CDMA, EV-DO, UMTS, LTE, 5G, Bluetooth, Wi-Fi, WiMAX | Amplo suporte: GSM/EDGE, CDMA, EV-DO, UMTS, LTE, 5G, Bluetooth, Wi-Fi |
| **Segurança** | Sandboxing de apps, modelo de permissões, criptografia, Google Play Protect | Sandboxing de apps, modelo de permissões, criptografia forte, Face ID/Touch ID, Secure Enclave |
| **Ecossistema de Aplicativos** | Google Play Store, permite sideloading (instalação de apps de fora da loja oficial) | Apple App Store, política restrita de distribuição de apps |

: Comparativo entre **Android** e **iOS**, destacando suas características fundamentais e abordagens recentes, especialmente no que tange ao gerenciamento de energia. {#tbl-androidios1}

A atenta leitora deve observar que esta comparação evidencia as filosofias distintas de design e as abordagens para desafios comuns. Parecendo indicar que as duas plataformas estão convergindo para soluções mais inteligentes e adaptativas.

### Sistemas Distribuídos

_Um sistema distribuído é conceitualmente definido como uma coleção de computadores autônomos que se comunicam e cooperam por meio de uma rede, mas que se apresentam aos seus usuários como um único sistema coerente e unificado_. Em sistemas distribuídos a leitora usará um número indefinido de máquinas mas todas serão vistas como uma só. Os principais objetivos para a construção de sistemas distribuídos são: o compartilhamento eficiente de recursos, hardware, software ou dados; o aumento de desempenho por meio do processamento paralelo; e a obtenção de maior confiabilidade e disponibilidade.

Para que um sistema distribuído funcione efetivamente e seja percebido como uma entidade única, ele deve exibir algumas características. Entre elas, a observadora leitora deve considerar a **transparência** como uma das mais importantes. Neste caso, _usamos a palavra transparência para fazer referência à capacidade do sistema de ocultar a separação e a distribuição de seus componentes dos usuários e dos programadores de aplicação_. Existem diversas formas de transparência, sendo a **transparência de localização** e a **transparência de acesso** particularmente comuns e dignas de nota: a **transparência de localização** garante que usuários e aplicações não precisem conhecer a localização física dos recursos, enquanto a **transparência de acesso** assegura que recursos locais e remotos sejam acessados utilizando operações idênticas, abstraindo os detalhes de como o acesso é realizado. Outras formas de transparência incluem as transparências de replicação, correção de falhas, concorrência e migração, todas contribuindo para a ilusão de um sistema único.

A **escalabilidade** é outra característica importante. Neste caso, _a escalabilidade denota a capacidade do sistema operar de forma eficaz e eficiente em diferentes escalas, ou seja, de se adaptar ao aumento da demanda por recursos sem que haja uma degradação significativa no desempenho ou a necessidade de alterar fundamentalmente o software ou as aplicações existentes_. Em um mundo ideal, o céu será sempre azul, os ventos sempre justos, e o processamento será independente do tamanho da rede. No entanto, chove e a escalabilidade pode ser limitada por gargalos como algoritmos centralizados, dados centralizados ou serviços centralizados que atendem a todos os usuários e que não possam ser distribuídos por limitações técnicas, econômicas ou de segurança. O mundo ideal só existe nos nossos sonhos, planos e desejos.

Sistemas, independentemente do seu tamanho e função, falham. _A **tolerância a falhas** é a propriedade que permite a um sistema distribuído continuar operando corretamente, possivelmente com desempenho degradado, mesmo quando alguns de seus componentes falham_. Isso é geralmente alcançado por meio da redundância de hardware, software e dados, combinada com um design de software que permite a recuperação do último estado consistente após a detecção de uma falha. As falhas podem ser classificadas como  transientes, ocorrem uma vez e desaparecem; intermitentes, ocorrem esporadicamente; ou permanentes, persistem até serem reparadas. Intimamente relacionada à tolerância a falhas está a **disponibilidade**, que em **sistemas distribuídos implica que a falha de um componente deve afetar apenas a parte do sistema que utiliza diretamente aquele componente, permitindo que o restante continue funcional**.

Atualmente, uma das tendências mais significativas no desenvolvimento de sistemas distribuídos, com impacto direto na criação de **Sistemas Operacionais**, é a adoção da **arquitetura de microsserviços**, que propõe _a decomposição de aplicações monolíticas complexas em um conjunto de serviços menores, independentes e fracamente acoplados_. Cada microsserviço executa seu próprio processo e se comunica com outros serviços por meio de `APIs` leves, frequentemente baseadas nos protocolos HTTP/REST. Esta abordagem oferece benefícios como implantação independente de cada serviço, escalabilidade granular, permitindo que apenas os serviços mais demandados sejam escalados, e a possibilidade de usar diferentes tecnologias para diferentes serviços. A integração de microsserviços com **tecnologias de conteinerização**, como [Docker](https://www.docker.com/), e orquestradores, como [Kubernetes](https://kubernetes.io/), tem se mostrado particularmente eficaz para aumentar a tolerância a falhas e a resiliência, pois falhas em um microsserviço podem ser isoladas sem derrubar toda a aplicação. A @fig-docker1 mostra uma arquitetura de microsserviços utilizando containers Docker, onde cada serviço é isolado em seu próprio container, permitindo uma gestão eficiente e escalável.

::: {#fig-docker1}
![](/images/docker_container_diagram.webp)

Esta figura ilustra a estrutura hierárquica da virtualização em nível de **Sistema Operacional** utilizando tecnologia Docker. A arquitetura apresenta cinco camadas principais: (1) Camada de Hardware, contendo os recursos físicos fundamentais (CPU multi-core, memória RAM, armazenamento SSD/HDD, interfaces de rede e dispositivos de E/S); (2) `kernel` **Linux**, destacado como componente central que implementa as tecnologias essenciais para o uso de containers, Namespaces para isolamento de processos, **Cgroups** para limitação de recursos, SELinux para segurança, UnionFS para sistema de arquivos em camadas e Netfilter para gerenciamento de rede; (3) **Sistema Operacional** Ubuntu, fornecendo bibliotecas do sistema, shell, utilitários e serviços; (4) Docker Engine, implementando o daemon Docker, runtime de containers (containerd/runc), CLI e gerenciamento de rede; e (5) Containers Docker, demonstrados através de três instâncias isoladas executando aplicação web (Node.js), banco de dados (PostgreSQL) e _cache_ em memória (Redis). As setas verticais indicam a dependência hierárquica entre camadas, enquanto a legenda lateral destaca as características fundamentais dos containers: isolamento via Namespaces, limitação de recursos via Cgroups e compartilhamento eficiente do mesmo `kernel` **Linux** entre todos os containers, demonstrando como esta arquitetura oferece virtualização leve com overhead computacional mínimo comparado às máquinas virtuais tradicionais.
:::

::: callout-note
**Containers: Virtualização no Nível do Sistema Operacional**

Os **containers** representam uma forma leve e eficiente de virtualização que opera no nível do **Sistema Operacional** , permitindo que múltiplas aplicações isoladas compartilhem o mesmo `Kernel` do sistema hospedeiro. _Diferentemente das máquinas virtuais tradicionais, que virtualizam hardware completo, os containers virtualizam apenas o espaço do usuário, criando ambientes isolados que compartilham recursos do **Sistema Operacional** subjacente_.

Graças a sua arquitetura, os containers consomem significativamente menos recursos que máquinas virtuais, com custo computacional extra típico inferior a $2\%$ permitindo:

- **Isolamento de Processos**: cada container executa em seu próprio espaço de usuário isolado, com processos, sistema de arquivos e interfaces de rede separados;

- **Compartilhamento de `Kernel`**: todos os containers em um sistemas hospedeiro compartilham o mesmo `Kernel` do **Sistema Operacional** , eliminando a sobrecarga de múltiplos sistemas operacionais;

- **Portabilidade**: containers encapsulam aplicações e suas dependências, garantindo execução consistente em diferentes ambientes;

Os containers **Linux** baseiam-se em duas tecnologias principais do `Kernel`. os containers de **Namespaces**: criam isolamento de recursos como processos, rede e sistema de arquivos enquanto os de **Cgroups**, em inglês **C**ontrol **groups**: limitam e monitoram o uso de recursos como `CPU`, memória e `E/S`.

Esta combinação permite que o **Sistema Operacional** mantenha múltiplos ambientes isolados sem a complexidade de virtualização completa de hardware. Os containers tem aplicação direta na facilitação da implantação de aplicações, na criação e gestão de microsserviços, permitindo que cada serviço seja implantado independentemente e, como não poderia deixar de ser, na computação em nuvem: oferecendo densidade superior de aplicações por servidor físico. Plataformas como [**Docker**](https://www.docker.com/) popularizaram esta tecnologia, enquanto orquestradores como [**Kubernetes**](https://kubernetes.io/) gerenciam containers em escala empresarial.
:::

Outra tendência proeminente é a arquitetura orientada a eventos, em inglês **E**vent-**D**riven **A**rchitecture - **EDA**. Em sistemas **EDA**, os componentes reagem a eventos. Estes eventos são caracterizados por notificações assíncronas que representam ocorrências operacionais, promovendo um baixo acoplamento entre estas ocorrências e o **Sistema Operacional** facilitando a escalabilidade. Por exemplo, em um sistema de comércio eletrônico, a conclusão de uma compra pode gerar um evento que é consumido por outros serviços, como o de faturamento, o de notificação ao cliente e o de expedição, sem que o serviço de compra precise conhecer diretamente esses outros serviços. O uso de servidores de mensagens, como [Apache Kafka](https://kafka.apache.org/), é comum em **EDA**s para mediar a comunicação assíncrona.

O **modelo distribuído [AKKA](https://akka.io/)**, implementado por um conjunto de ferramentas de desenvolvimento e um ambiente de execução para construir aplicações concorrentes, distribuídas e resilientes na **JVM**, **J**ava **V**irtual **M**achine, baseado no modelo de atores, também ganhououtração para a construção de sistemas concorrentes e distribuídos resilientes e escaláveis. Neste cenário, os atores são entidades computacionais leves que se comunicam exclusivamente por meio da troca de mensagens assíncronas, embora padrões síncronos como `ask` possam ser implementados sobre a comunicação assíncrona `tell`. Estes atores podem ser distribuídos em um cluster de máquinas, permitindo que aplicações complexas sejam construídas a partir da composição de múltiplos atores colaborando para um objetivo comum. Aqui há uma relação interessante entre **Sistemas Operacionais**, sistemas distribuídos, máquinas virtuais e linguagens de programação. O AKKA é uma implementação do modelo de atores, que foi proposto por [Carl Hewitt](https://en.wikipedia.org/wiki/Carl_Hewitt) em 1973, e que foi inspirado no conceito de processos concorrentes do [Lisp](https://pt.wikipedia.org/wiki/Lisp_(linguagem_de_programa%C3%A7%C3%A3o)). O **AKKA** foi escrito em [Scala](https://www.scala-lang.org/), uma linguagem funcional que roda na JVM, e que permite a criação de aplicações distribuídas e reativas com alta performance e baixa latência.

A esperta leitora deve considerar que as novas tendências arquitetônicas, como microserviços e **EDA**, não surgem isoladamente, mas como respostas evolutivas diretas aos desafios de concretizar as características fundamentais de escalabilidade e tolerância a falhas em sistemas que se tornam progressivamente mais complexos e com demandas crescentes. Aplicações monolíticas tradicionais enfrentam dificuldades intrínsecas para escalar componentes individuais de forma granular ou para isolar falhas eficazmente; uma falha em um módulo pode comprometer todo o sistema. Em contraste, a arquitetura de microserviços, ao decompor a aplicação em unidades menores e independentes, permite que cada serviço seja escalado conforme sua necessidade específica e que falhas sejam contidas dentro do serviço afetado, preservando a funcionalidade do restante do sistema. Similarmente, a **EDA**, ao promover o desacoplamento por meio da comunicação assíncrona baseada em eventos, aumenta a resiliência, os serviços não dependem diretamente da disponibilidade imediata uns dos outros e a escalabilidade, os produtores de eventos podem operar independentemente dos consumidores.

A proliferação de componentes distribuídos, sejam eles microserviços, atores,  os inúmeros dispositivos de borda em um sistema de **IoT**, acarreta um aumento exponencial na complexidade do gerenciamento do sistema como um todo. Manter a coerência, a eficiência, o monitoramento e a depuração em um ambiente com milhares ou milhões de partes móveis é um desafio formidável que deve ser enfrentado pelos **Sistemas Operacionais**. Isso aponta para uma possível evolução em direção a **Sistemas Operacionais** Distribuídos,  camadas de gerenciamento de sistema equivalentes, que incorporem níveis mais elevados de Inteligência Artificial e aprendizado de máquina. Tais sistemas poderiam realizar auto-configuração, auto-otimização, auto-reparação e gerenciamento proativo de recursos de forma mais autônoma, uma trajetória análoga à observada nos **Sistemas Operacionais Móveis** com suas capacidades adaptativas de gerenciamento de energia.

A tabela a seguir resume as propriedades essenciais dos sistemas distribuídos e como as tendências arquitetônicas modernas se alinham e aprimoram essas propriedades.

| Paradigma/Característica | Descrição | Tecnologias/Exemplos Chave | Benefícios Principais |
| :---- | :---- | :---- | :---- |
| **Transparência** (Localização, Acesso) | Ocultar a distribuição dos componentes, permitindo acesso uniforme a recursos locais/remotos. | Middleware, RPC, Nomes de Serviço. | Simplificação do desenvolvimento, percepção de sistema único. |
| **Escalabilidade** | Capacidade de operar eficientemente em diferentes escalas, adaptando-se ao aumento da demanda. | Balanceamento de Carga, Replicação, Particionamento de Dados. | Suporte ao crescimento, desempenho consistente. |
| **Tolerância a Falhas** | Continuar operando corretamente mesmo com falhas em componentes, por meio de redundância e recuperação. | Replicação de Dados/Serviços, Checkpointing, transações Distribuídas. | Alta disponibilidade, resiliência. |
| **Arquitetura de Microserviços** | Decomposição da aplicação em pequenos serviços independentes e fracamente acoplados. | Docker, Kubernetes, `APIs`REST/gRPC. | Implantação independente, escalabilidade granular, diversidade tecnológica, resiliência. |
| **Arquitetura Orientada a Eventos (EDA)** | Sistemas reagem a eventos assíncronos, promovendo baixo acoplamento e escalabilidade. | Apache Kafka, RabbitMQ, Filas de Mensagens. | Desacoplamento, escalabilidade, resiliência, capacidade de resposta em tempo real. |
| **Computação de Borda/Névoa** | Processamento de dados mais próximo da origem, reduzindo latência e uso de banda. | Dispositivos IoT, Gateways de Borda, Edge AI, Plataformas de Fog Computing. | Baixa latência, economia de banda, processamento em tempo real, privacidade aprimorada. |

: Propriedades essenciais dos sistemas distribuídos e como as tendências arquitetônicas modernas se alinham e aprimoram essas propriedades. {#tbl-distributed-systems}

Esta visão panorâmica conecta os conceitos teóricos fundamentais dos sistemas distribuídos com as implementações práticas e as tendências que estão moldando ativamente este campo vital da computação. A evolução dos sistemas distribuidos levou a computação em nuvem.

### Computação em Nuvem: transformando o Design de Sistemas Operacionais

A [computação em nuvem](https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf) representou uma das transformações mais profundas na arquitetura de **Sistemas Operacionais** desde o advento da multiprogramação. _Esta revolução tecnológica não apenas alterou como recursos computacionais são disponibilizados e consumidos, mas fundamentalmente redefiniu os requisitos e expectativas que recaem sobre os **Sistemas Operacionais** modernos_. A computação em nuvem é, em última instância, uma atualização da **computação distribuida**, que vimos antes, com algumas características de interfacemento com usuários e de demanda. A curiosa leitora pode começar a compreender impacto destas tecnologias observando como as características essenciais da computação em nuvem criaram novos paradigmas de design que os **Sistemas Operacionais** devem abraçar. Imagine um sistema na nuvem, a primeira coisa que vem a mente é que a própria leitora terá que configurar suas máquinas e seus serviços. Isto é **autoatendimento**.

O conceito de autoatendimento sob demanda, que permite aos usuários provisionarem recursos computacionais automaticamente sem intervenção humana de terceiros, impõe aos **Sistemas Operacionais** a necessidade de suportar automação extensiva por meio de interfaces de programação robustas. Esta característica elimina a configuração manual tradicional e exige que o **Sistema Operacional** seja capaz de responder dinamicamente a requisições de provisionamento, configurando-se automaticamente conforme demandas específicas. Simultaneamente, o amplo acesso à rede, outra característica fundamental da nuvem, demanda que **Sistemas Operacionais** sejam otimizados para uma heterogeneidade de plataformas e dispositivos nunca antes vista, necessitando de adaptabilidade e eficiência operacional em ambientes de rede complexos e distribuídos.

A implementação de agrupamento de recursos em **modelos multilocatários** introduz complexidades arquiteturais significativas que transcendem os desafios tradicionais de segurança e isolamento. Modelos multilocatários em inglês multitenancy, referem-se a uma arquitetura de software na qual uma única instância de uma aplicação ou sistema serve múltiplos clientes independentes, chamados de locatários ou inquilinos, no inglês tenants. Neste modelos _Os **Sistemas Operacionais** devem garantir que recursos físicos e virtuais sejam dinamicamente alocados entre múltiplos usuários sem comprometer a privacidade, a performance ou a integridade dos dados_.

Para entender este modelo multilocatário a criativa leitora pode imaginar um serviço de email como o Gmail. Milhões de usuários diferentes usam a mesma plataforma, mesmos servidores, mesmo software, mas cada usuário vê apenas seus próprios emails e configurações. Os dados do usuário $A$ nunca aparecem para o usuário $B$, mesmo estando no mesmo sistema. Esta característica da nuvem exige mecanismos sofisticados de virtualização que vão além das implementações tradicionais, incorporando controle de acesso granular e técnicas de isolamento que permitem o compartilhamento seguro da mesma infraestrutura física entre múltiplos locatários.

A **elasticidade** emerge como o desafio mais significativo e transformador para os **Sistemas Operacionais** modernos. _Esta capacidade de ajustar dinamicamente os recursos disponíveis de acordo com a demanda representa uma mudança fundamental na forma como é concebido o gerenciamento de recursos computacionais_. Os **Sistemas Operacionais** devem ser capazes de expandir e contrair a disponibilidade de recursos como `CPU`, memória e armazenamento, em tempo real, respondendo a flutuações de demanda sem degradação perceptível de performance. Isso requer arquiteturas que suportem tanto escalonamento horizontal, adicionando mais instâncias de recursos, quanto vertical, aumentando a capacidade de recursos existentes, tudo isso acontecendo de forma automática e transparente. A eficiência da elasticidade depende da capacidade de provisionamento dos sistemas.

O **provisionamento sob demanda**, que permite a criação instantânea de recursos computacionais, impõe requisitos rigorosos de velocidade e automação aos **Sistemas Operacionais**. _Estes devem suportar inicialização extremamente rápida, configuração automática inteligente e integração perfeita com interfaces de programação_, permitindo que recursos sejam disponibilizados em questão de segundos por meio de portais web ou chamadas de API. Esta capacidade de resposta instantânea requer modificações e personalizações específicas para este cenário de provisionamento nos processos de `boot`, inicialização de serviços e configuração de rede.

Estando na nuvem ou não, recursos computacionais são caros. A precavida leitora terá que pagar pelos recursos que usar. Para que estes recursos sejam cobrados eles precisam ser medidos. Assim, o paradigma de **serviço mensurado** introduz uma dimensão econômica direta no design de **Sistemas Operacionais**. _Esta característica indispensável para a engenharia econômica dos sistemas em nuvem exige que os **Sistemas Operacionais** implementem capacidades extensivas de monitoramento e coleta de dados de utilização de recursos_. Deste ponto em diante, neste livro, esses dados de utilização serão chamados de métricas. Em sistemas na nuvem cada operação, cada ciclo de `CPU`, cada byte de memória ou cada byte transferido deve ser contabilizado com precisão para facilitar modelos de cobrança por uso e permitir otimizações realizadas sobre dados reais de uso e demanda. Esta necessidade de instrumentação abrangente influencia profundamente a arquitetura interna dos **Sistemas Operacionais**, exigindo sistemas de telemetria integrados nos **Sistemas Operacionais** desde a sua concepção.

#### A Influência dos Modelos de Serviço

Os modelos de serviço que emergiram como consequencia da popularização da computação em nuvem também exercem influência profunda no design de **Sistemas Operacionais**, cada um destes modelos cria demandas específicas e requer cuidados personalizados. Três desses modelos de serviço tem impacto relevante no projeto, desenvolvimento e operação de Sistemas Operacionais:

1. O **Software como Serviço** **S**oftware **a**s **a** **S**ervice, **SaaS**, representa o modelo de computação em nuvem mais próximo do usuário final, no qual aplicações completas são entregues pela internet como serviços prontos para uso, eliminando a necessidade de instalação, manutenção ou gerenciamento local de software pelos usuários. _Neste paradigma, exemplificado por serviços como Gmail, Salesforce ou Microsoft 365, os usuários acessam funcionalidades complexas por meio de navegadores web ou aplicativos leves, enquanto toda a infraestrutura computacional permanece centralizada e gerenciada pelo provedor do serviço_. Este modelo requer **Sistemas Operacionais** especificamente adaptados e otimizados para hospedar aplicações multi-usuário massivas com garantias rigorosas de alta disponibilidade e performance consistente, capazes de servir simultaneamente milhões de usuários concorrentes sem degradação perceptível de serviço. _O **Sistema Operacional** deve tornar-se completamente invisível e transparente ao usuário final, focando exclusivamente na eficiência de execução de aplicações_ e na otimização de recursos para maximizar a capacidade de atendimento simultâneo. Esta invisibilidade operacional exige que o sistema abstrai completamente a complexidade da infraestrutura subjacente, incluindo gerenciamento de sessões de usuário, balanceamento de carga dinâmico, replicação de dados em tempo real e recuperação automática de falhas, tudo isso ocorrendo de forma transparente enquanto mantém a ilusão de um serviço único e coeso para cada usuário individual.

2. A **Plataforma como Serviço**, **P**latform **a**s **a** **S**ervice, **PaaS**, representa um modelo de computação em nuvem que fornece uma plataforma completa de desenvolvimento e implantação, permitindo aos desenvolvedores criar aplicações sem se preocupar com a complexidade da infraestrutura subjacente. _Este paradigma apresenta demandas arquiteturais diferentes mas igualmente exigentes aos **Sistemas Operacionais**, que devem atuar como uma fundação invisível porém robusta para ecossistemas de desenvolvimento inteiros_. Os ambientes **PaaS** requerem **Sistemas Operacionais** capazes de suportar uma diversidade extraordinária de linguagens de programação, desde Python e Java até linguagens emergentes, bem como suas respectivas bibliotecas, frameworks e dependências específicas, tudo isso de forma simultânea e isolada. _Esta flexibilidade linguística deve coexistir com capacidades robustas de isolamento entre aplicações em desenvolvimento_, garantindo que projetos de diferentes equipes ou organizações não interfiram uns com os outros mesmo compartilhando a mesma infraestrutura física. O gerenciamento automático de recursos de desenvolvimento constitui outro requisito fundamental, englobando a orquestração  transparente de ambientes de teste, processos de compilação automatizados e pipelines de implantação contínua, todas essas funcionalidades devendo ser integradas nativamente ao **Sistema Operacional** para proporcionar uma experiência de desenvolvimento fluida e eficiente.

3. A Infraestrutura como Serviço, **I**nfrastructure **a**s **a** **S**ervice, **IaaS**, constitui o modelo mais fundamental da computação em nuvem, oferecendo recursos de computação virtualizados sob demanda, incluindo servidores virtuais, armazenamento e redes, permitindo aos usuários construir suas próprias soluções sobre uma infraestrutura física compartilhada. _Este modelo exige que **Sistemas Operacionais** funcionem eficientemente como hospedeiros de virtualização de alta performance, atuando como uma camada de orquestração sofisticada que deve maximizar a utilização do hardware físico enquanto mantém isolamento perfeito entre inquilinos_. Os sistemas devem suportar a criação, gerenciamento e destruição de múltiplas instâncias de **Sistemas Operacionais** convidados de forma dinâmica e escalável, oferecendo aos usuários a flexibilidade de provisionar desde pequenas instâncias para desenvolvimento até poderosos servidores virtuais para cargas de trabalho empresariais críticas. _A performance destas máquinas virtuais deve aproximar-se significativamente do hardware nativo_, um requisito que demanda a implementação de técnicas avançadas de virtualização assistida por hardware, como [Intel VT-x](https://www.thomas-krenn.com/en/wiki/Overview_of_the_Intel_VT_Virtualization_Features) e [AMD-V](https://fastneuron.com/forum/showthread.php?tid=5085), bem como otimizações específicas para cargas de trabalho virtualizadas que minimizem a sobrecarga introduzida pelas camadas de abstração. O **Sistema Operacional** hospedeiro deve gerenciar eficientemente recursos como memória, `CPU` e `E/S` entre múltiplas máquinas virtuais concorrentes, implementando algoritmos de escalonamento e alocação que garantam tanto performance quanto isolamento, enquanto fornece interfaces de gerenciamento que permitam aos usuários controlar seus recursos virtuais como se fossem hardware físico dedicado.

Esta convergência de requisitos transform  fundamentalmente como **Sistemas Operacionais** são concebidos e implementados. _A computação em nuvem não representa meramente uma evolução incremental, mas uma reconfiguração fundamental dos princípios que governam o design de Sistemas Operacionais_. As arquiteturas resultantes devem ser mais modulares, eficientes em recursos e capazes de operação verdadeiramente autônoma em ambientes distribuídos e dinamicamente reconfiguráveis, estabelecendo novos paradigmas que continuarão a influenciar o desenvolvimento de **Sistemas Operacionais** nas próximas décadas.

### Inteligência Artificial e Modelos de Linguagem de Grande Escala (LLMs)

Os LLMs, como o GPT-4 ou modelos similares, são exemplos excelentes para avaliação dos impactos que as tecnologias de Inteligência Artificial terão sobre os **Sistemas Operacionais**. Os LLMs requerem recursos computacionais significativos, geralmente executados em sistemas de computação de alto desempenho equipados com [GPUs](https://www.nvidia.com/en-us/technologies/), [TPUs](https://www.nvidia.com/en-us/technologies/) ou [LPUs](https://groq.com/the-groq-lpu-explained/). Esses modelos possuem bilhões de parâmetros, exigindo processamento paralelo eficiente. Neste caso, a criativa leitora deve considerar que a execução de **LLMs** sobrecarrega a `CPU`, `GPU` e memória, exigindo **Sistemas Operacionais** que otimizem a alocação de recursos. Além disso, o treinamento e a inferência de **LLMs** consomem grandes quantidades de energia, com estimativas de até [1.287.000 kWh](https://www.oneadvanced.com/resources/large-language-models-part-1-hardware-and-software-aspects/) para treinamento, gerando preocupações ambientais com emissões de carbono de cerca de $552$ toneladas e colocando pressão sobre os **Sistemas Operacionais** para implementar técnicas de gerenciamento de energia mais eficientes. Finalmente os **Sistemas Operacionais** precisam gerenciar eficientemente grandes quantidades de memória para suportar os modelos, especialmente em dispositivos de borda, dispositivos móveis e embarcados com recursos limitados.

A necessidade de cálculos numéricos, operações com matrizes, necessárias aos algoritmos de Inteligência Artificial, fez com que o hardware evoluísse para novas formas de processamento: as `GPU`s, `TPU`s, e `LPU`s. 

As `GPU`s, originalmente projetadas para renderização gráfica, evoluíram para computação paralela de alta performance, exigindo drivers específicos como [CUDA](https://developer.nvidia.com/cuda-toolkit) para NVIDIA ou [ROCm](https://www.amd.com/en/products/software/rocm.html) para AMD, além de bibliotecas que gerenciem o paralelismo massivo. Isso requer otimizações no **Sistema Operacional** para lidar com tarefas de computação intensiva, como alocação de memória em `GPU` e comunicação eficiente entre `CPU` e `GPU`. Já as `TPUs`, desenvolvidas pelo Google para acelerar tarefas de aprendizado de máquina, demandam integração com frameworks como [TensorFlow](https://www.tensorflow.org/?hl=pt-br), necessitando que os **Sistemas Operacionais** suportem `API`s específicas e gerenciem a comunicação com esses chips otimizados para operações de tensor, o que pode incluir ajustes no `kernel` para chamadas de sistema ou gerenciamento de energia. As `LPUs`, como as projetadas pela Groq para processamento de linguagem natural, requerem bibliotecas especializadas e otimizações para execução de modelos de linguagem, focando em baixa latência e alta eficiência em tarefas de inferência e treinamento. A @fig-gputpulpu permite a comparação entre estas tecnologias de hardware especializado.

::: {#fig-gputpulpu}
![](/images/gpu_tpu_lpu_comparison.webp)

O diagrama apresenta três colunas verticais representando as arquiteturas modernas de processadores de IA mais avançadas: GPU (NVIDIA Hopper H100), TPU (Google v6e Trillium) e LPU (Groq TSP v1). Cada coluna segue uma organização hierárquica de camadas, do nível de aplicação até o hardware físico.
:::

Estes dispositivos especiais também demandam novas abstrações de programação, com `API`s e frameworks como [OpenCL](https://www.khronos.org/opencl/), [Vulkan](https://www.vulkan.org/) ou [XLA](https://openxla.org/xla?hl=pt-br) integrados ao **Sistema Operacional** para facilitar o desenvolvimento de aplicações, simplificando a complexidade para os desenvolvedores. Na operação, o impacto se reflete no desempenho e eficiência. GPUs exigem que os **Sistemas Operacionais** gerenciem kernels gráficos e computacionais, garantindo baixa latência para aplicações como jogos ou renderização 3D e alta taxa de transferência para computação científica. `TPU`s, frequentemente usadas em data centers, requerem **Sistemas Operacionais** otimizados para `pipelines` de dados que alimentem grandes volumes de informações, minimizando o custo computacional extra de comunicação. `LPUs`, por sua vez, priorizam baixa latência em inferências de modelos de linguagem, essenciais para aplicações em tempo real como _chatbots_ ou assistentes de Inteligência Artificial, exigindo ajustes no agendador de tarefas do sistema para priorizar essas tarefas. Exemplos práticos ilustram esses impactos:

1. O **Linux**, amplamente usado em servidores e `data centers`, suporta `GPU`s, `TPU`s e, potencialmente, `LPU`s por meio de drivers e frameworks como CUDA, ROCm e TensorFlow, beneficiando-se de um `kernel` modular que facilita a adição de suporte a novos dispositivos.

2. O Windows, otimizado para `GPU`s em jogos e aplicações gráficas, suporta [DirectX](https://microsoft.github.io/DirectX-Specs/) e CUDA, sendo menos comum para `TPU`s, mas deve ser capaz de suportar `LPU`s via [WSL](https://learn.microsoft.com/en-us/windows/wsl/about).

3. Sistemas embarcados, como o Android, otimizam `GPU`s como [Adreno](https://www.qualcomm.com/products/technology/processors/adreno) ou [Mali](https://www.arm.com/products/silicon-ip-multimedia) para gráficos e, cada vez mais, para aprendizado de máquina com suporte a `TPU`s e `LPU`s em chips como o [Google Tensor](https://cloud.google.com/tpu), demonstrando a adaptação dos **Sistemas Operacionais** a esses dispositivos especializados.

Existem também preocupações de segurança associadas ao uso de **LLMs**. Os **LLMs** podem ser explorados para gerar conteúdo malicioso, como e-mails de `phishing` ou código malicioso, representando riscos de segurança. Os **Sistemas Operacionais** precisam se adaptar para mitigar esses problemas atuando na **Detecção de Conteúdo Malicioso**, os **Sistemas Operacionais** podem e devem incorporar ferramentas de segurança avançadas para identificar e bloquear conteúdo gerado por **LLMs** que possa comprometer a segurança. Os **Sistemas Operacionais** também devem implementar medidas de **Proteção de Dados**, sistemas como os **LLMs** frequentemente requerem acesso a grandes quantidades de dados do usuário. Os **Sistemas Operacionais** precisam implementar medidas robustas de proteção de dados para evitar vazamentos.

#### Integração de LLMs em Sistemas Operacionais

A atenta leitora vai lembrar que já discutimos a integração de Inteligência Artificial nos **Sistemas Operacionais** na @sec-desafios-arquitetura-operacao. Agora precisamos voltar para a interação com os usuários e analisar qual será o impacto dos Modelos de Linguagem nos **Sistemas Operacionais**. Paulatinamente aparecem tendências e pesquisas indicando que os **LLMs** estão sendo integrados aos **Sistemas Operacionais**, por uma tecnologia conhecida como **L**arge **L**anguage **Mo**del**s** ou, com um pouco de marketing podemos chamar de [LLMOS Revolution](https://medium.com/%40lucien1999s.pro/llmos-revolutionizing-operating-systems-with-large-language-models-86ff61a714a4), com a esperança de transformar a interação entre usuários e dispositivos. Esta integração começa a ser percebida na possiblidade de integração de **LLMs** diretamente no **Sistema Operacional** , funcionando como um `Kernel` para interações em linguagem natural. Parte desta tecnologia pode ser vista no [LLMO](https://www.llmo.org/), um **Sistema Operacional** que utiliza **LLMs** para fornecer uma interface de usuário baseada em linguagem natural, permitindo que os usuários interajam com o sistema de forma mais intuitiva e eficiente.

Em **Sistemas Operacionais** tradicionais, como o Windows, podemos ver esta integração por meio de **APIs e Plugins**. Um bom exemplo pode ser visto observando os assistentes inteligentes, como o [Windows Copilot](https://copilot.microsoft.com/), utilizam `APIs`para integrar capacidades de **LLMs**, permitindo comandos simplificados e algum nível de automação. Por fim, já existem aplicações especializadas. Aplicativos que aproveitam **LLM**s, como ferramentas de geração de texto ou análise de dados, dependem de **Sistemas Operacionais** para gerenciar suas operações. Um bom exemplo de aplicação especializada é o [GitHub Copilot](https://github.com/features/copilot). O GitHub Copilot é uma ferramenta de programação desenvolvida pelo GitHub e pela OpenAI. Ele funciona como um assistente inteligente que se integra a editores de código, como o próprio [VS Code](https://code.visualstudio.com/), e sugere linhas de código ou funções inteiras em tempo real, enquanto o desenvolvedor digita.

Neste momento da história em que o pobre autor tem a ousadia de escrever, a integração de **LLMs** em **Sistemas Operacionais** parece ter impacto positivo na experiência do usuário por meio de interfaces mais intuitivas e inteligentes. Os assistentes de voz evoluem para compreender contexto, nuances linguísticas e intenções implícitas, permitindo conversas naturais que transcendem comandos rígidos e pré-definidos. Esta capacidade estende-se ao processamento de comandos complexos expressos em linguagem natural, na qual usuários podem descrever tarefas multifacetadas sem conhecer sintaxes específicas ou sequências de operações técnicas. Acrescente a isso, amável leitora, que a **capacidade de busca e integração semântica** representa uma transformação paradigmática. Um degrau de evolução. A perspicaz leitora pode quantificar este impacto focando no sistema de arquivos. Considere que a integração semântica irá permitir que **Sistemas Operacionais** compreendam intenções de busca independentemente de palavras-chave exatas. Usuários podem localizar arquivos, aplicações e informações por meio de descrições conceituais, relacionamentos semânticos e associações contextuais, integrando busca multimodal que correlaciona texto, imagens, áudio e vídeo de forma unificada e inteligente. Porém, há sempre um porém. _A implementação de **LLMs** em **Sistemas Operacionais** apresenta limitações técnicas e operacionais que afetam sua integração prática_. O consumo de recursos computacionais é substancial, com modelos requerendo quantidades específicas de memória e capacidade de processamento que podem impactar a performance global do sistema, particularmente em dispositivos com recursos limitados como smartphones, tablets e sistemas embarcados. Esta demanda computacional exigirá um balanceamento entre funcionalidade e capacidade computacional um pouco mais delicada do que as escolhas que já existem nos sistemas puramente determinísticos.

Além dos limites impostos pelos limites de memória e velociade, existem as preocupações relacionadas com a privacidade. Precisamos considerar que a operação destes modelos envolvem coleta, processamento e armazenamento de dados pessoais. Os modelos requerem acesso a padrões de uso, preferências comportamentais e dados contextuais para funcionalidade efetiva, criando requisitos específicos para conformidade regulatória, gestão de consentimento e proteção de dados. Existem perigos relacionados ao mal uso destes dados embutidos nesta integração.

Para terminar os desafios. Podemos falar em confiabilidade. A confiabilidade dos **LLMs** apresenta características operacionais específicas, incluindo variabilidade nas respostas geradas, potencial para produzir saídas imprecisas ou contextualmente inadequadas, e dependência de dados de treinamento que podem conter vieses. Esta variabilidade requer implementação de mecanismos de validação, monitoramento de saídas e sistemas de verificação para operação consistente do **Sistema Operacional** .

A tabela @tbl-llms1 resume os desafios que serão, e já estão sendo, enfrentados por desenvolvedores de sistmeas operacionais.

| **Aspecto** | **Desafio**  | **Efeito no Sistema Operacional**  |
|----------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------|
| Recursos Computacionais | Alta demanda por GPUs/TPUs e memória | Necessidade de otimização de alocação de recursos|
| Consumo de Energia| Uso intensivo de energia durante treinamento e inferência | Gestão de energia eficiente para reduzir custos e impacto ambiental|
| Segurança| Geração de conteúdo malicioso  | Implementação de ferramentas de detecção e mitigação|
| Privacidade | Acesso a grandes quantidades de dados do usuário | Medidas robustas de proteção de dados|
| Confiabilidade | Saídas imprevisíveis ou tendenciosas | Validação e monitoramento contínuos

: Desafios da Integração de LLMs em **Sistemas Operacionais** e seus Efeitos {#tbl-llms1}

### O Impacto da Computação Quântica em Sistemas Operacionais

A possibilidade de computação quântica efetiva e prática representa uma mudança de paradigma fundamental, utilizando como elemento fundamental os **qubits** que podem existir em superposição, $0$, $1$ ou combinação de ambos, e **emaranhamento** entre múltiplos **qubits**. Essas propriedades permitem explorar um espaço computacional vastamente maior e realizar alguns cálculos exponencialmente mais rápidos que os computadores clássicos, com potencial para resolver problemas NP-difíceis em otimização, simulação molecular, criptografia e aprendizado de máquina.

::: callout-note
**Problemas NP-difíceis** são uma classe de problemas computacionais extremamente desafiadores que não possuem algoritmos conhecidos capazes de resolvê-los em tempo polinomial, ou de forma eficiente, em computadores clássicos. Exemplos destes problemas incluem o problema do caixeiro viajante, fatoração de números grandes e o Problema da Mochila, _Knapsack Problem_. Estes problemas são fundamentais em criptografia, logística e simulação científica. A computação quântica oferece potencial para resolver alguns destes problemas exponencialmente mais rápido que métodos clássicos, talvez instantaneamente, representando uma das principais motivações para o desenvolvimento de tecnologias quânticas.
:::

Atualmente, a computação quântica encontra-se na era **NISQ**, **N**oisy **I**ntermediate-**S**cale **Q**uantum, com **qubits** limitados e suscetíveis a ruído e decoerência, restringindo a profundidade dos circuitos executáveis e a precisão dos cálculos. As características e limitações dos sistemas computacionais quânticos tornam necessária a existência de um **Sistema Operacional Quântico** em inglês **Q**uantum **C**omputer **O**perational **S**ystem ou **QCOS**.

Um **QCOS** é uma camada de software especializada que gerencia hardware quântico, coordena alocação de recursos quânticos e facilita a execução de algoritmos quânticos em `QPU`s, unidades de processamento quânticas. Enquanto **Sistemas Operacionais** clássicos gerenciam `CPU`, memória e `E/S`, um **QCOS** deve lidar com desafios únicos como gerenciamento de emaranhamento, manutenção de coerência de **qubits** e correção de erros quânticos. As funções primárias de um **QCOS** incluem: o **Gerenciamento de Recursos Quânticos**, a manipulação cuidadosa de qubits, garantindo inicialização correta, emaranhamento conforme necessário e medição precisa; a **Correção de Erros Quânticos e Mitigação de Ruído**, caracterizada pela aplicação de algoritmos de correção de erros quânticos (QEC) ou técnicas de mitigação de erros na era NISQ para aumentar a fidelidade dos resultados; o **Escalonamento e Otimização de Algoritmos Quânticos**, para o escalonamento eficiente de operações quânticas, otimizando execução para reduzir tempo e maximizar utilização de recursos por meio de compiladores e runtime que gerenciam execução em múltiplas `QPU`s; e, finalmente, a **Abstração e Interface**, o fornecimento de camada de abstração sobre complexidade do hardware quântico, como a abstração *Qernel* que expõe `APIs`transparentes para execução de jobs quânticos.

::: callout-note
**Qernel: Interface para Hardware Quântico**  
O termo **Qernel**, mencionado nos **Sistemas Operacionais Quânticos** (QCOS), refere-se a uma camada de abstração que simplifica a interação com hardware quântico. Semelhante a um `Kernel` clássico, o **Qernel** gerencia qubits, operações quânticas e medições, oferecendo `APIs`para desenvolvimento e conexão. Ele oculta complexidades como decoerência e conectividade de **qubits**, permitindo o desenvolvimento de algoritmos quânticos sem conhecimento detalhado do hardware. Plataformas como [Qiskit](https://www.ibm.com/quantum/qiskit) e [Cirq](https://quantumai.google/cirq) utilizam conceitos similares para facilitar a programação quântica.
:::  

O desenvolvimento de **QCOS** enfrenta uma constelação de desafios intrínsecos que emergem diretamente das propriedades fundamentais da mecânica quântica e das limitações tecnológicas atuais. A decoerência e o ruído constituem obstáculos primordiais, exigindo a implementação de mecanismos de mitigação extremamente sofisticados que devem operar continuamente para preservar a integridade dos estados quânticos. Estes fenômenos físicos inevitáveis degradam rapidamente a informação quântica, forçando os desenvolvedores de **QCOS** a criar estratégias de correção de erros em tempo real que frequentemente consomem recursos computacionais significativos. Simultaneamente, a escalabilidade apresenta-se como um desafio de complexidade exponencial, onde cada **qubit** adicional ao sistema não apenas aumenta linearmente os recursos necessários, mas multiplica exponencialmente as interações possíveis e os estados que devem ser gerenciados, criando uma barreira fundamental para sistemas quânticos de grande escala. Além disso, a integração harmoniosa com sistemas clássicos emerge como requisito indispensável para a viabilidade prática dos modelos híbridos, demandando protocolos de comunicação eficientes e sincronização precisa entre paradigmas computacionais fundamentalmente diferentes. Por fim, a confiabilidade operacional do próprio **QCOS** deve atingir padrões de excelência que transcendem os requisitos de sistemas clássicos, uma vez que a natureza probabilística e frágil dos estados quânticos torna qualquer falha do **Sistema Operacional** potencialmente catastrófica para a integridade dos cálculos em andamento.

As **arquiteturas de computação híbrida quântica-clássica** estão emergindo como abordagem promissora, nas quais computadores clássicos trabalham com `QPU`s. O `QPU` atua como acelerador especializado para partes de software que podem ser beneficiadas com o uso da computação quântica, enquanto o sistema clássico lida com conversão de código entre paradigmas, orquestração, preparação de dados e pós-processamento. Nestes sistemas, hardware especializado como `GPU`s e `FPGA`s/`RFSoC`s desempenha as funções de controle e medição de **qubits**.

::: callout-note
**FPGA e RFSoC: Hardware Especializado para Controle Quântico**

**Field-Programmable Gate Arrays (FPGAs)** são circuitos integrados reconfiguráveis que podem ser programados após a fabricação para implementar qualquer função lógica digital. _Diferentemente de processadores tradicionais que executam instruções sequencialmente, os FPGAs permitem a criação de circuitos digitais customizados que operam em paralelo massivo_. Esta característica os torna ideais para aplicações que exigem processamento em tempo real de alta velocidade, baixa latência e controle preciso de timing.

No contexto da computação quântica, os **FPGAs** desempenham funções críticas de **controle de qubits**, gerando pulsos de micro-ondas precisos para manipulação de estados quânticos, **aquisição de dados em tempo real** para medição de qubits, e **processamento de sinais** para correção de erros e calibração do sistema. Sua capacidade de reconfiguração permite que algoritmos de controle sejam atualizados conforme necessário sem alterações de hardware.

**RF System-on-Chip (RFSoCs)** representam uma evolução dos FPGAs, integrando capacidades de radiofrequência diretamente no chip. _Os RFSoCs combinam um FPGA com conversores analógico-digitais (ADCs) e digital-analógicos (DACs) de alta velocidade, permitindo processamento direto de sinais de RF sem necessidade de componentes externos_. Esta integração é particularmente valiosa em sistemas quânticos que operam em frequências de micro-ondas.

Em aplicações quânticas, **RFSoCs** oferecem vantagens significativas: **geração direta de sinais de controle** para qubits sem conversões intermediárias, **redução de latência** crítica para operações de correção de erros em tempo real, **menor consumo de energia** através da integração, e **sincronização precisa** entre múltiplos canais de controle. Empresas como Xilinx (agora AMD) desenvolveram RFSoCs especificamente otimizados para controle de sistemas quânticos, como a série Zynq UltraScale+ RFSoC.

A importância destes dispositivos na computação quântica está na sua capacidade de fornecer o controle de timing de nanossegundos necessário para manipular estados quânticos frágeis, processo esse que seria impossível com hardware de propósito geral devido às limitações de latência e precisão temporal.
:::

O **Sistema Operacional** em ambiente híbrido orquestra tarefas entre componentes clássicos e quânticos, gerenciando fluxo de dados e sincronização, abstraindo complexidade do hardware quântico e facilitando algoritmos híbridos como VQE e QAOA.

::: callout-note
**VQE e QAOA: Algoritmos Híbridos Quântico-Clássicos**

**Variational Quantum Eigensolver (VQE)** é um algoritmo híbrido projetado para encontrar o estado fundamental de sistemas quânticos, particularmente útil para simulação molecular e problemas de química quântica. _O VQE combina um circuito quântico parametrizado, executado em uma QPU, com um otimizador clássico que ajusta os parâmetros do circuito para minimizar a energia do sistema_. Esta abordagem permite que sistemas quânticos NISQ, apesar de suas limitações de ruído e profundidade de circuito, contribuam efetivamente para resolver problemas práticos.

O algoritmo opera em um ciclo iterativo: o **processador quântico** prepara estados candidatos usando circuitos parametrizados e mede o valor esperado de energia, enquanto o **processador clássico** utiliza algoritmos de otimização como gradiente descendente ou métodos evolutivos para ajustar os parâmetros e minimizar a função objetivo. Esta divisão de tarefas aproveita as forças de cada paradigma computacional: a capacidade quântica de explorar espaços de estados exponenciais e a robustez clássica para otimização numérica.

**Quantum Approximate Optimization Algorithm (QAOA)** é um algoritmo híbrido especificamente desenvolvido para problemas de otimização combinatória, como o problema do corte máximo em grafos ou otimização de portfólios financeiros. _O QAOA utiliza uma sequência alternada de operadores quânticos: um Hamiltoniano de problema que codifica a função objetivo e um Hamiltoniano de mistura que explora o espaço de soluções_. Os parâmetros destes operadores são otimizados classicamente para maximizar a probabilidade de medir a solução ótima.

A estrutura do **QAOA** é parametrizada por sua profundidade $p$, onde circuitos mais profundos teoricamente oferecem melhor aproximação da solução ótima, mas requerem maior fidelidade quântica. Para $p=1$, o algoritmo reduz-se a uma heurística simples, enquanto no limite $p \to \infty$, reproduz o algoritmo quântico adiabático ótimo.

**Importância para Sistemas Híbridos**: ambos os algoritmos exemplificam o paradigma emergente de computação híbrida, onde **QPUs** atuam como coprocessadores especializados para exploração de espaços quânticos, enquanto **CPUs/GPUs** clássicas gerenciam otimização, pré-processamento e análise de resultados. Esta abordagem híbrida é fundamental para a era NISQ, permitindo que dispositivos quânticos imperfeitos contribuam para resolução de problemas práticos através da sinergia com recursos computacionais clássicos robustos.

Aplicações práticas incluem descoberta de fármacos (VQE para simulação molecular), otimização logística (QAOA para roteamento de veículos), e finanças quantitativas (ambos para otimização de portfólios e gestão de risco).
:::

A @tbl-quantum1 mostra um resumo dos desafios associados a criação de **Sistemas Operacionais** para Computadores quânticos.

| Componente/Função do **QCOS** | Descrição da Função | Desafios Associados | Tecnologias/Abordagens Relevantes |
| :---- | :---- | :---- | :---- |
| **Gerenciamento de Qubits** | Inicialização, manipulação de estados quânticos (superposição, emaranhamento), medição precisa de qubits. | Manter a coerência dos qubits, controle preciso de operações quânticas, escalabilidade para grande número de qubits. | Pulsos de micro-ondas/laser, armadilhas de íons, **qubits** supercondutores, hardware de controle (FPGAs, RFSoCs). |
| **Correção de Erros Quânticos/Mitigação de Ruído** | Identificar e corrigir/mitigar erros devido à decoerência e ruído para manter a integridade da computação. | Fragilidade dos estados quânticos, overhead de **qubits** e operações para QEC, complexidade dos códigos corretores. | Códigos de correção de erros quânticos (e.g., código de superfície), técnicas de mitigação de erros (e.g., Zero Noise Extrapolation). |
| **Escalonamento e Otimização de Circuitos** | Agendar operações quânticas eficientemente, otimizar circuitos para reduzir profundidade/contagem de portas. | Limitações de conectividade entre qubits, tempos de coerência finitos, heterogeneidade de QPUs. | Compiladores quânticos, algoritmos de roteamento e mapeamento de qubits, técnicas de otimização de circuitos, multi-programação. |
| **Interface/Abstração de Hardware** (e.g., Qernel) | Fornecer uma interface de alto nível para programadores, abstraindo a complexidade do hardware quântico. | Diversidade de arquiteturas de hardware quântico, ocultar a natureza ruidosa do hardware. | `APIs`de programação quântica (e.g., Qiskit, Cirq), linguagens de descrição de circuitos, abstração Qernel. |
| **Suporte a Modelos Híbridos** | Orquestrar a execução entre processadores clássicos e quânticos, gerenciar fluxo de dados e sincronização. | Latência na comunicação clássico-quântica, sincronização eficiente, desenvolvimento de algoritmos híbridos. | Algoritmos variacionais (VQE, QAOA), plataformas de computação híbrida (Ex.: Azure Quantum). |

: Desafios e tecnologias da criação de **Sistemas Operacionais** para computadores quânticos. {#tbl-quantum1}

#### Exemplos de **Sistemas Operacionais Quânticos**

Um **QCOS** gerencia recursos quânticos, como qubits, portas quânticas e circuitos, de forma análoga a como um **Sistema Operacional** clássico gerencia `CPU`, memória e `E/S`. O `Qernel`, componente central, orquestra a alocação de **qubits**, o escalonamento de operações quânticas e a mitigação de erros causados por decoerência e ruído, desafios inerentes aos processadores quânticos atuais. Ainda que eles se distinguam dos **Sistemas Operacionais** clássicos, a curiosa leitora deveria prestar atenção aos seguintes sistemas:

1. **Qiskit Runtime (IBM)**
    O [Qiskit](https://www.ibm.com/quantum/qiskit) Runtime é uma plataforma que atua como uma camada de abstração para gerenciar computação quântica em hardware da IBM, como o [IBM Quantum Eagle](https://www.ibm.com/quantum/blog/127-qubit-quantum-processor-eagle) (127 qubits). Ele fornece uma interface para escalonar circuitos quânticos, otimizar alocação de **qubits** e executar algoritmos híbridos quântico-clássicos. Em 2025, espera-se que o [Qiskit Runtime](https://docs.quantum.ibm.com/api/qiskit-ibm-runtime) seja usado em aplicações como simulação de moléculas químicas e otimização logística, demonstrando a viabilidade de um **QCOS** ainda que seja rudimentar.

2. **Cirq e TensorFlow Quantum (Google)**
    O [Cirq](https://quantumai.google/cirq), combinado com o [TensorFlow Quantum](https://www.tensorflow.org/quantum?hl=pt-br), forma um ecossistema para desenvolver e executar programas quânticos no hardware do Google, como o processador [Sycamore](https://quantumai.google/quantumcomputer). Essas ferramentas gerenciam a compilação de circuitos quânticos, a alocação de recursos e a integração com algoritmos de machine learning. Um exemplo prático é a simulação de sistemas quânticos em física de matéria condensada, onde o Cirq atua como uma interface de **Sistema Operacional** Quântico.

3. **SpinQ QOS**
    A [SpinQ](https://www.spinquanta.com/) Technology, uma empresa chinesa, lançou em 2025 um **QOS**, projetado para seus processadores quânticos de pequena escala (2-20 qubits). O **QOS** gerencia a inicialização de qubits, a execução de circuitos e a correção de erros em tempo real, com uma interface amigável para pesquisadores. Ele é usado em educação e pesquisa, permitindo experimentos com algoritmos como [Shor](https://www.classiq.io/insights/shors-algorithm-explained) e [Grover](https://learning.quantum.ibm.com/course/fundamentals-of-quantum-algorithms/grovers-algorithm) em hardware acessível.

4. **Orca Computing PT-1**
    A [Orca Computing](https://orcacomputing.com/) desenvolveu um **Sistema Operacional** Quântico para seu processador fotônico [PT-1](https://orcacomputing.com/orca-pt-1/), que opera com qubits baseados em fótons. Esse sistema gerencia a multiplexação temporal de qubits e a integração com sistemas clássicos, sendo aplicado em problemas de otimização em finanças e telecomunicações.

5. **D-Wave Ocean SDK**
    O [Ocean SDK](https://www.dwavequantum.com/solutions-and-products/ocean/) da [D-Wave](https://www.dwavequantum.com/) é um **Sistema Operacional** Quântico híbrido que combina recozimento quântico com computação clássica. Ele permite a modelagem de problemas de otimização complexos, como roteamento de veículos e alocação de recursos, utilizando o processador quântico Advantage. O Ocean SDK gerencia a alocação de qubits, a execução de algoritmos quânticos e a integração com sistemas clássicos.

Estes **Sistemas Operacionais Quânticos**, se já pudermos chamar assim, estão em estágios iniciais de desenvolvimento. Contudo, já demonstram a viabilidade de gerenciar recursos quânticos e executar algoritmos complexos:

- **Simulação Química**: O Qiskit Runtime foi usado pela [Merck em 2025](https://www.ddw-online.com/how-quantum-computing-is-revolutionising-drug-development-34423-202504/) para simular interações moleculares, reduzindo o tempo de desenvolvimento de novos fármacos.
- **Otimização Logística**: A D-Wave, com seu **Sistema Operacional** híbrido para recozimento quântico, otimizou rotas de entrega para [empresas de hortifruti](https://www.dwavequantum.com/media/2sof3qhz/the-pattison-food-group_case_story_v8.pdf), integrando recursos quânticos e clássicos.
- **Criptografia**: O **QOS** da **SpinQ** permitiu experimentos com algoritmos quânticos resistentes a ataques, como os baseados em reticulados, em [laboratórios acadêmicos](https://arxiv.org/abs/2308.06736). Em [um artigo](http://cjc.ict.ac.cn/online/onlinepaper/wc-202458160402.pdf) publicado em maio de 2024 no *Chinese Journal of Computers*, pesquisadores da Universidade de Xangai detalharam avanços significativos na fatoração de inteiros utilizando o computador quântico de recozimento **D-Wave Advantage**. A pesquisa demonstrou essa capacidade através de duas abordagens distintas. Primeiramente, utilizando um método que converte o problema de fatoração $N=pq$ em um problema de otimização, a equipe conseguiu fatorar o inteiro de **22 bits** 2.269.753 . Em um feito mais notável, a equipe realizou a primeira fatoração de um inteiro de **50 bits** (845.546.611.823.483) em um sistema **D-Wave**. Nesta abordagem híbrida, o computador quântico não resolveu o problema inteiro diretamente, mas atuou como um acelerador para um algoritmo clássico. O recozimento quântico foi usado para otimizar a solução do **P**roblema do **V**etor **M**ais **P**róximo, **CVP**, encontrando um vetor mais próximo do que o algoritmo clássico de Babai conseguiria sozinho, graças ao efeito de tunelamento quântico. Este trabalho não representa uma "quebra" da criptografia RSA utilizada comercialmente, que usa chaves de $2048$ bits ou mais, mas demonstra uma capacidade de ataque realista para a tecnologia de recozimento quântico, que, segundo os autores, mostra um progresso mais estável para este tipo de problema do que os computadores quânticos universais.

::: {.callout-note}
**O Problema do Vetor Mais Próximo**

O Problema do Vetor Mais Próximo, em inglês **C**losest **V**ector **P**roblem*, é uma questão importante em matemática e ciência da computação. Para entender vamos ver uma analogia com um pomar:

1.  **A Grade (Lattice)**: imagine um pomar onde as árvores foram plantadas em uma grade perfeitamente regular. Cada árvore representa um ponto,  vetor, nesta grade;
2.  **O Vetor Alvo**: agora, imagine que você joga uma bola para dentro do pomar. A bola cai em um local aleatório, que não é exatamente onde uma árvore está. A posição da bola é o seu _vetor alvo_;
3.  **O Problema**: o **CVP** é a pergunta: "Qual é a árvore mais próxima de onde a bola caiu?"

Em termos simples, dado um conjunto de pontos que formam uma grade regular e um ponto alvo qualquer no espaço, o **CVP** consiste em encontrar o ponto da grade mais próximo do ponto alvo. Embora pareça fácil em duas dimensões, em centenas ou milhares de dimensões, como nos problemas de criptografia, encontrar a resposta exata é um problema **NP-difícil**, o que significa que é computacionalmente inviável para computadores clássicos resolverem de forma eficiente à medida que o problema cresce.

**O Algoritmo de Babai**

O Algoritmo de Babai é uma solução inteligente e prática para o **CVP**. Como resolver o **CVP** perfeitamente é muito difícil, o algoritmo de Babai não tenta encontrar a resposta *perfeita*, mas sim uma resposta "boa o suficiente" e muito próxima da ideal. O Algoritmo de Babai é um algoritmo de aproximação. Vamos voltar ao pomar, mas desta vez com um mapa sobre ele.

1.  **O Problema do Pomar Inclinado**: o seu pomar pode ter sido plantado com as fileiras de árvores um pouco inclinadas, dificultando o cálculo de distâncias. Essa é a "grade" original e complexa.
2.  **O Algoritmo "Endireita" a Grade**: o Algoritmo de Babai primeiro "endireita" a grade, transformando-a em uma grade perfeitamente perpendicular, ortogonal, como uma folha de papel quadriculado. Isso é feito usando um outro algoritmo, como o **LLL**.
3.  **Encontrando o Ponto Mais Próximo no Mapa**: com a grade agora "quadriculada" e fácil de usar, o algoritmo simplesmente arredonda as coordenadas da sua bola para encontrar o cruzamento mais próximo no papel quadriculado.
4.  **A Solução Aproximada**: esse ponto no papel corresponde a uma árvore específica no pomar original. Essa árvore é a solução que o Algoritmo de Babai fornece.

Ela pode não ser a árvore *exatamente* mais próxima, mas é garantido que será uma das mais próximas, e o cálculo é imensamente mais rápido do que tentar medir a distância para todas as árvores do pomar.

Em resumo, o **Algoritmo de Babai** é uma ferramenta prática para encontrar uma solução "boa o suficiente" para o **Problema do Vetor Mais Próximo**, que de outra forma seria computacionalmente inviável de resolver perfeitamente. É por isso que os pesquisadores no artigo usaram o computador quântico para tentar melhorar a resposta "boa o suficiente" do algoritmo de Babai e encontrar uma solução ainda melhor.
:::

O progresso na computação quântica ocorre por meio da interdependência entre hardware, software e algoritmos. Trata-se do nosso conhecido laço de realimentação positiva. Avanços no hardware quântico permitem **QCOS**s mais sofisticados, que por sua vez viabilizam algoritmos mais complexos, criando um ciclo de feedback positivo. Os **Sistemas Operacionais Quânticos** estão no nexo dessa coevolução, atuando como elemento unificador entre hardware e software algorítmico, sendo essenciais para democratizar o acesso e operacionalizar o potencial da computação quântica.

